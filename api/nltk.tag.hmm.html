<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>NLTK :: nltk.tag.hmm module</title>
  

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">NLTK</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">NLTK Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="nltk.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto.html">Example Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk">NLTK on GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing to NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../team.html">NLTK Team</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <section id="module-nltk.tag.hmm">
<span id="nltk-tag-hmm-module"></span><h1>nltk.tag.hmm module<a class="headerlink" href="#module-nltk.tag.hmm" title="Permalink to this heading">¶</a></h1>
<p>Hidden Markov Models (HMMs) largely used to assign the correct label sequence
to sequential data or assess the probability of a given label and data
sequence. These models are finite state machines characterised by a number of
states, transitions between these states, and output symbols emitted while in
each state. The HMM is an extension to the Markov chain, where each state
corresponds deterministically to a given event. In the HMM the observation is
a probabilistic function of the state. HMMs share the Markov chain’s
assumption, being that the probability of transition from one state to another
only depends on the current state - i.e. the series of states that led to the
current state are not used. They are also time invariant.</p>
<p>The HMM is a directed graph, with probability weighted edges (representing the
probability of a transition between the source and sink states) where each
vertex emits an output symbol when entered. The symbol (or observation) is
non-deterministically generated. For this reason, knowing that a sequence of
output observations was generated by a given HMM does not mean that the
corresponding sequence of states (and what the current state is) is known.
This is the ‘hidden’ in the hidden markov model.</p>
<p>Formally, a HMM can be characterised by:</p>
<ul class="simple">
<li><p>the output observation alphabet. This is the set of symbols which may be
observed as output of the system.</p></li>
<li><p>the set of states.</p></li>
<li><p>the transition probabilities <em>a_{ij} = P(s_t = j | s_{t-1} = i)</em>. These
represent the probability of transition to each state from a given state.</p></li>
<li><p>the output probability matrix <em>b_i(k) = P(X_t = o_k | s_t = i)</em>. These
represent the probability of observing each symbol in a given state.</p></li>
<li><p>the initial state distribution. This gives the probability of starting
in each state.</p></li>
</ul>
<p>To ground this discussion, take a common NLP application, part-of-speech (POS)
tagging. An HMM is desirable for this task as the highest probability tag
sequence can be calculated for a given sequence of word forms. This differs
from other tagging techniques which often tag each word individually, seeking
to optimise each individual tagging greedily without regard to the optimal
combination of tags for a larger unit, such as a sentence. The HMM does this
with the Viterbi algorithm, which efficiently computes the optimal path
through the graph given the sequence of words forms.</p>
<p>In POS tagging the states usually have a 1:1 correspondence with the tag
alphabet - i.e. each state represents a single tag. The output observation
alphabet is the set of word forms (the lexicon), and the remaining three
parameters are derived by a training regime. With this information the
probability of a given sentence can be easily derived, by simply summing the
probability of each distinct path through the model. Similarly, the highest
probability tagging sequence can be derived with the Viterbi algorithm,
yielding a state sequence which can be mapped into a tag sequence.</p>
<p>This discussion assumes that the HMM has been trained. This is probably the
most difficult task with the model, and requires either MLE estimates of the
parameters or unsupervised learning using the Baum-Welch algorithm, a variant
of EM.</p>
<p>For more information, please consult the source code for this module,
which includes extensive demonstration code.</p>
<dl class="py class">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">HiddenMarkovModelTagger</span></span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.tag.api.html#nltk.tag.api.TaggerI" title="nltk.tag.api.TaggerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">TaggerI</span></code></a></p>
<p>Hidden Markov model class, a generative model for labelling sequence data.
These models define the joint probability of a sequence of symbols and
their labels (state transitions) as the product of the starting state
probability, the probability of each state transition, and the probability
of each observation being generated from each state. This is described in
more detail in the module documentation.</p>
<p>This implementation is based on the HMM description in Chapter 8, Huang,
Acero and Hon, Spoken Language Processing and includes an extension for
training shallow HMM parsers or specialized HMMs as in Molina et.
al, 2002.  A specialized HMM modifies training data by applying a
specialization function to create a new training set that is more
appropriate for sequential tagging with an HMM.  A typical use case is
chunking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbols</strong> (<em>seq of any</em>) – the set of output symbols (alphabet)</p></li>
<li><p><strong>states</strong> (<em>seq of any</em>) – a set of states representing state space</p></li>
<li><p><strong>transitions</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ConditionalProbDistI" title="nltk.probability.ConditionalProbDistI"><em>ConditionalProbDistI</em></a>) – transition probabilities; Pr(s_i | s_j) is the
probability of transition from state i given the model is in
state_j</p></li>
<li><p><strong>outputs</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ConditionalProbDistI" title="nltk.probability.ConditionalProbDistI"><em>ConditionalProbDistI</em></a>) – output probabilities; Pr(o_k | s_i) is the probability
of emitting symbol k when entering state i</p></li>
<li><p><strong>priors</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ProbDistI" title="nltk.probability.ProbDistI"><em>ProbDistI</em></a>) – initial state distribution; Pr(s_i) is the probability
of starting in state i</p></li>
<li><p><strong>transform</strong> (<em>callable</em>) – an optional function for transforming training
instances, defaults to the identity function.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transitions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform=&lt;function</span> <span class="pre">_identity&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.best_path">
<span class="sig-name descname"><span class="pre">best_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.best_path"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.best_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the state sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>sequence of any</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.best_path_simple">
<span class="sig-name descname"><span class="pre">best_path_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.best_path_simple"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.best_path_simple" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.  This uses a simple, direct method, and is included for
teaching purposes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the state sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>sequence of any</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the entropy over labellings of the given sequence. This is
given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span><span class="p">(</span><span class="n">O</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span>
</pre></div>
</div>
<p>where the summation ranges over all state sequences, S. Let
<em>Z = Pr(O) = sum_S Pr(S, O)}</em> where the summation ranges over all state
sequences and O is the observation sequence. As such the entropy can
be re-expressed as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="p">[</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="p">]</span>
<span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="p">[</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S_0</span><span class="p">)</span> <span class="o">+</span> <span class="n">sum_t</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S_t</span> <span class="o">|</span> <span class="n">S_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">})</span> <span class="o">+</span> <span class="n">sum_t</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span> <span class="o">|</span> <span class="n">S_t</span><span class="p">)</span> <span class="p">]</span>
</pre></div>
</div>
<p>The order of summation for the log terms can be flipped, allowing
dynamic programming to be used to calculate the entropy. Specifically,
we use the forward and backward probabilities (alpha, beta) giving:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_s0</span> <span class="n">alpha_0</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span> <span class="n">beta_0</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span>
<span class="o">+</span> <span class="n">sum_t</span><span class="p">,</span><span class="n">si</span><span class="p">,</span><span class="n">sj</span> <span class="n">alpha_t</span><span class="p">(</span><span class="n">si</span><span class="p">)</span> <span class="n">Pr</span><span class="p">(</span><span class="n">sj</span> <span class="o">|</span> <span class="n">si</span><span class="p">)</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span><span class="o">+</span><span class="mi">1</span> <span class="o">|</span> <span class="n">sj</span><span class="p">)</span> <span class="n">beta_t</span><span class="p">(</span><span class="n">sj</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">sj</span> <span class="o">|</span> <span class="n">si</span><span class="p">)</span>
<span class="o">+</span> <span class="n">sum_t</span><span class="p">,</span><span class="n">st</span> <span class="n">alpha_t</span><span class="p">(</span><span class="n">st</span><span class="p">)</span> <span class="n">beta_t</span><span class="p">(</span><span class="n">st</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span> <span class="o">|</span> <span class="n">st</span><span class="p">)</span>
</pre></div>
</div>
<p>This simply uses alpha and beta to find the probabilities of partial
sequences, constrained to include the given state(s) at some point in
time.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.log_probability">
<span class="sig-name descname"><span class="pre">log_probability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.log_probability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.log_probability" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the log-probability of the given symbol sequence. If the
sequence is labelled, then returns the joint log-probability of the
symbol, state sequence. Otherwise, uses the forward algorithm to find
the log-probability over all label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the log-probability of the sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sequence</strong> (<a class="reference internal" href="nltk.ccg.lexicon.html#nltk.ccg.lexicon.Token" title="nltk.ccg.lexicon.Token"><em>Token</em></a>) – the sequence of symbols which must contain the TEXT
property, and optionally the TAG property</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.point_entropy">
<span class="sig-name descname"><span class="pre">point_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.point_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.point_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the pointwise entropy over the possible states at each
position in the chain, given the observation sequence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.probability">
<span class="sig-name descname"><span class="pre">probability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.probability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.probability" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the probability of the given symbol sequence. If the sequence
is labelled, then returns the joint probability of the symbol, state
sequence. Otherwise, uses the forward algorithm to find the
probability over all label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the probability of the sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sequence</strong> (<a class="reference internal" href="nltk.ccg.lexicon.html#nltk.ccg.lexicon.Token" title="nltk.ccg.lexicon.Token"><em>Token</em></a>) – the sequence of symbols which must contain the TEXT
property, and optionally the TAG property</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.random_sample">
<span class="sig-name descname"><span class="pre">random_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.random_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.random_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the randomly created state/observation sequence,
generated according to the HMM’s probability
distributions. The SUBTOKENS have TEXT and TAG
properties containing the observation and state
respectively.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rng</strong> (<em>Random</em><em> (or </em><em>any object with a random</em><em>(</em><em>) </em><em>method</em><em>)</em>) – random number generator</p></li>
<li><p><strong>length</strong> (<em>int</em>) – desired output length</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.reset_cache">
<span class="sig-name descname"><span class="pre">reset_cache</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.reset_cache"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.reset_cache" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.tag">
<span class="sig-name descname"><span class="pre">tag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.tag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.tag" title="Permalink to this definition">¶</a></dt>
<dd><p>Tags the sequence with the highest probability state sequence. This
uses the best_path method to find the Viterbi path.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a labelled sequence of symbols</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.test" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests the HiddenMarkovModelTagger instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled test instances</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – boolean flag indicating whether training should be
verbose or include printed output</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTagger.train">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labeled_sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_sequence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTagger.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a hidden markov model tagger</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nltk.tag.hmm.HiddenMarkovModelTagger" title="nltk.tag.hmm.HiddenMarkovModelTagger">HiddenMarkovModelTagger</a></p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled training instances,
i.e. a list of sentences represented as tuples</p></li>
<li><p><strong>test_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled test instances</p></li>
<li><p><strong>unlabeled_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of unlabeled training instances,
i.e. a list of sentences represented as words</p></li>
<li><p><strong>transform</strong> (<em>function</em>) – an optional function for transforming training
instances, defaults to the identity function, see <code class="docutils literal notranslate"><span class="pre">transform()</span></code></p></li>
<li><p><strong>estimator</strong> (<em>class</em><em> or </em><em>function</em>) – an optional function or class that maps a
condition’s frequency distribution to its probability
distribution, defaults to a Lidstone distribution with gamma = 0.1</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – boolean flag indicating whether training should be
verbose or include printed output</p></li>
<li><p><strong>max_iterations</strong> (<em>int</em>) – number of Baum-Welch iterations to perform</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTrainer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">HiddenMarkovModelTrainer</span></span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTrainer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTrainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Algorithms for learning HMM parameters from training data. These include
both supervised learning (MLE) and unsupervised learning (Baum-Welch).</p>
<p>Creates an HMM trainer to induce an HMM with the given states and
output symbol alphabet. A supervised and unsupervised training
method may be used. If either of the states or symbols are not given,
these may be derived from supervised training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> (<em>sequence of any</em>) – the set of state labels</p></li>
<li><p><strong>symbols</strong> (<em>sequence of any</em>) – the set of observation symbols</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTrainer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symbols</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTrainer.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTrainer.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTrainer.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labeled_sequences</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unlabeled_sequences</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTrainer.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTrainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains the HMM using both (or either of) supervised and unsupervised
techniques.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the trained model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nltk.tag.hmm.HiddenMarkovModelTagger" title="nltk.tag.hmm.HiddenMarkovModelTagger">HiddenMarkovModelTagger</a></p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labelled_sequences</strong> (<em>list</em>) – the supervised training data, a set of
labelled sequences of observations
ex: [ (word_1, tag_1),…,(word_n,tag_n) ]</p></li>
<li><p><strong>unlabeled_sequences</strong> (<em>list</em>) – the unsupervised training data, a set of
sequences of observations
ex: [ word_1, …, word_n ]</p></li>
<li><p><strong>kwargs</strong> – additional arguments to pass to the training methods</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTrainer.train_supervised">
<span class="sig-name descname"><span class="pre">train_supervised</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labelled_sequences</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTrainer.train_supervised"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTrainer.train_supervised" title="Permalink to this definition">¶</a></dt>
<dd><p>Supervised training maximising the joint probability of the symbol and
state sequences. This is done via collecting frequencies of
transitions between states, symbol observations while within each
state and which states start a sentence. These frequency distributions
are then normalised into probability estimates, which can be
smoothed if desired.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the trained model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nltk.tag.hmm.HiddenMarkovModelTagger" title="nltk.tag.hmm.HiddenMarkovModelTagger">HiddenMarkovModelTagger</a></p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labelled_sequences</strong> (<em>list</em>) – the training data, a set of
labelled sequences of observations</p></li>
<li><p><strong>estimator</strong> – a function taking
a FreqDist and a number of bins and returning a CProbDistI;
otherwise a MLE estimate is used</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.hmm.HiddenMarkovModelTrainer.train_unsupervised">
<span class="sig-name descname"><span class="pre">train_unsupervised</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequences</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTrainer.train_unsupervised"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.HiddenMarkovModelTrainer.train_unsupervised" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains the HMM using the Baum-Welch algorithm to maximise the
probability of the data sequence. This is a variant of the EM
algorithm, and is unsupervised in that it doesn’t need the state
sequences for the symbols. The code is based on ‘A Tutorial on Hidden
Markov Models and Selected Applications in Speech Recognition’,
Lawrence Rabiner, IEEE, 1989.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the trained model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nltk.tag.hmm.HiddenMarkovModelTagger" title="nltk.tag.hmm.HiddenMarkovModelTagger">HiddenMarkovModelTagger</a></p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequences</strong> (<em>list</em>) – the training data, a set of
sequences of observations</p>
</dd>
</dl>
<p>kwargs may include following parameters:</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – a HiddenMarkovModelTagger instance used to begin
the Baum-Welch algorithm</p></li>
<li><p><strong>max_iterations</strong> – the maximum number of EM iterations</p></li>
<li><p><strong>convergence_logprob</strong> – the maximum change in log probability to
allow convergence</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.demo">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">demo</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#demo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.demo" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.demo_bw">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">demo_bw</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#demo_bw"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.demo_bw" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.demo_pos">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">demo_pos</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#demo_pos"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.demo_pos" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.demo_pos_bw">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">demo_pos_bw</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">supervised</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsupervised</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iterations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#demo_pos_bw"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.demo_pos_bw" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.load_pos">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">load_pos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_sents</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#load_pos"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.load_pos" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="nltk.tag.hmm.logsumexp2">
<span class="sig-prename descclassname"><span class="pre">nltk.tag.hmm.</span></span><span class="sig-name descname"><span class="pre">logsumexp2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">arr</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#logsumexp2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.hmm.logsumexp2" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/api/nltk.tag.hmm.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            
                <li class="footer-element">
                    <a href="https://github.com/nltk/nltk/tree/3.8.1">3.8.1</a>
                </li>
            

            
                <li class="footer-element">
                    Jan 02, 2023
                </li>
            
        </ul>

        
            <div id="copyright">
                &copy; 2023, NLTK Project
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>