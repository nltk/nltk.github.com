
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>nltk.lm package &#8212; NLTK 3.4 documentation</title>
    <link rel="stylesheet" href="../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
  </head><body>
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../index.html">NLTK 3.4 documentation</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="nltk-lm-package">
<h1>nltk.lm package<a class="headerlink" href="#nltk-lm-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-nltk.lm.api">
<span id="nltk-lm-api-module"></span><h2>nltk.lm.api module<a class="headerlink" href="#module-nltk.lm.api" title="Permalink to this headline">¶</a></h2>
<p>Language Model Interface.</p>
<dl class="class">
<dt id="nltk.lm.api.LanguageModel">
<em class="property">class </em><code class="descclassname">nltk.lm.api.</code><code class="descname">LanguageModel</code><span class="sig-paren">(</span><em>order</em>, <em>vocabulary=None</em>, <em>counter=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>ABC for Language Models.</p>
<p>Cannot be directly instantiated itself.</p>
<dl class="method">
<dt id="nltk.lm.api.LanguageModel.context_counts">
<code class="descname">context_counts</code><span class="sig-paren">(</span><em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.context_counts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.context_counts" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method for retrieving counts for a given context.</p>
<p>Assumes context has been checked and oov words in it masked.
:type context: tuple(str) or None</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.entropy">
<code class="descname">entropy</code><span class="sig-paren">(</span><em>text_ngrams</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.entropy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate cross-entropy of model for given evaluation text.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>text_ngrams</strong> (<em>Iterable</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>)</em><em>)</em>) – A sequence of ngram tuples.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">float</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>text</em>, <em>vocabulary_text=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains the model on a text.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>text</strong> – Training text as a sequence of sentences.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.generate">
<code class="descname">generate</code><span class="sig-paren">(</span><em>num_words=1</em>, <em>text_seed=None</em>, <em>random_seed=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate words from the model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_words</strong> (<em>int</em>) – How many words to generate. By default 1.</li>
<li><strong>text_seed</strong> – Generation can be conditioned on preceding context.</li>
<li><strong>random_seed</strong> – If provided, makes the random sampling part of</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>generation reproducible.
:return: One (str) word or a list of words generated from model.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">MLE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span> <span class="o">=</span> <span class="n">MLE</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">([[(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)]],</span> <span class="n">vocabulary_text</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">([[(</span><span class="s2">&quot;a&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,)]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">text_seed</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span>
<span class="go">&#39;b&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.logscore">
<code class="descname">logscore</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.logscore"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.logscore" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the log score of this word in this context.</p>
<p>The arguments are the same as for <cite>score</cite> and <cite>unmasked_score</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.perplexity">
<code class="descname">perplexity</code><span class="sig-paren">(</span><em>text_ngrams</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.perplexity"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.perplexity" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates the perplexity of the given text.</p>
<p>This is simply 2 ** cross-entropy for the text, so the arguments are the same.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.score">
<code class="descname">score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Masks out of vocab (OOV) words and computes their model score.</p>
<p>For model-specific logic of calculating scores, see the <cite>unmasked_score</cite>
method.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.api.LanguageModel.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#LanguageModel.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.LanguageModel.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score a word given some optional context.</p>
<p>Concrete models are expected to provide an implementation.
Note that this method does not mask its arguments with the OOV label.
Use the <cite>score</cite> method for that.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word</strong> (<em>str</em>) – Word for which we want the score</li>
<li><strong>context</strong> (<em>tuple</em><em>(</em><em>str</em><em>)</em>) – Context the word is in.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>If <cite>None</cite>, compute unigram score.
:param context: tuple(str) or None
:rtype: float</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.api.Smoothing">
<em class="property">class </em><code class="descclassname">nltk.lm.api.</code><code class="descname">Smoothing</code><span class="sig-paren">(</span><em>vocabulary</em>, <em>counter</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#Smoothing"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.Smoothing" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Ngram Smoothing Interface</p>
<p>Implements Chen &amp; Goodman 1995’s idea that all smoothing algorithms have
certain features in common. This should ideally allow smoothing algoritms to
work both with Backoff and Interpolation.</p>
<p>counter represents the number of counts for ngrams</p>
<dl class="method">
<dt id="nltk.lm.api.Smoothing.alpha_gamma">
<code class="descname">alpha_gamma</code><span class="sig-paren">(</span><em>word</em>, <em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#Smoothing.alpha_gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.Smoothing.alpha_gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.api.Smoothing.unigram_score">
<code class="descname">unigram_score</code><span class="sig-paren">(</span><em>word</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/api.html#Smoothing.unigram_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.api.Smoothing.unigram_score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.lm.counter">
<span id="nltk-lm-counter-module"></span><h2>nltk.lm.counter module<a class="headerlink" href="#module-nltk.lm.counter" title="Permalink to this headline">¶</a></h2>
<div class="section" id="language-model-counter">
<h3>Language Model Counter<a class="headerlink" href="#language-model-counter" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="nltk.lm.counter.NgramCounter">
<em class="property">class </em><code class="descclassname">nltk.lm.counter.</code><code class="descname">NgramCounter</code><span class="sig-paren">(</span><em>ngram_text=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.counter.NgramCounter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for counting ngrams.</p>
<p>Will count any ngram sequence you give it ;)</p>
<p>First we need to make sure we are feeding the counter sentences of ngrams.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="k">import</span> <span class="n">ngrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_bigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_unigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
</pre></div>
</div>
<p>The counting itself is very simple.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">(</span><span class="n">text_bigrams</span> <span class="o">+</span> <span class="n">text_unigrams</span><span class="p">)</span>
</pre></div>
</div>
<p>You can conveniently access ngram counts using standard python dictionary notation.
String keys will give you unigram counts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
</pre></div>
</div>
<p>If you want to access counts for higher order ngrams, use a list or a tuple.
These are treated as “context” keys, so what you get is a frequency distribution
over all continuations after the given context.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[(</span><span class="s1">&#39;a&#39;</span><span class="p">,)]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
</pre></div>
</div>
<p>This is equivalent to specifying explicitly the order of the ngram (in this case
2 for bigram) and indexing on the context.
&gt;&gt;&gt; ngram_counts[2][(‘a’,)] is ngram_counts[[‘a’]]
True</p>
<p>Note that the keys in <cite>ConditionalFreqDist</cite> cannot be lists, only tuples!
It is generally advisable to use the less verbose and more flexible square
bracket notation.</p>
<p>To get the count of the full ngram “a b”, do this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Specifying the ngram order as a number can be useful for accessing all ngrams
in that order.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="go">&lt;ConditionalFreqDist with 4 conditions&gt;</span>
</pre></div>
</div>
<p>The keys of this <cite>ConditionalFreqDist</cite> are the contexts we discussed earlier.
Unigrams can also be accessed with a human-friendly alias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">unigrams</span> <span class="ow">is</span> <span class="n">ngram_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Similarly to <cite>collections.Counter</cite>, you can update counts after initialization.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="n">ngrams</span><span class="p">([</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<dl class="method">
<dt id="nltk.lm.counter.NgramCounter.N">
<code class="descname">N</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.N"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.counter.NgramCounter.N" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns grand total number of ngrams stored.</p>
<p>This includes ngrams from all orders, so some duplication is expected.
:rtype: int</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">([[(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">)]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span><span class="o">.</span><span class="n">N</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="nltk.lm.counter.NgramCounter.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.counter.NgramCounter.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.counter.NgramCounter.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>ngram_text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.counter.NgramCounter.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates ngram counts from <cite>ngram_text</cite>.</p>
<p>Expects <cite>ngram_text</cite> to be a sequence of sentences (sequences).
Each sentence consists of ngrams as tuples of strings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>ngram_text</strong> (<em>Iterable</em><em>(</em><em>Iterable</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>)</em><em>)</em><em>)</em>) – Text containing senteces of ngrams.</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><strong>TypeError</strong> – if the ngrams are not tuples.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="module-nltk.lm.models">
<span id="nltk-lm-models-module"></span><h2>nltk.lm.models module<a class="headerlink" href="#module-nltk.lm.models" title="Permalink to this headline">¶</a></h2>
<p>Language Models</p>
<dl class="class">
<dt id="nltk.lm.models.InterpolatedLanguageModel">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">InterpolatedLanguageModel</code><span class="sig-paren">(</span><em>smoothing_cls</em>, <em>order</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#InterpolatedLanguageModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.InterpolatedLanguageModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.LanguageModel</span></code></a></p>
<p>Logic common to all interpolated language models.</p>
<p>The idea to abstract this comes from Chen &amp; Goodman 1995.</p>
<dl class="method">
<dt id="nltk.lm.models.InterpolatedLanguageModel.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#InterpolatedLanguageModel.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.InterpolatedLanguageModel.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score a word given some optional context.</p>
<p>Concrete models are expected to provide an implementation.
Note that this method does not mask its arguments with the OOV label.
Use the <cite>score</cite> method for that.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>word</strong> (<em>str</em>) – Word for which we want the score</li>
<li><strong>context</strong> (<em>tuple</em><em>(</em><em>str</em><em>)</em>) – Context the word is in.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>If <cite>None</cite>, compute unigram score.
:param context: tuple(str) or None
:rtype: float</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.models.KneserNeyInterpolated">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">KneserNeyInterpolated</code><span class="sig-paren">(</span><em>order</em>, <em>discount=0.1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#KneserNeyInterpolated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.KneserNeyInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Kneser-Ney smoothing.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.lm.models.Laplace">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Laplace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.Laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.Lidstone" title="nltk.lm.models.Lidstone"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.Lidstone</span></code></a></p>
<p>Implements Laplace (add one) smoothing.</p>
<p>Initialization identical to BaseNgramModel because gamma is always 1.</p>
<dl class="attribute">
<dt id="nltk.lm.models.Laplace.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.models.Laplace.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.models.Lidstone">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">Lidstone</code><span class="sig-paren">(</span><em>gamma</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.Lidstone" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.LanguageModel</span></code></a></p>
<p>Provides Lidstone-smoothed scores.</p>
<p>In addition to initialization arguments from BaseNgramModel also requires
a number by which to increase the counts, gamma.</p>
<dl class="attribute">
<dt id="nltk.lm.models.Lidstone.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.models.Lidstone.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.models.Lidstone.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.Lidstone.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Add-one smoothing: Lidstone or Laplace.</p>
<p>To see what kind, look at <cite>gamma</cite> attribute on the class.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.models.MLE">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">MLE</code><span class="sig-paren">(</span><em>order</em>, <em>vocabulary=None</em>, <em>counter=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.MLE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.LanguageModel</span></code></a></p>
<p>Class for providing MLE ngram model scores.</p>
<p>Inherits initialization from BaseNgramModel.</p>
<dl class="attribute">
<dt id="nltk.lm.models.MLE.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.models.MLE.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.models.MLE.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.MLE.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the MLE score for a word given a context.</p>
<p>Args:
- word is expcected to be a string
- context is expected to be something reasonably convertible to a tuple</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.models.WittenBellInterpolated">
<em class="property">class </em><code class="descclassname">nltk.lm.models.</code><code class="descname">WittenBellInterpolated</code><span class="sig-paren">(</span><em>order</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#WittenBellInterpolated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.models.WittenBellInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Witten-Bell smoothing.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.lm.preprocessing">
<span id="nltk-lm-preprocessing-module"></span><h2>nltk.lm.preprocessing module<a class="headerlink" href="#module-nltk.lm.preprocessing" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="nltk.lm.preprocessing.flatten">
<code class="descclassname">nltk.lm.preprocessing.</code><code class="descname">flatten</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#nltk.lm.preprocessing.flatten" title="Permalink to this definition">¶</a></dt>
<dd><p>chain.from_iterable(iterable) –&gt; chain object</p>
<p>Alternate chain() constructor taking a single iterable argument
that evaluates lazily.</p>
</dd></dl>

<dl class="function">
<dt id="nltk.lm.preprocessing.pad_both_ends">
<code class="descclassname">nltk.lm.preprocessing.</code><code class="descname">pad_both_ends</code><span class="sig-paren">(</span><em>sequence</em>, <em>n</em>, <em>*</em>, <em>pad_left=True</em>, <em>pad_right=True</em>, <em>left_pad_symbol='&lt;s&gt;'</em>, <em>right_pad_symbol='&lt;/s&gt;'</em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.lm.preprocessing.pad_both_ends" title="Permalink to this definition">¶</a></dt>
<dd><p>Pads both ends of a sentence to length specified by ngram order.</p>
<p>Following convention &lt;s&gt; pads the start of sentence &lt;/s&gt; pads its end.</p>
</dd></dl>

<dl class="function">
<dt id="nltk.lm.preprocessing.padded_everygram_pipeline">
<code class="descclassname">nltk.lm.preprocessing.</code><code class="descname">padded_everygram_pipeline</code><span class="sig-paren">(</span><em>order</em>, <em>text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/preprocessing.html#padded_everygram_pipeline"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.preprocessing.padded_everygram_pipeline" title="Permalink to this definition">¶</a></dt>
<dd><p>Default preprocessing for a sequence of sentences.</p>
<p>Creates two iterators:
- sentences padded and turned into sequences of <cite>nltk.util.everygrams</cite>
- sentences padded as above and chained together for a flat stream of words</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>order</strong> – Largest ngram length produced by <cite>everygrams</cite>.</li>
<li><strong>text</strong> – Text to iterate over. Expected to be an iterable of sentences:</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Iterable[Iterable[str]]
:return: iterator over text as ngrams, iterator over text as vocabulary data</p>
</dd></dl>

<dl class="function">
<dt id="nltk.lm.preprocessing.padded_everygrams">
<code class="descclassname">nltk.lm.preprocessing.</code><code class="descname">padded_everygrams</code><span class="sig-paren">(</span><em>order</em>, <em>sentence</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/preprocessing.html#padded_everygrams"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.preprocessing.padded_everygrams" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper with some useful defaults.</p>
<p>Applies pad_both_ends to sentence and follows it up with everygrams.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.lm.smoothing">
<span id="nltk-lm-smoothing-module"></span><h2>nltk.lm.smoothing module<a class="headerlink" href="#module-nltk.lm.smoothing" title="Permalink to this headline">¶</a></h2>
<p>Smoothing algorithms for language modeling.</p>
<p>According to Chen &amp; Goodman 1995 these should work with both Backoff and
Interpolation.</p>
<dl class="class">
<dt id="nltk.lm.smoothing.GoodTuring">
<em class="property">class </em><code class="descclassname">nltk.lm.smoothing.</code><code class="descname">GoodTuring</code><span class="sig-paren">(</span><em>vocabulary</em>, <em>counter</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#GoodTuring"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.GoodTuring" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.Smoothing" title="nltk.lm.api.Smoothing"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.Smoothing</span></code></a></p>
<p>Good-Turing Smoothing</p>
<dl class="method">
<dt id="nltk.lm.smoothing.GoodTuring.unigram_score">
<code class="descname">unigram_score</code><span class="sig-paren">(</span><em>word</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#GoodTuring.unigram_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.GoodTuring.unigram_score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.smoothing.KneserNey">
<em class="property">class </em><code class="descclassname">nltk.lm.smoothing.</code><code class="descname">KneserNey</code><span class="sig-paren">(</span><em>vocabulary</em>, <em>counter</em>, <em>discount=0.1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#KneserNey"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.KneserNey" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.Smoothing" title="nltk.lm.api.Smoothing"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.Smoothing</span></code></a></p>
<p>Kneser-Ney Smoothing.</p>
<dl class="method">
<dt id="nltk.lm.smoothing.KneserNey.alpha">
<code class="descname">alpha</code><span class="sig-paren">(</span><em>word</em>, <em>prefix_counts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#KneserNey.alpha"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.KneserNey.alpha" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.KneserNey.alpha_gamma">
<code class="descname">alpha_gamma</code><span class="sig-paren">(</span><em>word</em>, <em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#KneserNey.alpha_gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.KneserNey.alpha_gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.KneserNey.gamma">
<code class="descname">gamma</code><span class="sig-paren">(</span><em>prefix_counts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#KneserNey.gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.KneserNey.gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.KneserNey.unigram_score">
<code class="descname">unigram_score</code><span class="sig-paren">(</span><em>word</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#KneserNey.unigram_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.KneserNey.unigram_score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.smoothing.WittenBell">
<em class="property">class </em><code class="descclassname">nltk.lm.smoothing.</code><code class="descname">WittenBell</code><span class="sig-paren">(</span><em>vocabulary</em>, <em>counter</em>, <em>discount=0.1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#WittenBell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.WittenBell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.Smoothing" title="nltk.lm.api.Smoothing"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.Smoothing</span></code></a></p>
<p>Witten-Bell smoothing.</p>
<dl class="method">
<dt id="nltk.lm.smoothing.WittenBell.alpha">
<code class="descname">alpha</code><span class="sig-paren">(</span><em>word</em>, <em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#WittenBell.alpha"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.WittenBell.alpha" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.WittenBell.alpha_gamma">
<code class="descname">alpha_gamma</code><span class="sig-paren">(</span><em>word</em>, <em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#WittenBell.alpha_gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.WittenBell.alpha_gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.WittenBell.gamma">
<code class="descname">gamma</code><span class="sig-paren">(</span><em>context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#WittenBell.gamma"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.WittenBell.gamma" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="nltk.lm.smoothing.WittenBell.unigram_score">
<code class="descname">unigram_score</code><span class="sig-paren">(</span><em>word</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/smoothing.html#WittenBell.unigram_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.smoothing.WittenBell.unigram_score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.lm.util">
<span id="nltk-lm-util-module"></span><h2>nltk.lm.util module<a class="headerlink" href="#module-nltk.lm.util" title="Permalink to this headline">¶</a></h2>
<p>Language Model Utilities</p>
<dl class="function">
<dt id="nltk.lm.util.log_base2">
<code class="descclassname">nltk.lm.util.</code><code class="descname">log_base2</code><span class="sig-paren">(</span><em>score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/util.html#log_base2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.util.log_base2" title="Permalink to this definition">¶</a></dt>
<dd><p>Convenience function for computing logarithms with base 2.</p>
</dd></dl>

</div>
<div class="section" id="module-nltk.lm.vocabulary">
<span id="nltk-lm-vocabulary-module"></span><h2>nltk.lm.vocabulary module<a class="headerlink" href="#module-nltk.lm.vocabulary" title="Permalink to this headline">¶</a></h2>
<p>Language Model Vocabulary</p>
<dl class="class">
<dt id="nltk.lm.vocabulary.Vocabulary">
<em class="property">class </em><code class="descclassname">nltk.lm.vocabulary.</code><code class="descname">Vocabulary</code><span class="sig-paren">(</span><em>counts=None</em>, <em>unk_cutoff=1</em>, <em>unk_label='&lt;UNK&gt;'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.vocabulary.Vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores language model vocabulary.</p>
<p>Satisfies two common language modeling requirements for a vocabulary:
- When checking membership and calculating its size, filters items</p>
<blockquote>
<div>by comparing their counts to a cutoff value.</div></blockquote>
<ul class="simple">
<li>Adds a special “unknown” token which unseen words are mapped to.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Tokens with counts greater than or equal to the cutoff value will
be considered part of the vocabulary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;c&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;d&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Tokens with frequency counts less than the cutoff value will be considered not
part of the vocabulary even though their entries in the count dictionary are
preserved.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;b&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;aliens&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Keeping the count entries for seen words allows us to change the cutoff value
without having to recalculate the counts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab2</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;b&quot;</span> <span class="ow">in</span> <span class="n">vocab2</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The cutoff value influences not only membership checking but also the result of
getting the size of the vocabulary using the built-in <cite>len</cite>.
Note that while the number of keys in the vocabulary’s counter stays the same,
the items in the vocabulary differ depending on the cutoff.
We use <cite>sorted</cite> to demonstrate because it keeps the order consistent.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">[&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
<p>In addition to items it gets populated with, the vocabulary stores a special
token that stands in for so-called “unknown” items. By default it’s “&lt;UNK&gt;”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;&lt;UNK&gt;&quot;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>We can look up words in a vocabulary using its <cite>lookup</cite> method.
“Unseen” words (with counts less than cutoff) are looked up as the unknown label.
If given one word (a string) as an input, this method will return a string.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
</pre></div>
</div>
<p>If given a sequence, it will return an tuple of the looked up words.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="go">(&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;&lt;UNK&gt;&#39;, &#39;d&#39;, &#39;&lt;UNK&gt;&#39;, &#39;c&#39;)</span>
</pre></div>
</div>
<p>It’s possible to update the counts after the vocabulary has been created.
The interface follows that of <cite>collections.Counter</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">3</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="nltk.lm.vocabulary.Vocabulary.cutoff">
<code class="descname">cutoff</code><a class="headerlink" href="#nltk.lm.vocabulary.Vocabulary.cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Cutoff value.</p>
<p>Items with count below this value are not considered part of vocabulary.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.vocabulary.Vocabulary.lookup">
<code class="descname">lookup</code><span class="sig-paren">(</span><em>words</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.lookup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.vocabulary.Vocabulary.lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Look up one or more words in the vocabulary.</p>
<p>If passed one word as a string will return that word or <cite>self.unk_label</cite>.
Otherwise will assume it was passed a sequence of words, will try to look
each of them up and return an iterator over the looked up words.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>words</strong> (<em>Iterable</em><em>(</em><em>str</em><em>) or </em><em>str</em>) – Word(s) to look up.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">generator(str) or str</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body">TypeError for types other than strings or iterables</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">]])</span>
<span class="go">(&#39;a&#39;, &#39;b&#39;, &#39;&lt;UNK&gt;&#39;, (&#39;&lt;UNK&gt;&#39;, &#39;b&#39;))</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="nltk.lm.vocabulary.Vocabulary.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.vocabulary.Vocabulary.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.vocabulary.Vocabulary.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>*counter_args</em>, <em>**counter_kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.vocabulary.Vocabulary.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update vocabulary counts.</p>
<p>Wraps <cite>collections.Counter.update</cite> method.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-nltk.lm">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nltk.lm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="nltk-language-modeling-module">
<h3>NLTK Language Modeling Module.<a class="headerlink" href="#nltk-language-modeling-module" title="Permalink to this headline">¶</a></h3>
<p>Currently this module covers only ngram language models, but it should be easy
to extend to neural models.</p>
<div class="section" id="preparing-data">
<h4>Preparing Data<a class="headerlink" href="#preparing-data" title="Permalink to this headline">¶</a></h4>
<p>Before we train our ngram models it is necessary to make sure the data we put in
them is in the right format.
Let’s say we have a text that is a list of sentences, where each sentence is
a list of strings. For simplicity we just consider a text consisting of
characters instead of words.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">]]</span>
</pre></div>
</div>
<p>If we want to train a bigram model, we need to turn this text into bigrams.
Here’s what the first sentence of our text would look like if we use a function
from NLTK for this.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="k">import</span> <span class="n">bigrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bigrams</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="go">[(&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;)]</span>
</pre></div>
</div>
<p>Notice how “b” occurs both as the first and second member of different bigrams
but “a” and “c” don’t? Wouldn’t it be nice to somehow indicate how often sentences
start with “a” and end with “c”?
A standard way to deal with this is to add special “padding” symbols to the
sentence before splitting it into ngrams.
Fortunately, NLTK also has a function for that, let’s see what it does to the
first sentence.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="k">import</span> <span class="n">pad_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span><span class="n">pad_left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="n">left_pad_symbol</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="n">pad_right</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="n">right_pad_symbol</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>Note the <cite>n</cite> argument, that tells the function we need padding for bigrams.
Now, passing all these parameters every time is tedious and in most cases they
can be safely assumed as defaults anyway.
Thus our module provides a convenience function that has all these arguments
already set while the other arguments remain the same as for <cite>pad_sequence</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="k">import</span> <span class="n">pad_both_ends</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>Combining the two parts discussed so far we get the following preparation steps
for one sentence.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bigrams</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)))</span>
<span class="go">[(&#39;&lt;s&gt;&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;), (&#39;c&#39;, &#39;&lt;/s&gt;&#39;)]</span>
</pre></div>
</div>
<p>To make our model more robust we could also train it on unigrams (single words)
as well as bigrams, its main source of information.
NLTK once again helpfully provides a function called <cite>everygrams</cite>.
While not the most efficient, it is conceptually simple.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="k">import</span> <span class="n">everygrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded_bigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">everygrams</span><span class="p">(</span><span class="n">padded_bigrams</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[(&#39;&lt;s&gt;&#39;,),</span>
<span class="go"> (&#39;a&#39;,),</span>
<span class="go"> (&#39;b&#39;,),</span>
<span class="go"> (&#39;c&#39;,),</span>
<span class="go"> (&#39;&lt;/s&gt;&#39;,),</span>
<span class="go"> (&#39;&lt;s&gt;&#39;, &#39;a&#39;),</span>
<span class="go"> (&#39;a&#39;, &#39;b&#39;),</span>
<span class="go"> (&#39;b&#39;, &#39;c&#39;),</span>
<span class="go"> (&#39;c&#39;, &#39;&lt;/s&gt;&#39;)]</span>
</pre></div>
</div>
<p>We are almost ready to start counting ngrams, just one more step left.
During training and evaluation our model will rely on a vocabulary that
defines which words are “known” to the model.
To create this vocabulary we need to pad our sentences (just like for counting
ngrams) and then combine the sentences into one flat stream of words.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="k">import</span> <span class="n">flatten</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;, &#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>In most cases we want to use the same text as the source for both vocabulary
and ngram counts.
Now that we understand what this means for our preprocessing, we can simply import
a function that does everything for us.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="k">import</span> <span class="n">padded_everygram_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">padded_everygram_pipeline</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>So as to avoid re-creating the text in memory, both <cite>train</cite> and <cite>vocab</cite> are lazy
iterators. They are evaluated on demand at training time.</p>
</div>
<div class="section" id="training">
<h4>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h4>
<p>Having prepared our data we are ready to start training a model.
As a simple example, let us train a Maximum Likelihood Estimator (MLE).
We only need to specify the highest ngram order to instantiate it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">MLE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span> <span class="o">=</span> <span class="n">MLE</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>This automatically creates an empty vocabulary…</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">0</span>
</pre></div>
</div>
<p>… which gets filled as we fit the model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">&lt;Vocabulary with cutoff=1 unk_label=&#39;&lt;UNK&gt;&#39; and 9 items&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">9</span>
</pre></div>
</div>
<p>The vocabulary helps us handle words that have not occurred during training.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="go">(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;aliens&quot;</span><span class="p">,</span> <span class="s2">&quot;from&quot;</span><span class="p">,</span> <span class="s2">&quot;Mars&quot;</span><span class="p">])</span>
<span class="go">(&#39;&lt;UNK&gt;&#39;, &#39;&lt;UNK&gt;&#39;, &#39;&lt;UNK&gt;&#39;)</span>
</pre></div>
</div>
<p>Moreover, in some cases we want to ignore words that we did see during training
but that didn’t occur frequently enough, to provide us useful information.
You can tell the vocabulary to ignore such words.
To find out how that works, check out the docs for the <cite>Vocabulary</cite> class.</p>
</div>
<div class="section" id="using-a-trained-model">
<h4>Using a Trained Model<a class="headerlink" href="#using-a-trained-model" title="Permalink to this headline">¶</a></h4>
<p>When it comes to ngram models the training boils down to counting up the ngrams
from the training corpus.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">&lt;NgramCounter with 2 ngram orders and 24 ngrams&gt;</span>
</pre></div>
</div>
<p>This provides a convenient interface to access counts for unigrams…</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
<span class="go">2</span>
</pre></div>
</div>
<p>…and bigrams (in this case “a b”)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<p>And so on. However, the real purpose of training a language model is to have it
score how probable words are in certain contexts.
This being MLE, the model returns the item’s relative frequency as its score.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.15384615384615385</span>
</pre></div>
</div>
<p>Items that are not seen during training are mapped to the vocabulary’s
“unknown label” token. This is “&lt;UNK&gt;” by default.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Here’s how you get the score for a word given some preceding context.
For example we want to know what is the chance that “b” is preceded by “a”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
<p>To avoid underflow when working with many small score values it makes sense to
take their logarithm.
For convenience this can be done with the <cite>logscore</cite> method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">logscore</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">-2.700439718141092</span>
</pre></div>
</div>
<p>Building on this method, we can also evaluate our model’s cross-entropy and
perplexity with respect to sequences of ngrams.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">test</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="go">1.292481250360578</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="go">2.449489742783178</span>
</pre></div>
</div>
<p>It is advisable to preprocess your test text exactly the same way as you did
the training text.</p>
<p>One cool feature of ngram models is that they can be used to generate text.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">&#39;&lt;s&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>Provide <cite>random_seed</cite> if you want to consistently reproduce the same text all
other things being equal. Here we are using it to test the examples.</p>
<p>You can also condition your generation on some preceding text with the <cite>context</cite>
argument.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">text_seed</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">[&#39;&lt;/s&gt;&#39;, &#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;]</span>
</pre></div>
</div>
<p>Note that an ngram model is restricted in how much preceding context it can
take into account. For example, a trigram model can only condition its output
on 2 preceding words. If you pass in a 4-word context, the first two words
will be ignored.</p>
<dl class="class">
<dt id="nltk.lm.Vocabulary">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">Vocabulary</code><span class="sig-paren">(</span><em>counts=None</em>, <em>unk_cutoff=1</em>, <em>unk_label='&lt;UNK&gt;'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores language model vocabulary.</p>
<p>Satisfies two common language modeling requirements for a vocabulary:
- When checking membership and calculating its size, filters items</p>
<blockquote>
<div>by comparing their counts to a cutoff value.</div></blockquote>
<ul class="simple">
<li>Adds a special “unknown” token which unseen words are mapped to.</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Tokens with counts greater than or equal to the cutoff value will
be considered part of the vocabulary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;c&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;d&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Tokens with frequency counts less than the cutoff value will be considered not
part of the vocabulary even though their entries in the count dictionary are
preserved.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;b&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;aliens&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Keeping the count entries for seen words allows us to change the cutoff value
without having to recalculate the counts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab2</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;b&quot;</span> <span class="ow">in</span> <span class="n">vocab2</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The cutoff value influences not only membership checking but also the result of
getting the size of the vocabulary using the built-in <cite>len</cite>.
Note that while the number of keys in the vocabulary’s counter stays the same,
the items in the vocabulary differ depending on the cutoff.
We use <cite>sorted</cite> to demonstrate because it keeps the order consistent.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">[&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
<p>In addition to items it gets populated with, the vocabulary stores a special
token that stands in for so-called “unknown” items. By default it’s “&lt;UNK&gt;”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;&lt;UNK&gt;&quot;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>We can look up words in a vocabulary using its <cite>lookup</cite> method.
“Unseen” words (with counts less than cutoff) are looked up as the unknown label.
If given one word (a string) as an input, this method will return a string.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
</pre></div>
</div>
<p>If given a sequence, it will return an tuple of the looked up words.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="go">(&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;&lt;UNK&gt;&#39;, &#39;d&#39;, &#39;&lt;UNK&gt;&#39;, &#39;c&#39;)</span>
</pre></div>
</div>
<p>It’s possible to update the counts after the vocabulary has been created.
The interface follows that of <cite>collections.Counter</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">3</span>
</pre></div>
</div>
<dl class="attribute">
<dt id="nltk.lm.Vocabulary.cutoff">
<code class="descname">cutoff</code><a class="headerlink" href="#nltk.lm.Vocabulary.cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Cutoff value.</p>
<p>Items with count below this value are not considered part of vocabulary.</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.Vocabulary.lookup">
<code class="descname">lookup</code><span class="sig-paren">(</span><em>words</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.lookup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Vocabulary.lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Look up one or more words in the vocabulary.</p>
<p>If passed one word as a string will return that word or <cite>self.unk_label</cite>.
Otherwise will assume it was passed a sequence of words, will try to look
each of them up and return an iterator over the looked up words.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>words</strong> (<em>Iterable</em><em>(</em><em>str</em><em>) or </em><em>str</em>) – Word(s) to look up.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">generator(str) or str</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body">TypeError for types other than strings or iterables</td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">]])</span>
<span class="go">(&#39;a&#39;, &#39;b&#39;, &#39;&lt;UNK&gt;&#39;, (&#39;&lt;UNK&gt;&#39;, &#39;b&#39;))</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="nltk.lm.Vocabulary.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.Vocabulary.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.Vocabulary.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>*counter_args</em>, <em>**counter_kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Vocabulary.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update vocabulary counts.</p>
<p>Wraps <cite>collections.Counter.update</cite> method.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.NgramCounter">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">NgramCounter</code><span class="sig-paren">(</span><em>ngram_text=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.NgramCounter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for counting ngrams.</p>
<p>Will count any ngram sequence you give it ;)</p>
<p>First we need to make sure we are feeding the counter sentences of ngrams.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="k">import</span> <span class="n">ngrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_bigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_unigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
</pre></div>
</div>
<p>The counting itself is very simple.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">(</span><span class="n">text_bigrams</span> <span class="o">+</span> <span class="n">text_unigrams</span><span class="p">)</span>
</pre></div>
</div>
<p>You can conveniently access ngram counts using standard python dictionary notation.
String keys will give you unigram counts.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
</pre></div>
</div>
<p>If you want to access counts for higher order ngrams, use a list or a tuple.
These are treated as “context” keys, so what you get is a frequency distribution
over all continuations after the given context.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[(</span><span class="s1">&#39;a&#39;</span><span class="p">,)]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
</pre></div>
</div>
<p>This is equivalent to specifying explicitly the order of the ngram (in this case
2 for bigram) and indexing on the context.
&gt;&gt;&gt; ngram_counts[2][(‘a’,)] is ngram_counts[[‘a’]]
True</p>
<p>Note that the keys in <cite>ConditionalFreqDist</cite> cannot be lists, only tuples!
It is generally advisable to use the less verbose and more flexible square
bracket notation.</p>
<p>To get the count of the full ngram “a b”, do this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Specifying the ngram order as a number can be useful for accessing all ngrams
in that order.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="go">&lt;ConditionalFreqDist with 4 conditions&gt;</span>
</pre></div>
</div>
<p>The keys of this <cite>ConditionalFreqDist</cite> are the contexts we discussed earlier.
Unigrams can also be accessed with a human-friendly alias.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">unigrams</span> <span class="ow">is</span> <span class="n">ngram_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Similarly to <cite>collections.Counter</cite>, you can update counts after initialization.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="n">ngrams</span><span class="p">([</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<dl class="method">
<dt id="nltk.lm.NgramCounter.N">
<code class="descname">N</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.N"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.NgramCounter.N" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns grand total number of ngrams stored.</p>
<p>This includes ngrams from all orders, so some duplication is expected.
:rtype: int</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="k">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">([[(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">)]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span><span class="o">.</span><span class="n">N</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="nltk.lm.NgramCounter.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.NgramCounter.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.NgramCounter.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>ngram_text</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.NgramCounter.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates ngram counts from <cite>ngram_text</cite>.</p>
<p>Expects <cite>ngram_text</cite> to be a sequence of sentences (sequences).
Each sentence consists of ngrams as tuples of strings.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>ngram_text</strong> (<em>Iterable</em><em>(</em><em>Iterable</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>)</em><em>)</em><em>)</em>) – Text containing senteces of ngrams.</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><strong>TypeError</strong> – if the ngrams are not tuples.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.MLE">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">MLE</code><span class="sig-paren">(</span><em>order</em>, <em>vocabulary=None</em>, <em>counter=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.MLE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.LanguageModel</span></code></a></p>
<p>Class for providing MLE ngram model scores.</p>
<p>Inherits initialization from BaseNgramModel.</p>
<dl class="attribute">
<dt id="nltk.lm.MLE.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.MLE.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.MLE.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.MLE.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the MLE score for a word given a context.</p>
<p>Args:
- word is expcected to be a string
- context is expected to be something reasonably convertible to a tuple</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.Lidstone">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">Lidstone</code><span class="sig-paren">(</span><em>gamma</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Lidstone" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.api.LanguageModel</span></code></a></p>
<p>Provides Lidstone-smoothed scores.</p>
<p>In addition to initialization arguments from BaseNgramModel also requires
a number by which to increase the counts, gamma.</p>
<dl class="attribute">
<dt id="nltk.lm.Lidstone.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.Lidstone.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

<dl class="method">
<dt id="nltk.lm.Lidstone.unmasked_score">
<code class="descname">unmasked_score</code><span class="sig-paren">(</span><em>word</em>, <em>context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone.unmasked_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Lidstone.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Add-one smoothing: Lidstone or Laplace.</p>
<p>To see what kind, look at <cite>gamma</cite> attribute on the class.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.Laplace">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">Laplace</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Laplace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.Laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.Lidstone" title="nltk.lm.models.Lidstone"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.Lidstone</span></code></a></p>
<p>Implements Laplace (add one) smoothing.</p>
<p>Initialization identical to BaseNgramModel because gamma is always 1.</p>
<dl class="attribute">
<dt id="nltk.lm.Laplace.unicode_repr">
<code class="descname">unicode_repr</code><a class="headerlink" href="#nltk.lm.Laplace.unicode_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Return repr(self).</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="nltk.lm.WittenBellInterpolated">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">WittenBellInterpolated</code><span class="sig-paren">(</span><em>order</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#WittenBellInterpolated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.WittenBellInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Witten-Bell smoothing.</p>
</dd></dl>

<dl class="class">
<dt id="nltk.lm.KneserNeyInterpolated">
<em class="property">class </em><code class="descclassname">nltk.lm.</code><code class="descname">KneserNeyInterpolated</code><span class="sig-paren">(</span><em>order</em>, <em>discount=0.1</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#KneserNeyInterpolated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#nltk.lm.KneserNeyInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">nltk.lm.models.InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Kneser-Ney smoothing.</p>
</dd></dl>

</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table Of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">NLTK News</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to NLTK</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference internal" href="nltk.html">API</a></li>
<li class="toctree-l1"><a class="reference external" href="http://www.nltk.org/howto">HOWTO</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
            </form>
          </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
            <a href="../py-modindex.html" title="Python Module Index"
              >modules</a> |
            <a href="../genindex.html" title="General Index"
              >index</a>
          </div>
          <div role="note" aria-label="source link">
              <br/>
              <a href="../_sources/api/nltk.lm.rst.txt"
                rel="nofollow">Show Source</a>
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, NLTK Project.
      Last updated on Nov 17, 2018.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.9.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>