<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>NLTK :: nltk.lm package</title>
  

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">NLTK</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">NLTK Documentation</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="nltk.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto.html">Example Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk">NLTK on GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing to NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../team.html">NLTK Team</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <section id="nltk-lm-package">
<h1>nltk.lm package<a class="headerlink" href="#nltk-lm-package" title="Permalink to this heading">¶</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.api.html">nltk.lm.api module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.counter.html">nltk.lm.counter module</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nltk.lm.counter.html#language-model-counter">Language Model Counter</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.models.html">nltk.lm.models module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.preprocessing.html">nltk.lm.preprocessing module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.smoothing.html">nltk.lm.smoothing module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nltk.lm.util.html">nltk.lm.util module</a></li>
</ul>
</div>
</section>
<section id="module-nltk.lm">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-nltk.lm" title="Permalink to this heading">¶</a></h2>
<section id="nltk-language-modeling-module">
<h3>NLTK Language Modeling Module.<a class="headerlink" href="#nltk-language-modeling-module" title="Permalink to this heading">¶</a></h3>
<p>Currently this module covers only ngram language models, but it should be easy
to extend to neural models.</p>
<section id="preparing-data">
<h4>Preparing Data<a class="headerlink" href="#preparing-data" title="Permalink to this heading">¶</a></h4>
<p>Before we train our ngram models it is necessary to make sure the data we put in
them is in the right format.
Let’s say we have a text that is a list of sentences, where each sentence is
a list of strings. For simplicity we just consider a text consisting of
characters instead of words.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;e&#39;</span><span class="p">,</span> <span class="s1">&#39;f&#39;</span><span class="p">]]</span>
</pre></div>
</div>
<p>If we want to train a bigram model, we need to turn this text into bigrams.
Here’s what the first sentence of our text would look like if we use a function
from NLTK for this.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">bigrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bigrams</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="go">[(&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;)]</span>
</pre></div>
</div>
<p>Notice how “b” occurs both as the first and second member of different bigrams
but “a” and “c” don’t? Wouldn’t it be nice to somehow indicate how often sentences
start with “a” and end with “c”?
A standard way to deal with this is to add special “padding” symbols to the
sentence before splitting it into ngrams.
Fortunately, NLTK also has a function for that, let’s see what it does to the
first sentence.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span><span class="n">pad_left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="n">left_pad_symbol</span><span class="o">=</span><span class="s2">&quot;&lt;s&gt;&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="n">pad_right</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="n">right_pad_symbol</span><span class="o">=</span><span class="s2">&quot;&lt;/s&gt;&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>Note the <cite>n</cite> argument, that tells the function we need padding for bigrams.
Now, passing all these parameters every time is tedious and in most cases they
can be safely assumed as defaults anyway.
Thus our module provides a convenience function that has all these arguments
already set while the other arguments remain the same as for <cite>pad_sequence</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="kn">import</span> <span class="n">pad_both_ends</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>Combining the two parts discussed so far we get the following preparation steps
for one sentence.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">bigrams</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)))</span>
<span class="go">[(&#39;&lt;s&gt;&#39;, &#39;a&#39;), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;, &#39;c&#39;), (&#39;c&#39;, &#39;&lt;/s&gt;&#39;)]</span>
</pre></div>
</div>
<p>To make our model more robust we could also train it on unigrams (single words)
as well as bigrams, its main source of information.
NLTK once again helpfully provides a function called <cite>everygrams</cite>.
While not the most efficient, it is conceptually simple.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">everygrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">padded_bigrams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">everygrams</span><span class="p">(</span><span class="n">padded_bigrams</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="go">[(&#39;&lt;s&gt;&#39;,), (&#39;&lt;s&gt;&#39;, &#39;a&#39;), (&#39;a&#39;,), (&#39;a&#39;, &#39;b&#39;), (&#39;b&#39;,), (&#39;b&#39;, &#39;c&#39;), (&#39;c&#39;,), (&#39;c&#39;, &#39;&lt;/s&gt;&#39;), (&#39;&lt;/s&gt;&#39;,)]</span>
</pre></div>
</div>
<p>We are almost ready to start counting ngrams, just one more step left.
During training and evaluation our model will rely on a vocabulary that
defines which words are “known” to the model.
To create this vocabulary we need to pad our sentences (just like for counting
ngrams) and then combine the sentences into one flat stream of words.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="kn">import</span> <span class="n">flatten</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">pad_both_ends</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">))</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;&lt;/s&gt;&#39;, &#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;, &#39;&lt;/s&gt;&#39;]</span>
</pre></div>
</div>
<p>In most cases we want to use the same text as the source for both vocabulary
and ngram counts.
Now that we understand what this means for our preprocessing, we can simply import
a function that does everything for us.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm.preprocessing</span> <span class="kn">import</span> <span class="n">padded_everygram_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="n">padded_everygram_pipeline</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>So as to avoid re-creating the text in memory, both <cite>train</cite> and <cite>vocab</cite> are lazy
iterators. They are evaluated on demand at training time.</p>
</section>
<section id="training">
<h4>Training<a class="headerlink" href="#training" title="Permalink to this heading">¶</a></h4>
<p>Having prepared our data we are ready to start training a model.
As a simple example, let us train a Maximum Likelihood Estimator (MLE).
We only need to specify the highest ngram order to instantiate it.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">MLE</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span> <span class="o">=</span> <span class="n">MLE</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>This automatically creates an empty vocabulary…</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">0</span>
</pre></div>
</div>
<p>… which gets filled as we fit the model.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">&lt;Vocabulary with cutoff=1 unk_label=&#39;&lt;UNK&gt;&#39; and 9 items&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">9</span>
</pre></div>
</div>
<p>The vocabulary helps us handle words that have not occurred during training.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="go">(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;aliens&quot;</span><span class="p">,</span> <span class="s2">&quot;from&quot;</span><span class="p">,</span> <span class="s2">&quot;Mars&quot;</span><span class="p">])</span>
<span class="go">(&#39;&lt;UNK&gt;&#39;, &#39;&lt;UNK&gt;&#39;, &#39;&lt;UNK&gt;&#39;)</span>
</pre></div>
</div>
<p>Moreover, in some cases we want to ignore words that we did see during training
but that didn’t occur frequently enough, to provide us useful information.
You can tell the vocabulary to ignore such words.
To find out how that works, check out the docs for the <cite>Vocabulary</cite> class.</p>
</section>
<section id="using-a-trained-model">
<h4>Using a Trained Model<a class="headerlink" href="#using-a-trained-model" title="Permalink to this heading">¶</a></h4>
<p>When it comes to ngram models the training boils down to counting up the ngrams
from the training corpus.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">&lt;NgramCounter with 2 ngram orders and 24 ngrams&gt;</span>
</pre></div>
</div>
<p>This provides a convenient interface to access counts for unigrams…</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
<span class="go">2</span>
</pre></div>
</div>
<p>…and bigrams (in this case “a b”)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<p>And so on. However, the real purpose of training a language model is to have it
score how probable words are in certain contexts.
This being MLE, the model returns the item’s relative frequency as its score.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">0.15384615384615385</span>
</pre></div>
</div>
<p>Items that are not seen during training are mapped to the vocabulary’s
“unknown label” token. This is “&lt;UNK&gt;” by default.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Here’s how you get the score for a word given some preceding context.
For example we want to know what is the chance that “b” is preceded by “a”.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">])</span>
<span class="go">0.5</span>
</pre></div>
</div>
<p>To avoid underflow when working with many small score values it makes sense to
take their logarithm.
For convenience this can be done with the <cite>logscore</cite> method.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">logscore</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">-2.700439718141092</span>
</pre></div>
</div>
<p>Building on this method, we can also evaluate our model’s cross-entropy and
perplexity with respect to sequences of ngrams.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">test</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="go">1.292481250360578</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="go">2.449489742783178</span>
</pre></div>
</div>
<p>It is advisable to preprocess your test text exactly the same way as you did
the training text.</p>
<p>One cool feature of ngram models is that they can be used to generate text.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">&#39;&lt;s&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">[&#39;&lt;s&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
<p>Provide <cite>random_seed</cite> if you want to consistently reproduce the same text all
other things being equal. Here we are using it to test the examples.</p>
<p>You can also condition your generation on some preceding text with the <cite>context</cite>
argument.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">text_seed</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">],</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="go">[&#39;&lt;/s&gt;&#39;, &#39;c&#39;, &#39;d&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
<p>Note that an ngram model is restricted in how much preceding context it can
take into account. For example, a trigram model can only condition its output
on 2 preceding words. If you pass in a 4-word context, the first two words
will be ignored.</p>
<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.AbsoluteDiscountingInterpolated">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">AbsoluteDiscountingInterpolated</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#AbsoluteDiscountingInterpolated"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.AbsoluteDiscountingInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.models.html#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of smoothing with absolute discount.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.AbsoluteDiscountingInterpolated.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">order</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.75</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#AbsoluteDiscountingInterpolated.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.AbsoluteDiscountingInterpolated.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.KneserNeyInterpolated">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">KneserNeyInterpolated</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#KneserNeyInterpolated"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.KneserNeyInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.models.html#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Kneser-Ney smoothing.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.KneserNeyInterpolated.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">order</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">discount</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#KneserNeyInterpolated.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.KneserNeyInterpolated.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.Laplace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">Laplace</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#Laplace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Laplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.models.html#nltk.lm.models.Lidstone" title="nltk.lm.models.Lidstone"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lidstone</span></code></a></p>
<p>Implements Laplace (add one) smoothing.</p>
<p>Initialization identical to BaseNgramModel because gamma is always 1.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Laplace.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Laplace.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Laplace.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.Lidstone">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">Lidstone</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Lidstone" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.api.html#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LanguageModel</span></code></a></p>
<p>Provides Lidstone-smoothed scores.</p>
<p>In addition to initialization arguments from BaseNgramModel also requires
a number by which to increase the counts, gamma.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Lidstone.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Lidstone.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Lidstone.unmasked_score">
<span class="sig-name descname"><span class="pre">unmasked_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#Lidstone.unmasked_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Lidstone.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Add-one smoothing: Lidstone or Laplace.</p>
<p>To see what kind, look at <cite>gamma</cite> attribute on the class.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.MLE">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">MLE</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.MLE" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.api.html#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LanguageModel</span></code></a></p>
<p>Class for providing MLE ngram model scores.</p>
<p>Inherits initialization from BaseNgramModel.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.MLE.unmasked_score">
<span class="sig-name descname"><span class="pre">unmasked_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#MLE.unmasked_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.MLE.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the MLE score for a word given a context.</p>
<p>Args:
- word is expected to be a string
- context is expected to be something reasonably convertible to a tuple</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.NgramCounter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">NgramCounter</span></span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.NgramCounter" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class for counting ngrams.</p>
<p>Will count any ngram sequence you give it ;)</p>
<p>First we need to make sure we are feeding the counter sentences of ngrams.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">ngrams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_bigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_unigrams</span> <span class="o">=</span> <span class="p">[</span><span class="n">ngrams</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">text</span><span class="p">]</span>
</pre></div>
</div>
<p>The counting itself is very simple.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">(</span><span class="n">text_bigrams</span> <span class="o">+</span> <span class="n">text_unigrams</span><span class="p">)</span>
</pre></div>
</div>
<p>You can conveniently access ngram counts using standard python dictionary notation.
String keys will give you unigram counts.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
</pre></div>
</div>
<p>If you want to access counts for higher order ngrams, use a list or a tuple.
These are treated as “context” keys, so what you get is a frequency distribution
over all continuations after the given context.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">ngram_counts</span><span class="p">[(</span><span class="s1">&#39;a&#39;</span><span class="p">,)]</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
<span class="go">[(&#39;b&#39;, 1), (&#39;c&#39;, 1)]</span>
</pre></div>
</div>
<p>This is equivalent to specifying explicitly the order of the ngram (in this case
2 for bigram) and indexing on the context.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="mi">2</span><span class="p">][(</span><span class="s1">&#39;a&#39;</span><span class="p">,)]</span> <span class="ow">is</span> <span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]]</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Note that the keys in <cite>ConditionalFreqDist</cite> cannot be lists, only tuples!
It is generally advisable to use the less verbose and more flexible square
bracket notation.</p>
<p>To get the count of the full ngram “a b”, do this:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[[</span><span class="s1">&#39;a&#39;</span><span class="p">]][</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Specifying the ngram order as a number can be useful for accessing all ngrams
in that order.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="go">&lt;ConditionalFreqDist with 4 conditions&gt;</span>
</pre></div>
</div>
<p>The keys of this <cite>ConditionalFreqDist</cite> are the contexts we discussed earlier.
Unigrams can also be accessed with a human-friendly alias.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">unigrams</span> <span class="ow">is</span> <span class="n">ngram_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Similarly to <cite>collections.Counter</cite>, you can update counts after initialization.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="n">ngrams</span><span class="p">([</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">],</span> <span class="mi">1</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_counts</span><span class="p">[</span><span class="s1">&#39;e&#39;</span><span class="p">]</span>
<span class="go">1</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.NgramCounter.N">
<span class="sig-name descname"><span class="pre">N</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.N"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.NgramCounter.N" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns grand total number of ngrams stored.</p>
<p>This includes ngrams from all orders, so some duplication is expected.
:rtype: int</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">NgramCounter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="n">NgramCounter</span><span class="p">([[(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;c&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;d&quot;</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">)]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span><span class="o">.</span><span class="n">N</span><span class="p">()</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.NgramCounter.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ngram_text</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.NgramCounter.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a new NgramCounter.</p>
<p>If <cite>ngram_text</cite> is specified, counts ngrams from it, otherwise waits for
<cite>update</cite> method to be called explicitly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ngram_text</strong> (<em>Iterable</em><em>(</em><em>Iterable</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>)</em><em>)</em><em>) or </em><em>None</em>) – Optional text containing sentences of ngrams, as for <cite>update</cite> method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.NgramCounter.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ngram_text</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/counter.html#NgramCounter.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.NgramCounter.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates ngram counts from <cite>ngram_text</cite>.</p>
<p>Expects <cite>ngram_text</cite> to be a sequence of sentences (sequences).
Each sentence consists of ngrams as tuples of strings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ngram_text</strong> (<em>Iterable</em><em>(</em><em>Iterable</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>)</em><em>)</em><em>)</em>) – Text containing sentences of ngrams.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> – if the ngrams are not tuples.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.StupidBackoff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">StupidBackoff</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#StupidBackoff"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.StupidBackoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.api.html#nltk.lm.api.LanguageModel" title="nltk.lm.api.LanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">LanguageModel</span></code></a></p>
<p>Provides StupidBackoff scores.</p>
<p>In addition to initialization arguments from BaseNgramModel also requires
a parameter alpha with which we scale the lower order probabilities.
Note that this is not a true probability distribution as scores for ngrams
of the same order do not sum up to unity.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.StupidBackoff.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.4</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#StupidBackoff.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.StupidBackoff.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.StupidBackoff.unmasked_score">
<span class="sig-name descname"><span class="pre">unmasked_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#StupidBackoff.unmasked_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.StupidBackoff.unmasked_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Score a word given some optional context.</p>
<p>Concrete models are expected to provide an implementation.
Note that this method does not mask its arguments with the OOV label.
Use the <cite>score</cite> method for that.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>word</strong> (<em>str</em>) – Word for which we want the score</p></li>
<li><p><strong>context</strong> (<em>tuple</em><em>(</em><em>str</em><em>)</em>) – Context the word is in.
If <cite>None</cite>, compute unigram score.</p></li>
<li><p><strong>context</strong> – tuple(str) or None</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.Vocabulary">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">Vocabulary</span></span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Vocabulary" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stores language model vocabulary.</p>
<p>Satisfies two common language modeling requirements for a vocabulary:</p>
<ul class="simple">
<li><p>When checking membership and calculating its size, filters items
by comparing their counts to a cutoff value.</p></li>
<li><p>Adds a special “unknown” token which unseen words are mapped to.</p></li>
</ul>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Tokens with counts greater than or equal to the cutoff value will
be considered part of the vocabulary.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;c&#39;</span><span class="p">]</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;c&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;d&#39;</span><span class="p">]</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;d&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Tokens with frequency counts less than the cutoff value will be considered not
part of the vocabulary even though their entries in the count dictionary are
preserved.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;b&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;aliens&#39;</span><span class="p">]</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;aliens&#39;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Keeping the count entries for seen words allows us to change the cutoff value
without having to recalculate the counts.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab2</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;b&quot;</span> <span class="ow">in</span> <span class="n">vocab2</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The cutoff value influences not only membership checking but also the result of
getting the size of the vocabulary using the built-in <cite>len</cite>.
Note that while the number of keys in the vocabulary’s counter stays the same,
the items in the vocabulary differ depending on the cutoff.
We use <cite>sorted</cite> to demonstrate because it keeps the order consistent.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab2</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">counts</span><span class="p">)</span>
<span class="go">[&#39;-&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;r&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sorted</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="go">[&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;c&#39;, &#39;d&#39;]</span>
</pre></div>
</div>
<p>In addition to items it gets populated with, the vocabulary stores a special
token that stands in for so-called “unknown” items. By default it’s “&lt;UNK&gt;”.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="s2">&quot;&lt;UNK&gt;&quot;</span> <span class="ow">in</span> <span class="n">vocab</span>
<span class="go">True</span>
</pre></div>
</div>
<p>We can look up words in a vocabulary using its <cite>lookup</cite> method.
“Unseen” words (with counts less than cutoff) are looked up as the unknown label.
If given one word (a string) as an input, this method will return a string.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
</pre></div>
</div>
<p>If given a sequence, it will return an tuple of the looked up words.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">])</span>
<span class="go">(&#39;&lt;UNK&gt;&#39;, &#39;a&#39;, &#39;&lt;UNK&gt;&#39;, &#39;d&#39;, &#39;&lt;UNK&gt;&#39;, &#39;c&#39;)</span>
</pre></div>
</div>
<p>It’s possible to update the counts after the vocabulary has been created.
In general, the interface is the same as that of <cite>collections.Counter</cite>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">update</span><span class="p">([</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">]</span>
<span class="go">3</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Vocabulary.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">counts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unk_cutoff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unk_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'&lt;UNK&gt;'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Vocabulary.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a new Vocabulary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>counts</strong> – Optional iterable or <cite>collections.Counter</cite> instance to
pre-seed the Vocabulary. In case it is iterable, counts
are calculated.</p></li>
<li><p><strong>unk_cutoff</strong> (<em>int</em>) – Words that occur less frequently than this value
are not considered part of the vocabulary.</p></li>
<li><p><strong>unk_label</strong> – Label for marking words not part of vocabulary.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="nltk.lm.Vocabulary.cutoff">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">cutoff</span></span><a class="headerlink" href="#nltk.lm.Vocabulary.cutoff" title="Permalink to this definition">¶</a></dt>
<dd><p>Cutoff value.</p>
<p>Items with count below this value are not considered part of vocabulary.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Vocabulary.lookup">
<span class="sig-name descname"><span class="pre">lookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">words</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.lookup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Vocabulary.lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Look up one or more words in the vocabulary.</p>
<p>If passed one word as a string will return that word or <cite>self.unk_label</cite>.
Otherwise will assume it was passed a sequence of words, will try to look
each of them up and return an iterator over the looked up words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>words</strong> (<em>Iterable</em><em>(</em><em>str</em><em>) or </em><em>str</em>) – Word(s) to look up.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>generator(str) or str</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p>TypeError for types other than strings or iterables</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.lm</span> <span class="kn">import</span> <span class="n">Vocabulary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">Vocabulary</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">unk_cutoff</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">)</span>
<span class="go">&#39;a&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">(</span><span class="s2">&quot;aliens&quot;</span><span class="p">)</span>
<span class="go">&#39;&lt;UNK&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span><span class="o">.</span><span class="n">lookup</span><span class="p">([</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">]])</span>
<span class="go">(&#39;a&#39;, &#39;b&#39;, &#39;&lt;UNK&gt;&#39;, (&#39;&lt;UNK&gt;&#39;, &#39;b&#39;))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.Vocabulary.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">counter_args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">counter_kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/vocabulary.html#Vocabulary.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.Vocabulary.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update vocabulary counts.</p>
<p>Wraps <cite>collections.Counter.update</cite> method.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="nltk.lm.WittenBellInterpolated">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.lm.</span></span><span class="sig-name descname"><span class="pre">WittenBellInterpolated</span></span><a class="reference internal" href="../_modules/nltk/lm/models.html#WittenBellInterpolated"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.WittenBellInterpolated" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.lm.models.html#nltk.lm.models.InterpolatedLanguageModel" title="nltk.lm.models.InterpolatedLanguageModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">InterpolatedLanguageModel</span></code></a></p>
<p>Interpolated version of Witten-Bell smoothing.</p>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.lm.WittenBellInterpolated.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">order</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/lm/models.html#WittenBellInterpolated.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.lm.WittenBellInterpolated.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates new LanguageModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocabulary</strong> (<cite>nltk.lm.Vocabulary</cite> or None) – If provided, this vocabulary will be used instead
of creating a new one when training.</p></li>
<li><p><strong>counter</strong> (<cite>nltk.lm.NgramCounter</cite> or None) – If provided, use this object to count ngrams.</p></li>
<li><p><strong>ngrams_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are turned to ngram
sequences.</p></li>
<li><p><strong>pad_fn</strong> (<em>function</em><em> or </em><em>None</em>) – If given, defines how sentences in training text are padded.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>
</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/api/nltk.lm.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            
                <li class="footer-element">
                    <a href="https://github.com/nltk/nltk/tree/3.8.1">3.8.1</a>
                </li>
            

            
                <li class="footer-element">
                    Jan 02, 2023
                </li>
            
        </ul>

        
            <div id="copyright">
                &copy; 2023, NLTK Project
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>