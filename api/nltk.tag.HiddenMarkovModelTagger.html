<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>NLTK :: nltk.tag.HiddenMarkovModelTagger</title>
  

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">NLTK</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">NLTK Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="nltk.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../howto.html">Example Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk">NLTK on GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing to NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../team.html">NLTK Team</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <section id="nltk-tag-hiddenmarkovmodeltagger">
<h1>nltk.tag.HiddenMarkovModelTagger<a class="headerlink" href="#nltk-tag-hiddenmarkovmodeltagger" title="Permalink to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">nltk.tag.</span></span><span class="sig-name descname"><span class="pre">HiddenMarkovModelTagger</span></span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="nltk.tag.api.html#nltk.tag.api.TaggerI" title="nltk.tag.api.TaggerI"><code class="xref py py-class docutils literal notranslate"><span class="pre">TaggerI</span></code></a></p>
<p>Hidden Markov model class, a generative model for labelling sequence data.
These models define the joint probability of a sequence of symbols and
their labels (state transitions) as the product of the starting state
probability, the probability of each state transition, and the probability
of each observation being generated from each state. This is described in
more detail in the module documentation.</p>
<p>This implementation is based on the HMM description in Chapter 8, Huang,
Acero and Hon, Spoken Language Processing and includes an extension for
training shallow HMM parsers or specialized HMMs as in Molina et.
al, 2002.  A specialized HMM modifies training data by applying a
specialization function to create a new training set that is more
appropriate for sequential tagging with an HMM.  A typical use case is
chunking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>symbols</strong> (<em>seq of any</em>) – the set of output symbols (alphabet)</p></li>
<li><p><strong>states</strong> (<em>seq of any</em>) – a set of states representing state space</p></li>
<li><p><strong>transitions</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ConditionalProbDistI" title="nltk.probability.ConditionalProbDistI"><em>ConditionalProbDistI</em></a>) – transition probabilities; Pr(s_i | s_j) is the
probability of transition from state i given the model is in
state_j</p></li>
<li><p><strong>outputs</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ConditionalProbDistI" title="nltk.probability.ConditionalProbDistI"><em>ConditionalProbDistI</em></a>) – output probabilities; Pr(o_k | s_i) is the probability
of emitting symbol k when entering state i</p></li>
<li><p><strong>priors</strong> (<a class="reference internal" href="nltk.probability.html#nltk.probability.ProbDistI" title="nltk.probability.ProbDistI"><em>ProbDistI</em></a>) – initial state distribution; Pr(s_i) is the probability
of starting in state i</p></li>
<li><p><strong>transform</strong> (<em>callable</em>) – an optional function for transforming training
instances, defaults to the identity function.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">symbols</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transitions</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transform=&lt;function</span> <span class="pre">_identity&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.__init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.train">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labeled_sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_sequence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Train a new HiddenMarkovModelTagger using the given labeled and
unlabeled training instances. Testing will be performed if test
instances are provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a hidden markov model tagger</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="#nltk.tag.HiddenMarkovModelTagger" title="nltk.tag.HiddenMarkovModelTagger">HiddenMarkovModelTagger</a></p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labeled_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled training instances,
i.e. a list of sentences represented as tuples</p></li>
<li><p><strong>test_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled test instances</p></li>
<li><p><strong>unlabeled_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of unlabeled training instances,
i.e. a list of sentences represented as words</p></li>
<li><p><strong>transform</strong> (<em>function</em>) – an optional function for transforming training
instances, defaults to the identity function, see <code class="docutils literal notranslate"><span class="pre">transform()</span></code></p></li>
<li><p><strong>estimator</strong> (<em>class</em><em> or </em><em>function</em>) – an optional function or class that maps a
condition’s frequency distribution to its probability
distribution, defaults to a Lidstone distribution with gamma = 0.1</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – boolean flag indicating whether training should be
verbose or include printed output</p></li>
<li><p><strong>max_iterations</strong> (<em>int</em>) – number of Baum-Welch iterations to perform</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.probability">
<span class="sig-name descname"><span class="pre">probability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.probability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.probability" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the probability of the given symbol sequence. If the sequence
is labelled, then returns the joint probability of the symbol, state
sequence. Otherwise, uses the forward algorithm to find the
probability over all label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the probability of the sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sequence</strong> (<a class="reference internal" href="nltk.ccg.lexicon.html#nltk.ccg.lexicon.Token" title="nltk.ccg.lexicon.Token"><em>Token</em></a>) – the sequence of symbols which must contain the TEXT
property, and optionally the TAG property</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.log_probability">
<span class="sig-name descname"><span class="pre">log_probability</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.log_probability"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.log_probability" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the log-probability of the given symbol sequence. If the
sequence is labelled, then returns the joint log-probability of the
symbol, state sequence. Otherwise, uses the forward algorithm to find
the log-probability over all label sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the log-probability of the sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sequence</strong> (<a class="reference internal" href="nltk.ccg.lexicon.html#nltk.ccg.lexicon.Token" title="nltk.ccg.lexicon.Token"><em>Token</em></a>) – the sequence of symbols which must contain the TEXT
property, and optionally the TAG property</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.tag">
<span class="sig-name descname"><span class="pre">tag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.tag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.tag" title="Permalink to this definition">¶</a></dt>
<dd><p>Tags the sequence with the highest probability state sequence. This
uses the best_path method to find the Viterbi path.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a labelled sequence of symbols</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.reset_cache">
<span class="sig-name descname"><span class="pre">reset_cache</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.reset_cache"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.reset_cache" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.best_path">
<span class="sig-name descname"><span class="pre">best_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.best_path"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.best_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the state sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>sequence of any</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.best_path_simple">
<span class="sig-name descname"><span class="pre">best_path_simple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.best_path_simple"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.best_path_simple" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the state sequence of the optimal (most probable) path through
the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
programming.  This uses a simple, direct method, and is included for
teaching purposes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the state sequence</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>sequence of any</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>unlabeled_sequence</strong> (<em>list</em>) – the sequence of unlabeled symbols</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.random_sample">
<span class="sig-name descname"><span class="pre">random_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rng</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.random_sample"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.random_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Randomly sample the HMM to generate a sentence of a given length. This
samples the prior distribution then the observation distribution and
transition distribution for each subsequent observation and state.
This will mostly generate unintelligible garbage, but can provide some
amusement.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the randomly created state/observation sequence,
generated according to the HMM’s probability
distributions. The SUBTOKENS have TEXT and TAG
properties containing the observation and state
respectively.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rng</strong> (<em>Random</em><em> (or </em><em>any object with a random</em><em>(</em><em>) </em><em>method</em><em>)</em>) – random number generator</p></li>
<li><p><strong>length</strong> (<em>int</em>) – desired output length</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.entropy">
<span class="sig-name descname"><span class="pre">entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the entropy over labellings of the given sequence. This is
given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span><span class="p">(</span><span class="n">O</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span>
</pre></div>
</div>
<p>where the summation ranges over all state sequences, S. Let
<em>Z = Pr(O) = sum_S Pr(S, O)}</em> where the summation ranges over all state
sequences and O is the observation sequence. As such the entropy can
be re-expressed as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="p">[</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">O</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="p">]</span>
<span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_S</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S</span> <span class="o">|</span> <span class="n">O</span><span class="p">)</span> <span class="p">[</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S_0</span><span class="p">)</span> <span class="o">+</span> <span class="n">sum_t</span> <span class="n">Pr</span><span class="p">(</span><span class="n">S_t</span> <span class="o">|</span> <span class="n">S_</span><span class="p">{</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">})</span> <span class="o">+</span> <span class="n">sum_t</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span> <span class="o">|</span> <span class="n">S_t</span><span class="p">)</span> <span class="p">]</span>
</pre></div>
</div>
<p>The order of summation for the log terms can be flipped, allowing
dynamic programming to be used to calculate the entropy. Specifically,
we use the forward and backward probabilities (alpha, beta) giving:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H</span> <span class="o">=</span> <span class="n">log</span> <span class="n">Z</span> <span class="o">-</span> <span class="n">sum_s0</span> <span class="n">alpha_0</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span> <span class="n">beta_0</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span>
<span class="o">+</span> <span class="n">sum_t</span><span class="p">,</span><span class="n">si</span><span class="p">,</span><span class="n">sj</span> <span class="n">alpha_t</span><span class="p">(</span><span class="n">si</span><span class="p">)</span> <span class="n">Pr</span><span class="p">(</span><span class="n">sj</span> <span class="o">|</span> <span class="n">si</span><span class="p">)</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span><span class="o">+</span><span class="mi">1</span> <span class="o">|</span> <span class="n">sj</span><span class="p">)</span> <span class="n">beta_t</span><span class="p">(</span><span class="n">sj</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">sj</span> <span class="o">|</span> <span class="n">si</span><span class="p">)</span>
<span class="o">+</span> <span class="n">sum_t</span><span class="p">,</span><span class="n">st</span> <span class="n">alpha_t</span><span class="p">(</span><span class="n">st</span><span class="p">)</span> <span class="n">beta_t</span><span class="p">(</span><span class="n">st</span><span class="p">)</span> <span class="o">/</span> <span class="n">Z</span> <span class="o">*</span> <span class="n">log</span> <span class="n">Pr</span><span class="p">(</span><span class="n">O_t</span> <span class="o">|</span> <span class="n">st</span><span class="p">)</span>
</pre></div>
</div>
<p>This simply uses alpha and beta to find the probabilities of partial
sequences, constrained to include the given state(s) at some point in
time.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.point_entropy">
<span class="sig-name descname"><span class="pre">point_entropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">unlabeled_sequence</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.point_entropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.point_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the pointwise entropy over the possible states at each
position in the chain, given the observation sequence.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">test_sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/nltk/tag/hmm.html#HiddenMarkovModelTagger.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.test" title="Permalink to this definition">¶</a></dt>
<dd><p>Tests the HiddenMarkovModelTagger instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>test_sequence</strong> (<em>list</em><em>(</em><em>list</em><em>)</em>) – a sequence of labeled test instances</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – boolean flag indicating whether training should be
verbose or include printed output</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.accuracy">
<span class="sig-name descname"><span class="pre">accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Score the accuracy of the tagger against the gold standard.
Strip the tags from the gold standard text, retag it using
the tagger, then compute the accuracy score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to score the tagger on.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.confusion">
<span class="sig-name descname"><span class="pre">confusion</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.confusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a ConfusionMatrix with the tags from <code class="docutils literal notranslate"><span class="pre">gold</span></code> as the reference
values, with the predictions from <code class="docutils literal notranslate"><span class="pre">tag_sents</span></code> as the predicted values.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tag</span> <span class="kn">import</span> <span class="n">PerceptronTagger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">treebank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tagger</span> <span class="o">=</span> <span class="n">PerceptronTagger</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gold_data</span> <span class="o">=</span> <span class="n">treebank</span><span class="o">.</span><span class="n">tagged_sents</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tagger</span><span class="o">.</span><span class="n">confusion</span><span class="p">(</span><span class="n">gold_data</span><span class="p">))</span>
<span class="go">       |        -                                                                                     |</span>
<span class="go">       |        N                                                                                     |</span>
<span class="go">       |        O                                               P                                     |</span>
<span class="go">       |        N                       J  J        N  N  P  P  R     R           V  V  V  V  V  W    |</span>
<span class="go">       |  &#39;     E     C  C  D  E  I  J  J  J  M  N  N  N  O  R  P  R  B  R  T  V  B  B  B  B  B  D  ` |</span>
<span class="go">       |  &#39;  ,  -  .  C  D  T  X  N  J  R  S  D  N  P  S  S  P  $  B  R  P  O  B  D  G  N  P  Z  T  ` |</span>
<span class="go">-------+----------------------------------------------------------------------------------------------+</span>
<span class="go">    &#39;&#39; | &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">     , |  .&lt;15&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">-NONE- |  .  . &lt;.&gt; .  .  2  .  .  .  2  .  .  .  5  1  .  .  .  .  2  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">     . |  .  .  .&lt;10&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    CC |  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    CD |  .  .  .  .  . &lt;5&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    DT |  .  .  .  .  .  .&lt;20&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    EX |  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    IN |  .  .  .  .  .  .  .  .&lt;22&gt; .  .  .  .  .  .  .  .  .  .  3  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    JJ |  .  .  .  .  .  .  .  .  .&lt;16&gt; .  .  .  .  1  .  .  .  .  1  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   JJR |  .  .  .  .  .  .  .  .  .  . &lt;.&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   JJS |  .  .  .  .  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    MD |  .  .  .  .  .  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    NN |  .  .  .  .  .  .  .  .  .  .  .  .  .&lt;28&gt; 1  1  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   NNP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .&lt;25&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   NNS |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .&lt;19&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   POS |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   PRP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;4&gt; .  .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">  PRP$ |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;2&gt; .  .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    RB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;4&gt; .  .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">   RBR |  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  .  . |</span>
<span class="go">    RP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;1&gt; .  .  .  .  .  .  .  .  . |</span>
<span class="go">    TO |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;5&gt; .  .  .  .  .  .  .  . |</span>
<span class="go">    VB |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;3&gt; .  .  .  .  .  .  . |</span>
<span class="go">   VBD |  .  .  .  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  .  . &lt;6&gt; .  .  .  .  .  . |</span>
<span class="go">   VBG |  .  .  .  .  .  .  .  .  .  .  .  .  .  1  .  .  .  .  .  .  .  .  .  .  . &lt;4&gt; .  .  .  .  . |</span>
<span class="go">   VBN |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  1  . &lt;4&gt; .  .  .  . |</span>
<span class="go">   VBP |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;3&gt; .  .  . |</span>
<span class="go">   VBZ |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;7&gt; .  . |</span>
<span class="go">   WDT |  .  .  .  .  .  .  .  .  2  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;.&gt; . |</span>
<span class="go">    `` |  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . &lt;1&gt;|</span>
<span class="go">-------+----------------------------------------------------------------------------------------------+</span>
<span class="go">(row = reference; col = test)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to run the tagger with,
also used as the reference values in the generated confusion matrix.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="nltk.metrics.ConfusionMatrix.html#nltk.metrics.confusionmatrix.ConfusionMatrix" title="nltk.metrics.confusionmatrix.ConfusionMatrix">ConfusionMatrix</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>&#64;deprecated: Use accuracy(gold) instead.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.evaluate_per_tag">
<span class="sig-name descname"><span class="pre">evaluate_per_tag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">truncate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sort_by_count</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.evaluate_per_tag" title="Permalink to this definition">¶</a></dt>
<dd><p>Tabulate the <strong>recall</strong>, <strong>precision</strong> and <strong>f-measure</strong>
for each tag from <code class="docutils literal notranslate"><span class="pre">gold</span></code> or from running <code class="docutils literal notranslate"><span class="pre">tag</span></code> on the tokenized
sentences from <code class="docutils literal notranslate"><span class="pre">gold</span></code>.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tag</span> <span class="kn">import</span> <span class="n">PerceptronTagger</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">treebank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tagger</span> <span class="o">=</span> <span class="n">PerceptronTagger</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gold_data</span> <span class="o">=</span> <span class="n">treebank</span><span class="o">.</span><span class="n">tagged_sents</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tagger</span><span class="o">.</span><span class="n">evaluate_per_tag</span><span class="p">(</span><span class="n">gold_data</span><span class="p">))</span>
<span class="go">   Tag | Prec.  | Recall | F-measure</span>
<span class="go">-------+--------+--------+-----------</span>
<span class="go">    &#39;&#39; | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">     , | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">-NONE- | 0.0000 | 0.0000 | 0.0000</span>
<span class="go">     . | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    CC | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    CD | 0.7143 | 1.0000 | 0.8333</span>
<span class="go">    DT | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    EX | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    IN | 0.9167 | 0.8800 | 0.8980</span>
<span class="go">    JJ | 0.8889 | 0.8889 | 0.8889</span>
<span class="go">   JJR | 0.0000 | 0.0000 | 0.0000</span>
<span class="go">   JJS | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    MD | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    NN | 0.8000 | 0.9333 | 0.8615</span>
<span class="go">   NNP | 0.8929 | 1.0000 | 0.9434</span>
<span class="go">   NNS | 0.9500 | 1.0000 | 0.9744</span>
<span class="go">   POS | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">   PRP | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">  PRP$ | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    RB | 0.4000 | 1.0000 | 0.5714</span>
<span class="go">   RBR | 1.0000 | 0.5000 | 0.6667</span>
<span class="go">    RP | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    TO | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">    VB | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">   VBD | 0.8571 | 0.8571 | 0.8571</span>
<span class="go">   VBG | 1.0000 | 0.8000 | 0.8889</span>
<span class="go">   VBN | 1.0000 | 0.8000 | 0.8889</span>
<span class="go">   VBP | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">   VBZ | 1.0000 | 1.0000 | 1.0000</span>
<span class="go">   WDT | 0.0000 | 0.0000 | 0.0000</span>
<span class="go">    `` | 1.0000 | 1.0000 | 1.0000</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to score the tagger on.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Ratio of the cost of false negative compared to false
positives, as used in the f-measure computation. Defaults to 0.5,
where the costs are equal.</p></li>
<li><p><strong>truncate</strong> (<em>int</em><em>, </em><em>optional</em>) – If specified, then only show the specified
number of values.  Any sorting (e.g., sort_by_count)
will be performed before truncation. Defaults to None</p></li>
<li><p><strong>sort_by_count</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to sort the outputs on number of
occurrences of that tag in the <code class="docutils literal notranslate"><span class="pre">gold</span></code> data, defaults to False</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tabulated recall, precision and f-measure string</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.f_measure">
<span class="sig-name descname"><span class="pre">f_measure</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.f_measure" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the f-measure for each tag from <code class="docutils literal notranslate"><span class="pre">gold</span></code> or from running <code class="docutils literal notranslate"><span class="pre">tag</span></code>
on the tokenized sentences from <code class="docutils literal notranslate"><span class="pre">gold</span></code>. Then, return the dictionary
with mappings from tag to f-measure. The f-measure is the harmonic mean
of the <code class="docutils literal notranslate"><span class="pre">precision</span></code> and <code class="docutils literal notranslate"><span class="pre">recall</span></code>, weighted by <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.
In particular, given the precision <em>p</em> and recall <em>r</em> defined by:</p>
<ul class="simple">
<li><p><em>p</em> = true positive / (true positive + false negative)</p></li>
<li><p><em>r</em> = true positive / (true positive + false positive)</p></li>
</ul>
<p>The f-measure is:</p>
<ul class="simple">
<li><p><em>1/(alpha/p + (1-alpha)/r)</em></p></li>
</ul>
<p>With <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0.5</span></code>, this reduces to:</p>
<ul class="simple">
<li><p><em>2pr / (p + r)</em></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to score the tagger on.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – Ratio of the cost of false negative compared to false
positives. Defaults to 0.5, where the costs are equal.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mapping from tags to precision</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict[str, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the precision for each tag from <code class="docutils literal notranslate"><span class="pre">gold</span></code> or from running <code class="docutils literal notranslate"><span class="pre">tag</span></code>
on the tokenized sentences from <code class="docutils literal notranslate"><span class="pre">gold</span></code>. Then, return the dictionary
with mappings from tag to precision. The precision is defined as:</p>
<ul class="simple">
<li><p><em>p</em> = true positive / (true positive + false negative)</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to score the tagger on.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mapping from tags to precision</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict[str, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.recall">
<span class="sig-name descname"><span class="pre">recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gold</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the recall for each tag from <code class="docutils literal notranslate"><span class="pre">gold</span></code> or from running <code class="docutils literal notranslate"><span class="pre">tag</span></code>
on the tokenized sentences from <code class="docutils literal notranslate"><span class="pre">gold</span></code>. Then, return the dictionary
with mappings from tag to recall. The recall is defined as:</p>
<ul class="simple">
<li><p><em>r</em> = true positive / (true positive + false positive)</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gold</strong> (<em>list</em><em>(</em><em>list</em><em>(</em><em>tuple</em><em>(</em><em>str</em><em>, </em><em>str</em><em>)</em><em>)</em><em>)</em>) – The list of tagged sentences to score the tagger on.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A mapping from tags to recall</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict[str, float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="nltk.tag.HiddenMarkovModelTagger.tag_sents">
<span class="sig-name descname"><span class="pre">tag_sents</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentences</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nltk.tag.HiddenMarkovModelTagger.tag_sents" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply <code class="docutils literal notranslate"><span class="pre">self.tag()</span></code> to each element of <em>sentences</em>.  I.e.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tag</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/api/nltk.tag.HiddenMarkovModelTagger.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            
                <li class="footer-element">
                    <a href="https://github.com/nltk/nltk/tree/3.8.1">3.8.1</a>
                </li>
            

            
                <li class="footer-element">
                    Jan 02, 2023
                </li>
            
        </ul>

        
            <div id="copyright">
                &copy; 2023, NLTK Project
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>