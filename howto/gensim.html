<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>NLTK :: Sample usage for gensim</title>
  

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">NLTK</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">NLTK Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api/nltk.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../howto.html">Example Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk">NLTK on GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing to NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../team.html">NLTK Team</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <section id="sample-usage-for-gensim">
<h1>Sample usage for gensim<a class="headerlink" href="#sample-usage-for-gensim" title="Permalink to this heading">¶</a></h1>
<section id="demonstrate-word-embedding-using-gensim">
<h2>Demonstrate word embedding using Gensim<a class="headerlink" href="#demonstrate-word-embedding-using-gensim" title="Permalink to this heading">¶</a></h2>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.test.gensim_fixt</span> <span class="kn">import</span> <span class="n">setup_module</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">setup_module</span><span class="p">()</span>
</pre></div>
</div>
<p>We demonstrate three functions:
- Train the word embeddings using brown corpus;
- Load the pre-trained model and perform simple tasks; and
- Pruning the pre-trained binary model.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">gensim</span>
</pre></div>
</div>
<section id="train-the-model">
<h3>Train the model<a class="headerlink" href="#train-the-model" title="Permalink to this heading">¶</a></h3>
<p>Here we train a word embedding using the Brown Corpus:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_set</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">()[:</span><span class="mi">10000</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span>
</pre></div>
</div>
<p>It might take some time to train the model. So, after it is trained, it can be saved as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;brown.embedding&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;brown.embedding&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The model will be the list of words with their embedding. We can easily get the vector representation of a word.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">new_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;university&#39;</span><span class="p">])</span>
<span class="go">100</span>
</pre></div>
</div>
<p>There are some supporting functions already implemented in Gensim to manipulate with word embeddings.
For example, to compute the cosine similarity between 2 words:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">new_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;university&#39;</span><span class="p">,</span><span class="s1">&#39;school&#39;</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.3</span>
<span class="go">True</span>
</pre></div>
</div>
</section>
<section id="using-the-pre-trained-model">
<h3>Using the pre-trained model<a class="headerlink" href="#using-the-pre-trained-model" title="Permalink to this heading">¶</a></h3>
<p>NLTK includes a pre-trained model which is part of a model that is trained on 100 billion words from the Google News Dataset.
The full model is from <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a> (about 3 GB).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.data</span> <span class="kn">import</span> <span class="n">find</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word2vec_sample</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;models/word2vec_sample/pruned.word2vec.txt&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">word2vec_sample</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>We pruned the model to only include the most common words (~44k words).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="go">43981</span>
</pre></div>
</div>
<p>Each word is represented in the space of 300 dimensions:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;university&#39;</span><span class="p">])</span>
<span class="go">300</span>
</pre></div>
</div>
<p>Finding the top n words that are similar to a target word is simple. The result is the list of n words with the score.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;university&#39;</span><span class="p">],</span> <span class="n">topn</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">[(&#39;universities&#39;, 0.70039...), (&#39;faculty&#39;, 0.67809...), (&#39;undergraduate&#39;, 0.65870...)]</span>
</pre></div>
</div>
<p>Finding a word that is not in a list is also supported, although, implementing this by yourself is simple.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s1">&#39;breakfast cereal dinner lunch&#39;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">&#39;cereal&#39;</span>
</pre></div>
</div>
<p>Mikolov et al. (2013) figured out that word embedding captures much of syntactic and semantic regularities. For example,
the vector ‘King - Man + Woman’ is close to ‘Queen’ and ‘Germany - Berlin + Paris’ is close to ‘France’.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;woman&#39;</span><span class="p">,</span><span class="s1">&#39;king&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;man&#39;</span><span class="p">],</span> <span class="n">topn</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">[(&#39;queen&#39;, 0.71181...)]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Paris&#39;</span><span class="p">,</span><span class="s1">&#39;Germany&#39;</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Berlin&#39;</span><span class="p">],</span> <span class="n">topn</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">[(&#39;France&#39;, 0.78840...)]</span>
</pre></div>
</div>
<p>We can visualize the word embeddings using t-SNE (<a class="reference external" href="https://lvdmaaten.github.io/tsne/">https://lvdmaaten.github.io/tsne/</a>). For this demonstration, we visualize the first 1000 words.</p>
<div class="line-block">
<div class="line">import numpy as np</div>
<div class="line">labels = []</div>
<div class="line">count = 0</div>
<div class="line">max_count = 1000</div>
<div class="line">X = np.zeros(shape=(max_count,len(model[‘university’])))</div>
<div class="line"><br /></div>
<div class="line">for term in model.index_to_key:</div>
<div class="line-block">
<div class="line">X[count] = model[term]</div>
<div class="line">labels.append(term)</div>
<div class="line">count+= 1</div>
<div class="line">if count &gt;= max_count: break</div>
<div class="line"><br /></div>
</div>
<div class="line"># It is recommended to use PCA first to reduce to ~50 dimensions</div>
<div class="line">from sklearn.decomposition import PCA</div>
<div class="line">pca = PCA(n_components=50)</div>
<div class="line">X_50 = pca.fit_transform(X)</div>
<div class="line"><br /></div>
<div class="line"># Using TSNE to further reduce to 2 dimensions</div>
<div class="line">from sklearn.manifold import TSNE</div>
<div class="line">model_tsne = TSNE(n_components=2, random_state=0)</div>
<div class="line">Y = model_tsne.fit_transform(X_50)</div>
<div class="line"><br /></div>
<div class="line"># Show the scatter plot</div>
<div class="line">import matplotlib.pyplot as plt</div>
<div class="line">plt.scatter(Y[:,0], Y[:,1], 20)</div>
<div class="line"><br /></div>
<div class="line"># Add labels</div>
<div class="line">for label, x, y in zip(labels, Y[:, 0], Y[:, 1]):</div>
<div class="line-block">
<div class="line">plt.annotate(label, xy = (x,y), xytext = (0, 0), textcoords = ‘offset points’, size = 10)</div>
<div class="line"><br /></div>
</div>
<div class="line">plt.show()</div>
</div>
</section>
<section id="prune-the-trained-binary-model">
<h3>Prune the trained binary model<a class="headerlink" href="#prune-the-trained-binary-model" title="Permalink to this heading">¶</a></h3>
<p>Here is the supporting code to extract part of the binary model (GoogleNews-vectors-negative300.bin.gz) from <a class="reference external" href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a>
We use this code to get the <cite>word2vec_sample</cite> model.</p>
<div class="line-block">
<div class="line">import gensim</div>
<div class="line"># Load the binary model</div>
<div class="line">model = gensim.models.KeyedVectors.load_word2vec_format(‘GoogleNews-vectors-negative300.bin.gz’, binary = True)</div>
<div class="line"><br /></div>
<div class="line"># Only output word that appear in the Brown corpus</div>
<div class="line">from nltk.corpus import brown</div>
<div class="line">words = set(brown.words())</div>
<div class="line">print(len(words))</div>
<div class="line"><br /></div>
<div class="line"># Output presented word to a temporary file</div>
<div class="line">out_file = ‘pruned.word2vec.txt’</div>
<div class="line">with open(out_file,’w’) as f:</div>
<div class="line-block">
<div class="line">word_presented = words.intersection(model.index_to_key)</div>
<div class="line">f.write(‘{} {}n’.format(len(word_presented),len(model[‘word’])))</div>
<div class="line"><br /></div>
<div class="line">for word in word_presented:</div>
<div class="line-block">
<div class="line">f.write(‘{} {}n’.format(word, ‘ ‘.join(str(value) for value in model[word])))</div>
</div>
</div>
</div>
</section>
</section>
</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/howto/gensim.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            
                <li class="footer-element">
                    <a href="https://github.com/nltk/nltk/tree/3.8.1">3.8.1</a>
                </li>
            

            
                <li class="footer-element">
                    Jan 02, 2023
                </li>
            
        </ul>

        
            <div id="copyright">
                &copy; 2023, NLTK Project
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>