<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>NLTK :: Sample usage for tokenize</title>
  

  <link rel="stylesheet" href="../_static/css/nltk_theme.css"/>
  <link rel="stylesheet" href="../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="nltk-theme-container">
    <header>
      <div id="logo-container">
          
          <h1>
            <a href="../index.html">NLTK</a>
          </h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>

      <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

      <script type="text/javascript">
        $("#menu-toggle").click(function() {
          $("#menu-toggle").toggleClass("toggled");
          $("#side-menu-container").slideToggle(300);
        });
      </script>
    </header>

    <div id="content-container">

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
            <input type="text" name="q" placeholder="Search" />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">
          
  
    
  
  
    <p class="caption" role="heading"><span class="caption-text">NLTK Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api/nltk.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../howto.html">Example Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../py-modindex.html">Module Index</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/issues">Open Issues</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk">NLTK on GitHub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data.html">Installing NLTK Data</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">More</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../news.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contributing to NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../team.html">NLTK Team</a></li>
</ul>

  

        </div>

        
      </div>

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <section id="sample-usage-for-tokenize">
<h1>Sample usage for tokenize<a class="headerlink" href="#sample-usage-for-tokenize" title="Permalink to this heading">¶</a></h1>
<section id="regression-tests-nltkwordtokenizer">
<h2>Regression Tests: NLTKWordTokenizer<a class="headerlink" href="#regression-tests-nltkwordtokenizer" title="Permalink to this heading">¶</a></h2>
<p>Tokenizing some test strings.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="s2">&quot;On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
<span class="go">[&#39;On&#39;, &#39;a&#39;, &#39;$&#39;, &#39;50,000&#39;, &#39;mortgage&#39;, &#39;of&#39;, &#39;30&#39;, &#39;years&#39;, &#39;at&#39;, &#39;8&#39;, &#39;percent&#39;, &#39;,&#39;, &#39;the&#39;, &#39;monthly&#39;, &#39;payment&#39;, &#39;would&#39;, &#39;be&#39;, &#39;$&#39;, &#39;366.88&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">We beat some pretty good teams to get here,</span><span class="se">\&quot;</span><span class="s2"> Slocum said.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span>
<span class="go">[&#39;``&#39;, &#39;We&#39;, &#39;beat&#39;, &#39;some&#39;, &#39;pretty&#39;, &#39;good&#39;, &#39;teams&#39;, &#39;to&#39;, &#39;get&#39;, &#39;here&#39;, &#39;,&#39;, &quot;&#39;&#39;&quot;, &#39;Slocum&#39;, &#39;said&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s3</span> <span class="o">=</span> <span class="s2">&quot;Well, we couldn&#39;t have this predictable, cliche-ridden, </span><span class="se">\&quot;</span><span class="s2">Touched by an Angel</span><span class="se">\&quot;</span><span class="s2"> (a show creator John Masius worked on) wanna-be if she didn&#39;t.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">)</span>
<span class="go">[&#39;Well&#39;, &#39;,&#39;, &#39;we&#39;, &#39;could&#39;, &quot;n&#39;t&quot;, &#39;have&#39;, &#39;this&#39;, &#39;predictable&#39;, &#39;,&#39;, &#39;cliche-ridden&#39;, &#39;,&#39;, &#39;``&#39;, &#39;Touched&#39;, &#39;by&#39;, &#39;an&#39;, &#39;Angel&#39;, &quot;&#39;&#39;&quot;, &#39;(&#39;, &#39;a&#39;, &#39;show&#39;, &#39;creator&#39;, &#39;John&#39;, &#39;Masius&#39;, &#39;worked&#39;, &#39;on&#39;, &#39;)&#39;, &#39;wanna-be&#39;, &#39;if&#39;, &#39;she&#39;, &#39;did&#39;, &quot;n&#39;t&quot;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s4</span> <span class="o">=</span> <span class="s2">&quot;I cannot cannot work under these conditions!&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s4</span><span class="p">)</span>
<span class="go">[&#39;I&#39;, &#39;can&#39;, &#39;not&#39;, &#39;can&#39;, &#39;not&#39;, &#39;work&#39;, &#39;under&#39;, &#39;these&#39;, &#39;conditions&#39;, &#39;!&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s5</span> <span class="o">=</span> <span class="s2">&quot;The company spent $30,000,000 last year.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s5</span><span class="p">)</span>
<span class="go">[&#39;The&#39;, &#39;company&#39;, &#39;spent&#39;, &#39;$&#39;, &#39;30,000,000&#39;, &#39;last&#39;, &#39;year&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s6</span> <span class="o">=</span> <span class="s2">&quot;The company spent 40.75</span><span class="si">% o</span><span class="s2">f its income last year.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s6</span><span class="p">)</span>
<span class="go">[&#39;The&#39;, &#39;company&#39;, &#39;spent&#39;, &#39;40.75&#39;, &#39;%&#39;, &#39;of&#39;, &#39;its&#39;, &#39;income&#39;, &#39;last&#39;, &#39;year&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s7</span> <span class="o">=</span> <span class="s2">&quot;He arrived at 3:00 pm.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s7</span><span class="p">)</span>
<span class="go">[&#39;He&#39;, &#39;arrived&#39;, &#39;at&#39;, &#39;3:00&#39;, &#39;pm&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s8</span> <span class="o">=</span> <span class="s2">&quot;I bought these items: books, pencils, and pens.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s8</span><span class="p">)</span>
<span class="go">[&#39;I&#39;, &#39;bought&#39;, &#39;these&#39;, &#39;items&#39;, &#39;:&#39;, &#39;books&#39;, &#39;,&#39;, &#39;pencils&#39;, &#39;,&#39;, &#39;and&#39;, &#39;pens&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s9</span> <span class="o">=</span> <span class="s2">&quot;Though there were 150, 100 of them were old.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s9</span><span class="p">)</span>
<span class="go">[&#39;Though&#39;, &#39;there&#39;, &#39;were&#39;, &#39;150&#39;, &#39;,&#39;, &#39;100&#39;, &#39;of&#39;, &#39;them&#39;, &#39;were&#39;, &#39;old&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s10</span> <span class="o">=</span> <span class="s2">&quot;There were 300,000, but that wasn&#39;t enough.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s10</span><span class="p">)</span>
<span class="go">[&#39;There&#39;, &#39;were&#39;, &#39;300,000&#39;, &#39;,&#39;, &#39;but&#39;, &#39;that&#39;, &#39;was&#39;, &quot;n&#39;t&quot;, &#39;enough&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s11</span> <span class="o">=</span> <span class="s2">&quot;It&#39;s more&#39;n enough.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s11</span><span class="p">)</span>
<span class="go">[&#39;It&#39;, &quot;&#39;s&quot;, &#39;more&#39;, &quot;&#39;n&quot;, &#39;enough&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>Gathering the spans of the tokenized strings.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;Good muffins cost $3.88</span><span class="se">\n</span><span class="s1">in New (York).  Please (buy) me</span><span class="se">\n</span><span class="s1">two of them.</span><span class="se">\n</span><span class="s1">(Thanks).&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">),</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">),</span> <span class="p">(</span><span class="mi">19</span><span class="p">,</span> <span class="mi">23</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">26</span><span class="p">),</span> <span class="p">(</span><span class="mi">27</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">36</span><span class="p">),</span> <span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">),</span> <span class="p">(</span><span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mi">46</span><span class="p">),</span> <span class="p">(</span><span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">),</span> <span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="mi">51</span><span class="p">),</span> <span class="p">(</span><span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">),</span> <span class="p">(</span><span class="mi">53</span><span class="p">,</span> <span class="mi">55</span><span class="p">),</span> <span class="p">(</span><span class="mi">56</span><span class="p">,</span> <span class="mi">59</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">62</span><span class="p">),</span> <span class="p">(</span><span class="mi">63</span><span class="p">,</span> <span class="mi">68</span><span class="p">),</span> <span class="p">(</span><span class="mi">69</span><span class="p">,</span> <span class="mi">70</span><span class="p">),</span> <span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">76</span><span class="p">),</span> <span class="p">(</span><span class="mi">76</span><span class="p">,</span> <span class="mi">77</span><span class="p">),</span> <span class="p">(</span><span class="mi">77</span><span class="p">,</span> <span class="mi">78</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">NLTKWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Good&#39;</span><span class="p">,</span> <span class="s1">&#39;muffins&#39;</span><span class="p">,</span> <span class="s1">&#39;cost&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;3.88&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;New&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;York&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;Please&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;buy&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;me&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;them.&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;Thanks&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">NLTKWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;I said, &quot;I&#39;d like to buy some &#39;&#39;good muffins&quot; which cost $3.88</span><span class="se">\n</span><span class="s1"> each in New (York).&quot;&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">),</span> <span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">21</span><span class="p">,</span> <span class="mi">24</span><span class="p">),</span> <span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">29</span><span class="p">),</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">36</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">37</span><span class="p">,</span> <span class="mi">44</span><span class="p">),</span> <span class="p">(</span><span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">),</span> <span class="p">(</span><span class="mi">46</span><span class="p">,</span> <span class="mi">51</span><span class="p">),</span> <span class="p">(</span><span class="mi">52</span><span class="p">,</span> <span class="mi">56</span><span class="p">),</span> <span class="p">(</span><span class="mi">57</span><span class="p">,</span> <span class="mi">58</span><span class="p">),</span> <span class="p">(</span><span class="mi">58</span><span class="p">,</span> <span class="mi">62</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">68</span><span class="p">),</span> <span class="p">(</span><span class="mi">69</span><span class="p">,</span> <span class="mi">71</span><span class="p">),</span> <span class="p">(</span><span class="mi">72</span><span class="p">,</span> <span class="mi">75</span><span class="p">),</span> <span class="p">(</span><span class="mi">76</span><span class="p">,</span> <span class="mi">77</span><span class="p">),</span> <span class="p">(</span><span class="mi">77</span><span class="p">,</span> <span class="mi">81</span><span class="p">),</span> <span class="p">(</span><span class="mi">81</span><span class="p">,</span> <span class="mi">82</span><span class="p">),</span>
<span class="gp">... </span><span class="p">(</span><span class="mi">82</span><span class="p">,</span> <span class="mi">83</span><span class="p">),</span> <span class="p">(</span><span class="mi">83</span><span class="p">,</span> <span class="mi">84</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">list</span><span class="p">(</span><span class="n">NLTKWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;said&#39;</span><span class="p">,</span> <span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s2">&quot;&#39;d&quot;</span><span class="p">,</span> <span class="s1">&#39;like&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;buy&#39;</span><span class="p">,</span> <span class="s1">&#39;some&#39;</span><span class="p">,</span> <span class="s2">&quot;&#39;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;good&quot;</span><span class="p">,</span> <span class="s1">&#39;muffins&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;which&#39;</span><span class="p">,</span> <span class="s1">&#39;cost&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;3.88&#39;</span><span class="p">,</span> <span class="s1">&#39;each&#39;</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">,</span> <span class="s1">&#39;New&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;York&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">NLTKWordTokenizer</span><span class="p">()</span><span class="o">.</span><span class="n">span_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)]</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Testing improvement made to the TreebankWordTokenizer</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sx1</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\xab</span><span class="s1">Now that I can do.</span><span class="se">\xbb</span><span class="s1">&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="se">\xab</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;Now&#39;</span><span class="p">,</span> <span class="s1">&#39;that&#39;</span><span class="p">,</span> <span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;can&#39;</span><span class="p">,</span> <span class="s1">&#39;do&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\xbb</span><span class="s1">&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sx1</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sx2</span> <span class="o">=</span> <span class="s1">&#39;The unicode 201C and 201D </span><span class="se">\u201c</span><span class="s1">LEFT(RIGHT) DOUBLE QUOTATION MARK</span><span class="se">\u201d</span><span class="s1"> is also OPEN_PUNCT and CLOSE_PUNCT.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;The&#39;</span><span class="p">,</span> <span class="s1">&#39;unicode&#39;</span><span class="p">,</span> <span class="s1">&#39;201C&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;201D&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\u201c</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT&#39;</span><span class="p">,</span> <span class="s1">&#39;)&#39;</span><span class="p">,</span> <span class="s1">&#39;DOUBLE&#39;</span><span class="p">,</span> <span class="s1">&#39;QUOTATION&#39;</span><span class="p">,</span> <span class="s1">&#39;MARK&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\u201d</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;also&#39;</span><span class="p">,</span> <span class="s1">&#39;OPEN_PUNCT&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;CLOSE_PUNCT&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sx2</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Testing treebank’s detokenizer</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.treebank</span> <span class="kn">import</span> <span class="n">TreebankWordDetokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span> <span class="o">=</span> <span class="n">TreebankWordDetokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">We beat some pretty good teams to get here,</span><span class="se">\&quot;</span><span class="s2"> Slocum said.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;&quot;We beat some pretty good teams to get here,&quot; Slocum said.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Well, we couldn&#39;t have this predictable, cliche-ridden, </span><span class="se">\&quot;</span><span class="s2">Touched by an Angel</span><span class="se">\&quot;</span><span class="s2"> (a show creator John Masius worked on) wanna-be if she didn&#39;t.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;Well, we couldn\&#39;t have this predictable, cliche-ridden, &quot;Touched by an Angel&quot; (a show creator John Masius worked on) wanna-be if she didn\&#39;t.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;I cannot cannot work under these conditions!&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;I cannot cannot work under these conditions!&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;The company spent $30,000,000 last year.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;The company spent $30,000,000 last year.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;The company spent 40.75</span><span class="si">% o</span><span class="s2">f its income last year.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;The company spent 40.75% of its income last year.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;He arrived at 3:00 pm.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;He arrived at 3:00 pm.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;I bought these items: books, pencils, and pens.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;I bought these items: books, pencils, and pens.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Though there were 150, 100 of them were old.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;Though there were 150, 100 of them were old.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;There were 300,000, but that wasn&#39;t enough.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&quot;There were 300,000, but that wasn&#39;t enough.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;How &quot;are&quot; you?&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;How &quot;are&quot; you?&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Hello (world)&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;Hello (world)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&lt;A sentence&gt; with (many) [kinds] of </span><span class="si">{parentheses}</span><span class="s1">. &quot;Sometimes it</span><span class="se">\&#39;</span><span class="s1">s inside (quotes)&quot;. (&quot;Sometimes the otherway around&quot;).&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;&lt;A sentence&gt; with (many) [kinds] of {parentheses}. &quot;Sometimes it\&#39;s inside (quotes)&quot;. (&quot;Sometimes the otherway around&quot;).&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Sentence ending with (parentheses)&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;Sentence ending with (parentheses)&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;(Sentence) starting with parentheses.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&#39;(Sentence) starting with parentheses.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;I&#39;ve&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&quot;I&#39;ve&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;Don&#39;t&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&quot;Don&#39;t&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;I&#39;d&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">detokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="go">&quot;I&#39;d&quot;</span>
</pre></div>
</div>
<p>Sentence tokenization in word_tokenize:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s11</span> <span class="o">=</span> <span class="s2">&quot;I called Dr. Jones. I called Dr. Jones.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s11</span><span class="p">)</span>
<span class="go">[&#39;I&#39;, &#39;called&#39;, &#39;Dr.&#39;, &#39;Jones&#39;, &#39;.&#39;, &#39;I&#39;, &#39;called&#39;, &#39;Dr.&#39;, &#39;Jones&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s12</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Ich muss unbedingt daran denken, Mehl, usw. fur einen &quot;</span>
<span class="gp">... </span>       <span class="s2">&quot;Kuchen einzukaufen. Ich muss.&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s12</span><span class="p">)</span>
<span class="go">[&#39;Ich&#39;, &#39;muss&#39;, &#39;unbedingt&#39;, &#39;daran&#39;, &#39;denken&#39;, &#39;,&#39;, &#39;Mehl&#39;, &#39;,&#39;, &#39;usw&#39;,</span>
<span class="go"> &#39;.&#39;, &#39;fur&#39;, &#39;einen&#39;, &#39;Kuchen&#39;, &#39;einzukaufen&#39;, &#39;.&#39;, &#39;Ich&#39;, &#39;muss&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">s12</span><span class="p">,</span> <span class="s1">&#39;german&#39;</span><span class="p">)</span>
<span class="go">[&#39;Ich&#39;, &#39;muss&#39;, &#39;unbedingt&#39;, &#39;daran&#39;, &#39;denken&#39;, &#39;,&#39;, &#39;Mehl&#39;, &#39;,&#39;, &#39;usw.&#39;,</span>
<span class="go"> &#39;fur&#39;, &#39;einen&#39;, &#39;Kuchen&#39;, &#39;einzukaufen&#39;, &#39;.&#39;, &#39;Ich&#39;, &#39;muss&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
</section>
<section id="regression-tests-regexp-tokenizer">
<h2>Regression Tests: Regexp Tokenizer<a class="headerlink" href="#regression-tests-regexp-tokenizer" title="Permalink to this heading">¶</a></h2>
<p>Some additional test strings.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Good muffins cost $3.88</span><span class="se">\n</span><span class="s2">in New York.  Please buy me</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="gp">... </span>     <span class="s2">&quot;two of them.</span><span class="se">\n\n</span><span class="s2">Thanks.&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;Alas, it has not rained today. When, do you think, &quot;</span>
<span class="gp">... </span>      <span class="s2">&quot;will it rain again?&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s3</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;&lt;p&gt;Although this is &lt;b&gt;not&lt;/b&gt; the case here, we must &quot;</span>
<span class="gp">... </span>      <span class="s2">&quot;not relax our vigilance!&lt;/p&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;[,\.\?!&quot;]\s*&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;, &#39;, &#39;. &#39;, &#39;, &#39;, &#39;, &#39;, &#39;?&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;[,\.\?!&quot;]\s*&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Alas&#39;, &#39;it has not rained today&#39;, &#39;When&#39;, &#39;do you think&#39;,</span>
<span class="go"> &#39;will it rain again&#39;]</span>
</pre></div>
</div>
<p>Take care to avoid using capturing groups:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;/?[bp]&gt;&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;&lt;p&gt;&#39;, &#39;&lt;b&gt;&#39;, &#39;&lt;/b&gt;&#39;, &#39;&lt;/p&gt;&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;/?(?:b|p)&gt;&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;&lt;p&gt;&#39;, &#39;&lt;b&gt;&#39;, &#39;&lt;/b&gt;&#39;, &#39;&lt;/p&gt;&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;/?(?:b|p)&gt;&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Although this is &#39;, &#39;not&#39;,</span>
<span class="go"> &#39; the case here, we must not relax our vigilance!&#39;]</span>
</pre></div>
</div>
<p>Named groups are capturing groups, and confuse the tokenizer:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;/?(?P&lt;named&gt;b|p)&gt;&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;p&#39;, &#39;b&#39;, &#39;b&#39;, &#39;p&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;/?(?P&lt;named&gt;b|p)&gt;&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;p&#39;, &#39;Although this is &#39;, &#39;b&#39;, &#39;not&#39;, &#39;b&#39;,</span>
<span class="go"> &#39; the case here, we must not relax our vigilance!&#39;, &#39;p&#39;]</span>
</pre></div>
</div>
<p>Make sure that nested groups don’t confuse the tokenizer:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(?:h|r|l)a(?:s|(?:i|n0))&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;las&#39;, &#39;has&#39;, &#39;rai&#39;, &#39;rai&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(?:h|r|l)a(?:s|(?:i|n0))&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;A&#39;, &#39;, it &#39;, &#39; not &#39;, &#39;ned today. When, do you think, will it &#39;,</span>
<span class="go"> &#39;n again?&#39;]</span>
</pre></div>
</div>
<p>Back-references require capturing groups, and these are not supported:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="s2">&quot;aabbbcccc&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;(.)\1&#39;</span><span class="p">)</span>
<span class="go">[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;]</span>
</pre></div>
</div>
<p>A simple sentence tokenizer ‘.(s+|$)’</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">regexp_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">pattern</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;\.(?:\s+|$)&#39;</span><span class="p">,</span> <span class="n">gaps</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">[&#39;Good muffins cost $3.88\nin New York&#39;,</span>
<span class="go"> &#39;Please buy me\ntwo of them&#39;, &#39;Thanks&#39;]</span>
</pre></div>
</div>
</section>
<section id="regression-tests-tweettokenizer">
<h2>Regression Tests: TweetTokenizer<a class="headerlink" href="#regression-tests-tweettokenizer" title="Permalink to this heading">¶</a></h2>
<p>TweetTokenizer is a tokenizer specifically designed for micro-blogging tokenization tasks.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TweetTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s0</span> <span class="o">=</span> <span class="s2">&quot;This is a cooool #dummysmiley: :-) :-P &lt;3 and some arrows &lt; &gt; -&gt; &lt;--&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s0</span><span class="p">)</span>
<span class="go">[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;cooool&#39;, &#39;#dummysmiley&#39;, &#39;:&#39;, &#39;:-)&#39;, &#39;:-P&#39;, &#39;&lt;3&#39;, &#39;and&#39;, &#39;some&#39;, &#39;arrows&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;-&gt;&#39;, &#39;&lt;--&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s1</span> <span class="o">=</span> <span class="s2">&quot;@Joyster2012 @CathStaincliffe Good for you, girl!! Best wishes :-)&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span>
<span class="go">[&#39;@Joyster2012&#39;, &#39;@CathStaincliffe&#39;, &#39;Good&#39;, &#39;for&#39;, &#39;you&#39;, &#39;,&#39;, &#39;girl&#39;, &#39;!&#39;, &#39;!&#39;, &#39;Best&#39;, &#39;wishes&#39;, &#39;:-)&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s2</span> <span class="o">=</span> <span class="s2">&quot;3Points for #DreamTeam Gooo BAILEY! :) #PBB737Gold @PBBabscbn&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span>
<span class="go">[&#39;3Points&#39;, &#39;for&#39;, &#39;#DreamTeam&#39;, &#39;Gooo&#39;, &#39;BAILEY&#39;, &#39;!&#39;, &#39;:)&#39;, &#39;#PBB737Gold&#39;, &#39;@PBBabscbn&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s3</span> <span class="o">=</span> <span class="s2">&quot;@Insanomania They do... Their mentality doesn&#39;t :(&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s3</span><span class="p">)</span>
<span class="go">[&#39;@Insanomania&#39;, &#39;They&#39;, &#39;do&#39;, &#39;...&#39;, &#39;Their&#39;, &#39;mentality&#39;, &quot;doesn&#39;t&quot;, &#39;:(&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s4</span> <span class="o">=</span> <span class="s2">&quot;RT @facugambande: Ya por arrancar a grabar !!! #TirenTirenTiren vamoo !!&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s4</span><span class="p">)</span>
<span class="go">[&#39;RT&#39;, &#39;@facugambande&#39;, &#39;:&#39;, &#39;Ya&#39;, &#39;por&#39;, &#39;arrancar&#39;, &#39;a&#39;, &#39;grabar&#39;, &#39;!&#39;, &#39;!&#39;, &#39;!&#39;, &#39;#TirenTirenTiren&#39;, &#39;vamoo&#39;, &#39;!&#39;, &#39;!&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">(</span><span class="n">reduce_len</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s5</span> <span class="o">=</span> <span class="s2">&quot;@crushinghes the summer holidays are great but I&#39;m so bored already :(&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s5</span><span class="p">)</span>
<span class="go">[&#39;@crushinghes&#39;, &#39;the&#39;, &#39;summer&#39;, &#39;holidays&#39;, &#39;are&#39;, &#39;great&#39;, &#39;but&#39;, &quot;I&#39;m&quot;, &#39;so&#39;, &#39;bored&#39;, &#39;already&#39;, &#39;:(&#39;]</span>
</pre></div>
</div>
<p>It is possible to specify <cite>strip_handles</cite> and <cite>reduce_len</cite> parameters for a TweetTokenizer instance. Setting <cite>strip_handles</cite> to True, the tokenizer will remove Twitter handles (e.g. usernames). Setting <cite>reduce_len</cite> to True, repeated character sequences of length 3 or greater will be replaced with sequences of length 3.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">(</span><span class="n">strip_handles</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reduce_len</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s6</span> <span class="o">=</span> <span class="s1">&#39;@remy: This is waaaaayyyy too much for you!!!!!!&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s6</span><span class="p">)</span>
<span class="go">[&#39;:&#39;, &#39;This&#39;, &#39;is&#39;, &#39;waaayyy&#39;, &#39;too&#39;, &#39;much&#39;, &#39;for&#39;, &#39;you&#39;, &#39;!&#39;, &#39;!&#39;, &#39;!&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s7</span> <span class="o">=</span> <span class="s1">&#39;@_willy65: No place for @chuck tonight. Sorry.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s7</span><span class="p">)</span>
<span class="go">[&#39;:&#39;, &#39;No&#39;, &#39;place&#39;, &#39;for&#39;, &#39;tonight&#39;, &#39;.&#39;, &#39;Sorry&#39;, &#39;.&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s8</span> <span class="o">=</span> <span class="s1">&#39;@mar_tin is a great developer. Contact him at mar_tin@email.com.&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s8</span><span class="p">)</span>
<span class="go">[&#39;is&#39;, &#39;a&#39;, &#39;great&#39;, &#39;developer&#39;, &#39;.&#39;, &#39;Contact&#39;, &#39;him&#39;, &#39;at&#39;, &#39;mar_tin@email.com&#39;, &#39;.&#39;]</span>
</pre></div>
</div>
<p>The <cite>preserve_case</cite> parameter (default: True) allows to convert uppercase tokens to lowercase tokens. Emoticons are not affected:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">(</span><span class="n">preserve_case</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s9</span> <span class="o">=</span> <span class="s2">&quot;@jrmy: I&#39;m REALLY HAPPYYY about that! NICEEEE :D :P&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s9</span><span class="p">)</span>
<span class="go">[&#39;@jrmy&#39;, &#39;:&#39;, &quot;i&#39;m&quot;, &#39;really&#39;, &#39;happyyy&#39;, &#39;about&#39;, &#39;that&#39;, &#39;!&#39;, &#39;niceeee&#39;, &#39;:D&#39;, &#39;:P&#39;]</span>
</pre></div>
</div>
<p>It should not hang on long sequences of the same punctuation character.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s10</span> <span class="o">=</span> <span class="s2">&quot;Photo: Aujourd&#39;hui sur http://t.co/0gebOFDUzn Projet... http://t.co/bKfIUbydz2.............................. http://fb.me/3b6uXpz0L&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">s10</span><span class="p">)</span>
<span class="go">[&#39;Photo&#39;, &#39;:&#39;, &quot;Aujourd&#39;hui&quot;, &#39;sur&#39;, &#39;http://t.co/0gebOFDUzn&#39;, &#39;Projet&#39;, &#39;...&#39;, &#39;http://t.co/bKfIUbydz2&#39;, &#39;...&#39;, &#39;http://fb.me/3b6uXpz0L&#39;]</span>
</pre></div>
</div>
<p>Tokenizing multiple sentences at once:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span> <span class="o">=</span> <span class="n">TweetTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s2">&quot;This is a cooool #dummysmiley: :-) :-P &lt;3 and some arrows &lt; &gt; -&gt; &lt;--&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;@jrmy: I&#39;m REALLY HAPPYYY about that! NICEEEE :D :P&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;@_willy65: No place for @chuck tonight. Sorry.&quot;</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tknzr</span><span class="o">.</span><span class="n">tokenize_sents</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span> 
<span class="go">[[&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;cooool&#39;, &#39;#dummysmiley&#39;, &#39;:&#39;, &#39;:-)&#39;, &#39;:-P&#39;, &#39;&lt;3&#39;, &#39;and&#39;, &#39;some&#39;, &#39;arrows&#39;, &#39;&lt;&#39;, &#39;&gt;&#39;, &#39;-&gt;&#39;, &#39;&lt;--&#39;],</span>
<span class="go">[&#39;@jrmy&#39;, &#39;:&#39;, &quot;I&#39;m&quot;, &#39;REALLY&#39;, &#39;HAPPYYY&#39;, &#39;about&#39;, &#39;that&#39;, &#39;!&#39;, &#39;NICEEEE&#39;, &#39;:D&#39;, &#39;:P&#39;],</span>
<span class="go">[&#39;@_willy65&#39;, &#39;:&#39;, &#39;No&#39;, &#39;place&#39;, &#39;for&#39;, &#39;@chuck&#39;, &#39;tonight&#39;, &#39;.&#39;, &#39;Sorry&#39;, &#39;.&#39;]]</span>
</pre></div>
</div>
</section>
<section id="regression-tests-punktsentencetokenizer">
<h2>Regression Tests: PunktSentenceTokenizer<a class="headerlink" href="#regression-tests-punktsentencetokenizer" title="Permalink to this heading">¶</a></h2>
<p>The sentence splitter should remove whitespace following the sentence boundary.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span> <span class="o">=</span> <span class="n">PunktSentenceTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;See Section 3).  Or Section 2).  &#39;</span><span class="p">)</span>
<span class="go">[&#39;See Section 3).&#39;, &#39;Or Section 2).&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;See Section 3.)  Or Section 2.)  &#39;</span><span class="p">)</span>
<span class="go">[&#39;See Section 3.)&#39;, &#39;Or Section 2.)&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;See Section 3.)  Or Section 2.)  &#39;</span><span class="p">,</span> <span class="n">realign_boundaries</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">[&#39;See Section 3.&#39;, &#39;)  Or Section 2.&#39;, &#39;)&#39;]</span>
</pre></div>
</div>
<p>Two instances of PunktSentenceTokenizer should not share PunktParameters.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span> <span class="o">=</span> <span class="n">PunktSentenceTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst2</span> <span class="o">=</span> <span class="n">PunktSentenceTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span><span class="o">.</span><span class="n">_params</span> <span class="ow">is</span> <span class="n">pst2</span><span class="o">.</span><span class="n">_params</span>
<span class="go">False</span>
</pre></div>
</div>
<p>Testing mutable default arguments for <a class="reference external" href="https://github.com/nltk/nltk/pull/2067">https://github.com/nltk/nltk/pull/2067</a></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.punkt</span> <span class="kn">import</span> <span class="n">PunktBaseClass</span><span class="p">,</span> <span class="n">PunktTrainer</span><span class="p">,</span> <span class="n">PunktSentenceTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize.punkt</span> <span class="kn">import</span> <span class="n">PunktLanguageVars</span><span class="p">,</span> <span class="n">PunktParameters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pbc</span> <span class="o">=</span> <span class="n">PunktBaseClass</span><span class="p">(</span><span class="n">lang_vars</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">pbc</span><span class="o">.</span><span class="n">_params</span><span class="p">)</span>
<span class="go">&lt;class &#39;nltk.tokenize.punkt.PunktParameters&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">pbc</span><span class="o">.</span><span class="n">_lang_vars</span><span class="p">)</span>
<span class="go">&lt;class &#39;nltk.tokenize.punkt.PunktLanguageVars&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pt</span> <span class="o">=</span> <span class="n">PunktTrainer</span><span class="p">(</span><span class="n">lang_vars</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">_lang_vars</span><span class="p">)</span>
<span class="go">&lt;class &#39;nltk.tokenize.punkt.PunktLanguageVars&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span> <span class="o">=</span> <span class="n">PunktSentenceTokenizer</span><span class="p">(</span><span class="n">lang_vars</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">pst</span><span class="o">.</span><span class="n">_lang_vars</span><span class="p">)</span>
<span class="go">&lt;class &#39;nltk.tokenize.punkt.PunktLanguageVars&#39;&gt;</span>
</pre></div>
</div>
<p>Testing that inputs can start with dots.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span> <span class="o">=</span> <span class="n">PunktSentenceTokenizer</span><span class="p">(</span><span class="n">lang_vars</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pst</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;. This input starts with a dot. This used to cause issues.&quot;</span><span class="p">)</span>
<span class="go">[&#39;.&#39;, &#39;This input starts with a dot.&#39;, &#39;This used to cause issues.&#39;]</span>
</pre></div>
</div>
</section>
<section id="regression-tests-align-tokens">
<h2>Regression Tests: align_tokens<a class="headerlink" href="#regression-tests-align-tokens" title="Permalink to this heading">¶</a></h2>
<p>Post-hoc alignment of tokens with a source string</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; from nltk.tokenize.util import align_tokens
&gt;&gt;&gt; list(align_tokens([&#39;&#39;], &quot;&quot;))
[(0, 0)]
&gt;&gt;&gt; list(align_tokens([&#39;&#39;], &quot; &quot;))
[(0, 0)]
&gt;&gt;&gt; list(align_tokens([], &quot;&quot;))
[]
&gt;&gt;&gt; list(align_tokens([], &quot; &quot;))
[]
&gt;&gt;&gt; list(align_tokens([&#39;a&#39;], &quot;a&quot;))
[(0, 1)]
&gt;&gt;&gt; list(align_tokens([&#39;abc&#39;, &#39;def&#39;], &quot;abcdef&quot;))
[(0, 3), (3, 6)]
&gt;&gt;&gt; list(align_tokens([&#39;abc&#39;, &#39;def&#39;], &quot;abc def&quot;))
[(0, 3), (4, 7)]
&gt;&gt;&gt; list(align_tokens([&#39;ab&#39;, &#39;cd&#39;], &quot;ab cd ef&quot;))
[(0, 2), (3, 5)]
&gt;&gt;&gt; list(align_tokens([&#39;ab&#39;, &#39;cd&#39;, &#39;ef&#39;], &quot;ab cd ef&quot;))
[(0, 2), (3, 5), (6, 8)]
&gt;&gt;&gt; list(align_tokens([&#39;ab&#39;, &#39;cd&#39;, &#39;efg&#39;], &quot;ab cd ef&quot;))
Traceback (most recent call last):
....
ValueError: substring &quot;efg&quot; not found in &quot;ab cd ef&quot;
&gt;&gt;&gt; list(align_tokens([&#39;ab&#39;, &#39;cd&#39;, &#39;ef&#39;, &#39;gh&#39;], &quot;ab cd ef&quot;))
Traceback (most recent call last):
....
ValueError: substring &quot;gh&quot; not found in &quot;ab cd ef&quot;
&gt;&gt;&gt; list(align_tokens([&#39;The&#39;, &#39;plane&#39;, &#39;,&#39;, &#39;bound&#39;, &#39;for&#39;, &#39;St&#39;, &#39;Petersburg&#39;, &#39;,&#39;, &#39;crashed&#39;, &#39;in&#39;, &#39;Egypt&#39;, &quot;&#39;s&quot;, &#39;Sinai&#39;, &#39;desert&#39;, &#39;just&#39;, &#39;23&#39;, &#39;minutes&#39;, &#39;after&#39;, &#39;take-off&#39;, &#39;from&#39;, &#39;Sharm&#39;, &#39;el-Sheikh&#39;, &#39;on&#39;, &#39;Saturday&#39;, &#39;.&#39;], &quot;The plane, bound for St Petersburg, crashed in Egypt&#39;s Sinai desert just 23 minutes after take-off from Sharm el-Sheikh on Saturday.&quot;))
[(0, 3), (4, 9), (9, 10), (11, 16), (17, 20), (21, 23), (24, 34), (34, 35), (36, 43), (44, 46), (47, 52), (52, 54), (55, 60), (61, 67), (68, 72), (73, 75), (76, 83), (84, 89), (90, 98), (99, 103), (104, 109), (110, 119), (120, 122), (123, 131), (131, 132)]
</pre></div>
</div>
</section>
<section id="regression-tests-mwetokenizer">
<h2>Regression Tests: MWETokenizer<a class="headerlink" href="#regression-tests-mwetokenizer" title="Permalink to this heading">¶</a></h2>
<p>Pickle an MWETokenizer</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">MWETokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">MWETokenizer</span><span class="p">([(</span><span class="s1">&#39;hors&#39;</span><span class="p">,</span> <span class="s2">&quot;d&#39;oeuvre&quot;</span><span class="p">)],</span> <span class="n">separator</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpickeled</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unpickeled</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">&quot;An hors d&#39;oeuvre tonight, sir?&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="go">[&#39;An&#39;, &quot;hors+d&#39;oeuvre&quot;, &#39;tonight,&#39;, &#39;sir?&#39;]</span>
</pre></div>
</div>
</section>
<section id="regression-tests-texttilingtokenizer">
<h2>Regression Tests: TextTilingTokenizer<a class="headerlink" href="#regression-tests-texttilingtokenizer" title="Permalink to this heading">¶</a></h2>
<p>TextTilingTokenizer tokenizes text into coherent subtopic chunks based upon Hearst’s TextTiling algorithm.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">TextTilingTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span> <span class="o">=</span> <span class="n">TextTilingTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tt</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">brown</span><span class="o">.</span><span class="n">raw</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">])</span>
<span class="go">[&quot;\n\n\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta&#39;s/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn &#39;&#39;/&#39;&#39; that/cs any/dti irregularities/nns took/vbd place/nn ./.\n\n\n\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/cc thanks/nns of/in the/at City/nn-tl of/in-tl Atlanta/np-tl &#39;&#39;/&#39;&#39; for/in the/at manner/nn in/in which/wdt the/at election/nn was/bedz conducted/vbn ./.\n\n\n\tThe/at September-October/np term/nn jury/nn had/hvd been/ben charged/vbn by/in Fulton/np-tl Superior/jj-tl Court/nn-tl Judge/nn-tl Durwood/np Pye/np to/to investigate/vb reports/nns of/in possible/jj ``/`` irregularities/nns &#39;&#39;/&#39;&#39; in/in the/at hard-fought/jj primary/nn which/wdt was/bedz won/vbn by/in Mayor-nominate/nn-tl Ivan/np Allen/np Jr./&quot;]</span>
</pre></div>
</div>
<p>Test that <cite>ValueError</cite> exceptions are raised when illegal arguments are used.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">TextTilingTokenizer</span><span class="p">(</span><span class="n">similarity_method</span><span class="o">=</span><span class="s1">&#39;foo&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">brown</span><span class="o">.</span><span class="n">raw</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">])</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">ValueError</span>: <span class="n">Similarity method foo not recognized</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TextTilingTokenizer</span><span class="p">(</span><span class="n">smoothing_method</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">brown</span><span class="o">.</span><span class="n">raw</span><span class="p">()[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1000</span><span class="p">])</span>
<span class="gt">Traceback (most recent call last):</span>
  <span class="c">...</span>
<span class="gr">ValueError</span>: <span class="n">Smoothing method bar not recognized</span>
</pre></div>
</div>
</section>
</section>


        </div>
      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../_sources/howto/tokenize.rst.txt" rel="nofollow"> source</a>
                    
                </li>
            

            
                <li class="footer-element">
                    <a href="https://github.com/nltk/nltk/tree/3.8.1">3.8.1</a>
                </li>
            

            
                <li class="footer-element">
                    Jan 02, 2023
                </li>
            
        </ul>

        
            <div id="copyright">
                &copy; 2023, NLTK Project
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/tomaarsen/nltk_theme">NLTK Theme</a>
        </div>
    </div>
</footer> 

</div>

</body>
</html>