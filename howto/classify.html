<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/" />
<title>Classifiers</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7614 2013-02-21 15:55:51Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* it is used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="classifiers">
<h1 class="title">Classifiers</h1>

<!-- Copyright (C) 2001-2015 NLTK Project -->
<!-- For license information, see LICENSE.TXT -->
<p>Classifiers label tokens with category labels (or <em>class labels</em>).
Typically, labels are represented with strings (such as <tt class="docutils literal">&quot;health&quot;</tt>
or <tt class="docutils literal">&quot;sports&quot;</tt>.  In NLTK, classifiers are defined using classes that
implement the <cite>ClassifyI</cite> interface:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.usage(nltk.classify.ClassifierI)
ClassifierI supports the following operations:
  - self.classify(featureset)
  - self.classify_many(featuresets)
  - self.labels()
  - self.prob_classify(featureset)
  - self.prob_classify_many(featuresets)
</pre>
</blockquote>
<p>NLTK defines several classifier classes:</p>
<ul class="simple">
<li><cite>ConditionalExponentialClassifier</cite></li>
<li><cite>DecisionTreeClassifier</cite></li>
<li><cite>MaxentClassifier</cite></li>
<li><cite>NaiveBayesClassifier</cite></li>
<li><cite>WekaClassifier</cite></li>
</ul>
<p>Classifiers are typically created by training them on a training
corpus.</p>
<div class="section" id="regression-tests">
<h1>Regression Tests</h1>
<p>We define a very simple training corpus with 3 binary features: ['a',
'b', 'c'], and are two labels: ['x', 'y'].  We use a simple feature set so
that the correct answers can be calculated analytically (although we
haven't done this yet for all tests).</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; train = [
...     (dict(a=1,b=1,c=1), 'y'),
...     (dict(a=1,b=1,c=1), 'x'),
...     (dict(a=1,b=1,c=0), 'y'),
...     (dict(a=0,b=1,c=1), 'x'),
...     (dict(a=0,b=1,c=1), 'y'),
...     (dict(a=0,b=0,c=1), 'y'),
...     (dict(a=0,b=1,c=0), 'x'),
...     (dict(a=0,b=0,c=0), 'x'),
...     (dict(a=0,b=1,c=1), 'y'),
...     ]
&gt;&gt;&gt; test = [
...     (dict(a=1,b=0,c=1)), # unseen
...     (dict(a=1,b=0,c=0)), # unseen
...     (dict(a=0,b=1,c=1)), # seen 3 times, labels=y,y,x
...     (dict(a=0,b=1,c=0)), # seen 1 time, label=x
...     ]
</pre>
</blockquote>
<p>Test the Naive Bayes classifier:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; classifier = nltk.classify.NaiveBayesClassifier.train(train)
&gt;&gt;&gt; sorted(classifier.labels())
['x', 'y']
&gt;&gt;&gt; classifier.classify_many(test)
['y', 'x', 'y', 'x']
&gt;&gt;&gt; for pdist in classifier.prob_classify_many(test):
...     print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))
0.3203 0.6797
0.5857 0.4143
0.3792 0.6208
0.6470 0.3530
&gt;&gt;&gt; classifier.show_most_informative_features()
Most Informative Features
                       c = 0                   x : y      =      2.0 : 1.0
                       c = 1                   y : x      =      1.5 : 1.0
                       a = 1                   y : x      =      1.4 : 1.0
                       b = 0                   x : y      =      1.2 : 1.0
                       a = 0                   x : y      =      1.2 : 1.0
                       b = 1                   y : x      =      1.1 : 1.0
</pre>
</blockquote>
<p>Test the Decision Tree classifier:</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; classifier = nltk.classify.DecisionTreeClassifier.train(
...     train, entropy_cutoff=0,
...                                                support_cutoff=0)
&gt;&gt;&gt; sorted(classifier.labels())
['x', 'y']
&gt;&gt;&gt; print(classifier)
c=0? .................................................. x
  a=0? ................................................ x
  a=1? ................................................ y
c=1? .................................................. y
&lt;BLANKLINE&gt;
&gt;&gt;&gt; classifier.classify_many(test)
['y', 'y', 'y', 'x']
&gt;&gt;&gt; for pdist in classifier.prob_classify_many(test):
...     print('%.4f %.4f' % (pdist.prob('x'), pdist.prob('y')))
Traceback (most recent call last):
  . . .
NotImplementedError
</pre>
</blockquote>
<p>Test SklearnClassifier, which requires the scikit-learn package.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.classify import SklearnClassifier
&gt;&gt;&gt; from sklearn.naive_bayes import BernoulliNB
&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; train_data = [({&quot;a&quot;: 4, &quot;b&quot;: 1, &quot;c&quot;: 0}, &quot;ham&quot;),
...               ({&quot;a&quot;: 5, &quot;b&quot;: 2, &quot;c&quot;: 1}, &quot;ham&quot;),
...               ({&quot;a&quot;: 0, &quot;b&quot;: 3, &quot;c&quot;: 4}, &quot;spam&quot;),
...               ({&quot;a&quot;: 5, &quot;b&quot;: 1, &quot;c&quot;: 1}, &quot;ham&quot;),
...               ({&quot;a&quot;: 1, &quot;b&quot;: 4, &quot;c&quot;: 3}, &quot;spam&quot;)]
&gt;&gt;&gt; classif = SklearnClassifier(BernoulliNB()).train(train_data)
&gt;&gt;&gt; test_data = [{&quot;a&quot;: 3, &quot;b&quot;: 2, &quot;c&quot;: 1},
...              {&quot;a&quot;: 0, &quot;b&quot;: 3, &quot;c&quot;: 7}]
&gt;&gt;&gt; classif.classify_many(test_data)
['ham', 'spam']
&gt;&gt;&gt; classif = SklearnClassifier(SVC(), sparse=False).train(train_data)
&gt;&gt;&gt; classif.classify_many(test_data)
['ham', 'spam']
</pre>
</blockquote>
<p>Test the Maximum Entropy classifier training algorithms; they should all
generate the same results.</p>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; def print_maxent_test_header():
...     print(' '*11+''.join(['      test[%s]  ' % i
...                           for i in range(len(test))]))
...     print(' '*11+'     p(x)  p(y)'*len(test))
...     print('-'*(11+15*len(test)))
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; def test_maxent(algorithm):
...     print('%11s' % algorithm, end=' ')
...     try:
...         classifier = nltk.classify.MaxentClassifier.train(
...                         train, algorithm, trace=0, max_iter=1000)
...     except Exception as e:
...         print('Error: %r' % e)
...         return
...
...     for featureset in test:
...         pdist = classifier.prob_classify(featureset)
...         print('%8.2f%6.2f' % (pdist.prob('x'), pdist.prob('y')), end=' ')
...     print()
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; print_maxent_test_header(); test_maxent('GIS'); test_maxent('IIS')
                 test[0]        test[1]        test[2]        test[3]
                p(x)  p(y)     p(x)  p(y)     p(x)  p(y)     p(x)  p(y)
-----------------------------------------------------------------------
        GIS     0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
        IIS     0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; test_maxent('MEGAM'); test_maxent('TADM') # doctest: +SKIP
        MEGAM   0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
        TADM    0.16  0.84     0.46  0.54     0.41  0.59     0.76  0.24
</pre>
</blockquote>
</div>
<div class="section" id="regression-tests-for-typedmaxentfeatureencoding">
<h1>Regression tests for TypedMaxentFeatureEncoding</h1>
<blockquote>
<pre class="doctest-block">
&gt;&gt;&gt; from nltk.classify import maxent
&gt;&gt;&gt; train = [
...     ({'a': 1, 'b': 1, 'c': 1}, 'y'),
...     ({'a': 5, 'b': 5, 'c': 5}, 'x'),
...     ({'a': 0.9, 'b': 0.9, 'c': 0.9}, 'y'),
...     ({'a': 5.5, 'b': 5.4, 'c': 5.3}, 'x'),
...     ({'a': 0.8, 'b': 1.2, 'c': 1}, 'y'),
...     ({'a': 5.1, 'b': 4.9, 'c': 5.2}, 'x')
... ]
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; test = [
...     {'a': 1, 'b': 0.8, 'c': 1.2},
...     {'a': 5.2, 'b': 5.1, 'c': 5}
... ]
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; encoding = maxent.TypedMaxentFeatureEncoding.train(
...     train, count_cutoff=3, alwayson_features=True)
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; classifier = maxent.MaxentClassifier.train(
...     train, bernoulli=False, encoding=encoding, trace=0)
</pre>
<pre class="doctest-block">
&gt;&gt;&gt; classifier.classify_many(test)
['y', 'x']
</pre>
</blockquote>
</div>
</div>
</body>
</html>
