<?xml version="1.0" encoding="ascii" ?>

<script language="javascript" type="text/javascript">

function astext(node)
{
    return node.innerHTML.replace(/(<([^>]+)>)/ig,"")
                         .replace(/&gt;/ig, ">")
                         .replace(/&lt;/ig, "<")
                         .replace(/&quot;/ig, '"')
                         .replace(/&amp;/ig, "&");
}

function copy_notify(node, bar_color, data)
{
    // The outer box: relative + inline positioning.
    var box1 = document.createElement("div");
    box1.style.position = "relative";
    box1.style.display = "inline";
    box1.style.top = "2em";
    box1.style.left = "1em";
  
    // A shadow for fun
    var shadow = document.createElement("div");
    shadow.style.position = "absolute";
    shadow.style.left = "-1.3em";
    shadow.style.top = "-1.3em";
    shadow.style.background = "#404040";
    
    // The inner box: absolute positioning.
    var box2 = document.createElement("div");
    box2.style.position = "relative";
    box2.style.border = "1px solid #a0a0a0";
    box2.style.left = "-.2em";
    box2.style.top = "-.2em";
    box2.style.background = "white";
    box2.style.padding = ".3em .4em .3em .4em";
    box2.style.fontStyle = "normal";
    box2.style.background = "#f0e0e0";

    node.insertBefore(box1, node.childNodes.item(0));
    box1.appendChild(shadow);
    shadow.appendChild(box2);
    box2.innerHTML="Copied&nbsp;to&nbsp;the&nbsp;clipboard: " +
                   "<pre class='copy-notify'>"+
                   data+"</pre>";
    setTimeout(function() { node.removeChild(box1); }, 1000);

    var elt = node.parentNode.firstChild;
    elt.style.background = "#ffc0c0";
    setTimeout(function() { elt.style.background = bar_color; }, 200);
}

function copy_codeblock_to_clipboard(node)
{
    var data = astext(node)+"\n";
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#40a060", data);
    }
}

function copy_doctest_to_clipboard(node)
{
    var s = astext(node)+"\n   ";
    var data = "";

    var start = 0;
    var end = s.indexOf("\n");
    while (end >= 0) {
        if (s.substring(start, start+4) == ">>> ") {
            data += s.substring(start+4, end+1);
        }
        else if (s.substring(start, start+4) == "... ") {
            data += s.substring(start+4, end+1);
        }
        /*
        else if (end-start > 1) {
            data += "# " + s.substring(start, end+1);
        }*/
        // Grab the next line.
        start = end+1;
        end = s.indexOf("\n", start);
    }
    
    if (copy_text_to_clipboard(data)) {
        copy_notify(node, "#4060a0", data);
    }
}
    
function copy_text_to_clipboard(data)
{
    if (window.clipboardData) {
        window.clipboardData.setData("Text", data);
        return true;
     }
    else if (window.netscape) {
        // w/ default firefox settings, permission will be denied for this:
        netscape.security.PrivilegeManager
                      .enablePrivilege("UniversalXPConnect");
    
        var clip = Components.classes["@mozilla.org/widget/clipboard;1"]
                      .createInstance(Components.interfaces.nsIClipboard);
        if (!clip) return;
    
        var trans = Components.classes["@mozilla.org/widget/transferable;1"]
                       .createInstance(Components.interfaces.nsITransferable);
        if (!trans) return;
    
        trans.addDataFlavor("text/unicode");
    
        var str = new Object();
        var len = new Object();
    
        var str = Components.classes["@mozilla.org/supports-string;1"]
                     .createInstance(Components.interfaces.nsISupportsString);
        var datacopy=data;
        str.data=datacopy;
        trans.setTransferData("text/unicode",str,datacopy.length*2);
        var clipid=Components.interfaces.nsIClipboard;
    
        if (!clip) return false;
    
        clip.setData(trans,null,clipid.kGlobalClipboard);
        return true;
    }
    return false;
}
//-->
</script>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii" />
<meta name="generator" content="Docutils 0.15: http://docutils.sourceforge.net/" />
<title>11. Managing Linguistic Data</title>
<style type="text/css">

/*
:Author: Edward Loper, James Curran
:Copyright: This stylesheet has been placed in the public domain.

Stylesheet for use with Docutils.

This stylesheet defines new css classes used by NLTK.

It uses a Python syntax highlighting scheme that matches
the colour scheme used by IDLE, which makes it easier for
beginners to check they are typing things in correctly.
*/

/* Include the standard docutils stylesheet. */
@import url(default.css);

/* Custom inline roles */
span.placeholder    { font-style: italic; font-family: monospace; }
span.example        { font-style: italic; }
span.emphasis       { font-style: italic; }
span.termdef        { font-weight: bold; }
/*span.term           { font-style: italic; }*/
span.category       { font-variant: small-caps; }
span.feature        { font-variant: small-caps; }
span.fval           { font-style: italic; }
span.math           { font-style: italic; }
span.mathit         { font-style: italic; }
span.lex            { font-variant: small-caps; }
span.guide-linecount{ text-align: right; display: block;}

/* Python souce code listings */
span.pysrc-prompt   { color: #9b0000; }
span.pysrc-more     { color: #9b00ff; }
span.pysrc-keyword  { color: #e06000; }
span.pysrc-builtin  { color: #940094; }
span.pysrc-string   { color: #00aa00; }
span.pysrc-comment  { color: #ff0000; }
span.pysrc-output   { color: #0000ff; }
span.pysrc-except   { color: #ff0000; }
span.pysrc-defname  { color: #008080; }


/* Doctest blocks */
pre.doctest         { margin: 0; padding: 0; font-weight: bold; }
div.doctest         { margin: 0 1em 1em 1em; padding: 0; }
table.doctest       { margin: 0; padding: 0;
                      border-top: 1px solid gray;
                      border-bottom: 1px solid gray; }
pre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;
                      background-color: #ffffff; }

/* Python source listings */
div.pylisting       { margin: 0 1em 1em 1em; padding: 0; }
table.pylisting     { margin: 0; padding: 0;
                      border-top: 1px solid gray; }
td.caption { border-top: 1px solid black; margin: 0; padding: 0; }
.caption-label { font-weight: bold;  }
td.caption p { margin: 0; padding: 0; font-style: normal;}

table tr td.codeblock { 
  padding: 0.2em ! important; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeffee;
}
table pre span {
  white-space: pre-wrap;
}
table tr td.doctest  { 
  padding: 0.2em; margin: 0;
  border-left: 1px solid gray;
  border-right: 2px solid gray;
  border-top: 0px solid gray;
  border-bottom: 1px solid gray;
  font-weight: bold; background-color: #eeeeff;
}

td.codeblock table tr td.copybar {
    background: #40a060; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }
td.doctest table tr td.copybar {
    background: #4060a0; border: 1px solid gray;
    font-family: monospace; padding: 0; margin: 0; }

td.pysrc { padding-left: 0.5em; }

img.callout { border-width: 0px; }

table.docutils {
    border-style: solid;
    border-width: 1px;
    margin-top: 6px;
    border-color: grey;
    border-collapse: collapse; }

table.docutils th {
    border-style: none;
    border-width: 1px;
    border-color: grey;
    padding: 0 .5em 0 .5em; }

table.docutils td {
    border-style: none;
    border-width: 1px;
    border-color: grey; 
    padding: 0 .5em 0 .5em; }

table.footnote td { padding: 0; }
table.footnote { border-width: 0; }
table.footnote td { border-width: 0; }
table.footnote th { border-width: 0; }

table.noborder { border-width: 0; }

table.example pre { margin-top: 4px; margin-bottom: 0; }

/* For figures & tables */
p.caption { margin-bottom: 0; }
div.figure { text-align: center; }

/* The index */
div.index { border: 1px solid black;
            background-color: #eeeeee; }
div.index h1 { padding-left: 0.5em; margin-top: 0.5ex;
               border-bottom: 1px solid black; }
ul.index { margin-left: 0.5em; padding-left: 0; }
li.index { list-style-type: none; }
p.index-heading { font-size: 120%; font-style: italic; margin: 0; }
li.index ul { margin-left: 2em; padding-left: 0; }

/* 'Note' callouts */
div.note
{
  border-right:   #87ceeb 1px solid;
  padding-right: 4px;
  border-top: #87ceeb 1px solid;
  padding-left: 4px;
  padding-bottom: 4px;
  margin: 2px 5% 10px;
  border-left: #87ceeb 1px solid;
  padding-top: 4px;
  border-bottom: #87ceeb 1px solid;
  font-style: normal;
  font-family: verdana, arial;
  background-color: #b0c4de;
}

table.avm { border: 0px solid black; width: 0; }
table.avm tbody tr {border: 0px solid black; }
table.avm tbody tr td { padding: 2px; }
table.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }
table.avm tbody tr td.avm-eq { padding: 5px; }
table.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }
p.avm-empty { font-style: normal; }
table.avm colgroup col { border: 0px solid black; }
table.avm tbody tr td.avm-topleft 
    { border-left: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botleft 
    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-topright
    { border-right: 2px solid #000080; border-top: 2px solid #000080; }
table.avm tbody tr td.avm-botright
    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }
table.avm tbody tr td.avm-left
    { border-left: 2px solid #000080; }
table.avm tbody tr td.avm-right
    { border-right: 2px solid #000080; }
table.avm tbody tr td.avm-topbotleft
    { border: 2px solid #000080; border-right: 0px solid black; }
table.avm tbody tr td.avm-topbotright
    { border: 2px solid #000080; border-left: 0px solid black; }
table.avm tbody tr td.avm-ident
    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }
.avm-pointer
{ border: 1px solid #008000; padding: 1px; color: #008000; 
  background: #c0ffc0; font-style: normal; }

table.gloss { border: 0px solid black; width: 0; }
table.gloss tbody tr { border: 0px solid black; }
table.gloss tbody tr td { border: 0px solid black; }
table.gloss colgroup col { border: 0px solid black; }
table.gloss p { margin: 0; padding: 0; }

table.rst-example { border: 1px solid black; }
table.rst-example tbody tr td { background: #eeeeee; }
table.rst-example thead tr th { background: #c0ffff; }
td.rst-raw { width: 0; }

/* Used by nltk.org/doc/test: */
div.doctest-list { text-align: center; }
table.doctest-list { border: 1px solid black;
  margin-left: auto; margin-right: auto;
}
table.doctest-list tbody tr td { background: #eeeeee;
  border: 1px solid #cccccc; text-align: left; }
table.doctest-list thead tr th { background: #304050; color: #ffffff;
  border: 1px solid #000000;}
table.doctest-list thead tr a { color: #ffffff; }
span.doctest-passed { color: #008000; }
span.doctest-failed { color: #800000; }

</style>
</head>
<body>
<div class="document" id="managing-linguistic-data">
<span id="chap-data"></span>
<h1 class="title">11. Managing Linguistic Data</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words 'inside' a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- standard global imports

>>> from __future__ import division
>>> import nltk, re, pprint -->
<!-- TODO: random syllables according to a canon "%s%s" % (random.choice('ptkbdg'), random.choice('aeiou')) -->
<!-- TODO: linguistic annotation -->
<!-- TODO: paradigms -->
<!-- TODO: XML query for access to treebank data (load tree as XML) -->
<!-- TODO: mention GOLD ontology -->
<!-- TODO: building resources for small languages, e.g. spell checker (cf Devonish question) -->
<!-- TODO: part of managing data is tool building...  analyzing needs... -->
<!-- TODO: mention language standards -->
<!-- TODO: instantiating a new corpus reader -->
<p>Structured collections of annotated linguistic data are essential in most areas of NLP,
however, we still face many obstacles in using them.
The goal of this chapter is to answer the following questions:</p>
<ol class="arabic simple">
<li>How do we design a new language resource and ensure that its
coverage, balance, and documentation support a wide range of uses?</li>
<li>When existing data is in the wrong format for some analysis tool,
how can we convert it to a suitable format?</li>
<li>What is a good way to document the existence of a resource we
have created so that others can easily find it?</li>
</ol>
<p>Along the way, we will study the design of existing corpora, the
typical workflow for creating a corpus, and the lifecycle of corpus.
As in other chapters, there will be many examples drawn from
practical experience managing linguistic data, including
data that has been collected in the course of linguistic fieldwork,
laboratory work, and web crawling.</p>
<div class="section" id="corpus-structure-a-case-study">
<h1>1&nbsp;&nbsp;&nbsp;Corpus Structure: a Case Study</h1>
<p>The TIMIT corpus of read speech was the first annotated speech database to be
widely distributed, and it has an especially clear organization.
TIMIT was developed by a consortium including Texas Instruments and MIT, from which
it derives its name.
It was designed to provide data for the acquisition of acoustic-phonetic knowledge and to
support the development and evaluation of automatic speech recognition systems.</p>
<div class="section" id="the-structure-of-timit">
<h2>1.1&nbsp;&nbsp;&nbsp;The Structure of TIMIT</h2>
<p>Like the Brown Corpus, which displays a balanced selection of text genres and sources,
TIMIT includes a balanced selection of dialects, speakers, and materials.  For each of
eight dialect regions, 50 male and female speakers having a range of ages and educational
backgrounds each read ten carefully chosen sentences.  Two sentences, read by all
speakers, were designed to bring out dialect variation:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>she had your dark suit in greasy wash water all year</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>don't ask me to carry an oily rag like that</td></tr></table></p>
</td></tr></table></p>
<p>The remaining sentences were chosen to be phonetically rich, involving all phones (sounds) and
a comprehensive range of diphones (phone bigrams).  Additionally, the design strikes a balance
between multiple speakers saying the same sentence in order to permit comparison across
speakers, and having a large range of sentences covered by the corpus to get maximal
coverage of diphones.  Five of the sentences read by each speaker are also read by six other
speakers (for comparability).  The remaining three sentences read by each speaker were unique
to that speaker (for coverage).</p>
<p>NLTK includes a sample from the TIMIT corpus.  You can access its documentation in the usual
way, using <tt class="doctest"><span class="pre">help(nltk.corpus.timit)</span></tt>.  Print <tt class="doctest"><span class="pre">nltk.corpus.timit.fileids()</span></tt> to see a list of the
160 recorded utterances in the corpus sample.
Each file name has internal structure as shown in <a class="reference internal" href="#fig-timit">1.1</a>.</p>
<span class="target" id="fig-timit"></span><div class="figure" id="fig-timit">
<img alt="../images/timit.png" src="../images/timit.png" style="width: 475.2px; height: 403.8px;" />
<p class="caption"><span class="caption-label">Figure 1.1</span>: Structure of a TIMIT Identifier: Each recording is labeled using a string made
up of the speaker's dialect region, gender, speaker identifier, sentence type,
and sentence identifier.</p>
</div>
<p>Each item has a phonetic transcription which can be accessed using the <tt class="doctest"><span class="pre">phones()</span></tt>
method.  We can access the corresponding word tokens in the customary way.  Both access
methods permit an optional argument <tt class="doctest"><span class="pre">offset=True</span></tt> which includes the start and end offsets
of the corresponding span in the audio file.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>phonetic = nltk.corpus.timit.phones(<span class="pysrc-string">'dr1-fvmh0/sa1'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>phonetic
<span class="pysrc-output">['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl',</span>
<span class="pysrc-output">'s', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa',</span>
<span class="pysrc-output">'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.corpus.timit.word_times(<span class="pysrc-string">'dr1-fvmh0/sa1'</span>)
<span class="pysrc-output">[('she', 7812, 10610), ('had', 10610, 14496), ('your', 14496, 15791),</span>
<span class="pysrc-output">('dark', 15791, 20720), ('suit', 20720, 25647), ('in', 25647, 26906),</span>
<span class="pysrc-output">('greasy', 26906, 32668), ('wash', 32668, 37890), ('water', 38531, 42417),</span>
<span class="pysrc-output">('all', 43091, 46052), ('year', 46052, 50522)]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>In addition to this text data, TIMIT includes a lexicon that provides the canonical
pronunciation of every word, which can be compared with a particular utterance:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>timitdict = nltk.corpus.timit.transcription_dict()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>timitdict[<span class="pysrc-string">'greasy'</span>] + timitdict[<span class="pysrc-string">'wash'</span>] + timitdict[<span class="pysrc-string">'water'</span>]
<span class="pysrc-output">['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>phonetic[17:30]
<span class="pysrc-output">['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>This gives us a sense of what a speech processing system
would have to do in producing or recognizing speech in this particular dialect
(New England).  Finally, TIMIT includes demographic data about the speakers,
permitting fine-grained study of vocal, social, and gender characteristics.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.corpus.timit.spkrinfo(<span class="pysrc-string">'dr1-fvmh0'</span>)
<span class="pysrc-output">SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86',</span>
<span class="pysrc-output">birthdate='01/08/60', ht='5\'05&quot;', race='WHT', edu='BS',</span>
<span class="pysrc-output">comments='BEST NEW ENGLAND ACCENT SO FAR')</span></pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="notable-design-features">
<h2>1.2&nbsp;&nbsp;&nbsp;Notable Design Features</h2>
<p>TIMIT illustrates several key features of corpus design.
First, the corpus contains two layers of annotation, at the phonetic and orthographic
levels.  In general, a text or speech corpus may be annotated at many different linguistic levels,
including morphological, syntactic, and discourse levels.  Moreover, even at a given
level there may be different labeling schemes or even disagreement amongst annotators,
such that we want to represent multiple versions.
A second property of TIMIT is its balance across multiple dimensions of variation,
for coverage of dialect regions and diphones.  The inclusion of speaker
demographics brings in many more independent variables, that may help to
account for variation in the data, and which facilitate later uses of the
corpus for purposes that were not envisaged when the corpus was created,
such as sociolinguistics.
A third property is that there is a sharp division between the original
linguistic event captured as an audio recording, and the annotations of that event.
The same holds true of text corpora, in the sense that the original text usually
has an external source, and is considered to be an immutable artifact.  Any transformations
of that artifact which involve human judgment &#8212; even something as
simple as tokenization &#8212; are subject to later revision, thus it is important to
retain the source material in a form that is as close to the original as possible.</p>
<span class="target" id="fig-timit-structure"></span><div class="figure" id="fig-timit-structure">
<img alt="../images/timit-structure.png" src="../images/timit-structure.png" style="width: 566.4px; height: 435.0px;" />
<p class="caption"><span class="caption-label">Figure 1.2</span>: Structure of the Published TIMIT Corpus: The CD-ROM contains doc, train, and test directories
at the top level; the train and test directories both have 8 sub-directories, one per
dialect region; each of these contains further subdirectories, one per speaker;
the contents of the directory for female speaker <tt class="doctest"><span class="pre">aks0</span></tt> are listed, showing
10 <tt class="doctest"><span class="pre">wav</span></tt> files accompanied by a text transcription, a word-aligned transcription,
and a phonetic transcription.</p>
</div>
<p>A fourth feature of TIMIT is the hierarchical structure of the corpus.
With 4 files per sentence, and 10 sentences for each of 500 speakers, there are 20,000 files.  These are
organized into a tree structure, shown schematically in <a class="reference internal" href="#fig-timit-structure">1.2</a>.
At the top level there is a split between training and testing sets, which gives
away its intended use for developing and evaluating statistical models.</p>
<p>Finally, notice that even though TIMIT is a speech corpus, its transcriptions and associated
data are just text, and can be processed using programs just like any other text corpus.
Therefore, many of the computational methods described in this book are applicable.
Moreover, notice that all of the data types included in the TIMIT corpus fall into
the two basic categories of lexicon and text, which we will discuss below.
Even the speaker demographics data is just another instance of the lexicon data type.</p>
<p>This last observation is less surprising when we consider that text and record structures are the
primary domains for the two subfields of computer science that focus on data management,
namely text retrieval and databases.  A notable feature of linguistic data management is
that usually brings both data types together, and that it can draw on results and techniques
from both fields.</p>
</div>
<div class="section" id="fundamental-data-types">
<h2>1.3&nbsp;&nbsp;&nbsp;Fundamental Data Types</h2>
<span class="target" id="fig-datatypes"></span><div class="figure" id="fig-datatypes">
<img alt="../images/datatypes.png" src="../images/datatypes.png" style="width: 625.8px; height: 535.5px;" />
<p class="caption"><span class="caption-label">Figure 1.3</span>: Basic Linguistic Data Types &#8212; Lexicons and Texts: amid their diversity,
lexicons have a record structure, while annotated texts have a temporal organization.</p>
</div>
<p>Despite its complexity, the TIMIT corpus only contains two fundamental data types,
namely lexicons and texts.
As we saw in <a class="reference external" href="ch02.html#chap-corpora">2.</a>, most lexical resources can be represented using
a record structure, i.e. a key plus one or more fields, as
shown in <a class="reference internal" href="#fig-datatypes">1.3</a>.  A lexical resource could be a conventional
dictionary or comparative wordlist, as illustrated.  It could also
be a phrasal lexicon, where the key field is a phrase rather than a single word.
A thesaurus also consists of record-structured data, where we look up entries
via non-key fields that correspond to topics.
We can also construct special tabulations (known as paradigms)
to illustrate contrasts and systematic variation, as shown in
<a class="reference internal" href="#fig-datatypes">1.3</a> for three verbs.  TIMIT's speaker table is also a kind
of lexicon.</p>
<p>At the most abstract level, a text is a representation of a real or fictional speech event,
and the time-course of that event carries over into the text itself.  A text could be a small
unit, such as a word or sentence, or a complete narrative or dialogue.  It may come with
annotations such as part-of-speech tags, morphological analysis, discourse structure, and so forth.
As we saw in the IOB tagging technique (<a class="reference external" href="ch07.html#chap-chunk">7.</a>), it is possible to represent higher-level
constituents using tags on individual words.  Thus the abstraction of text shown in
<a class="reference internal" href="#fig-datatypes">1.3</a> is sufficient.</p>
<p>Despite the complexities and idiosyncrasies of individual corpora, at base they are
collections of texts together with record-structured data.  The contents of a corpus
are often biased towards one or other of these types.
For example, the Brown Corpus contains 500 text files, but we still use a table to relate
the files to 15 different genres.  At the other end of the spectrum, WordNet
contains 117,659 synset records, yet it incorporates many example sentences (mini-texts)
to illustrate word usages.  TIMIT is an interesting mid-point on this spectrum, containing substantial
free-standing material of both the text and lexicon types.</p>
</div>
</div>
<div class="section" id="the-life-cycle-of-a-corpus">
<span id="sec-life-cycle-of-a-corpus"></span><h1>2&nbsp;&nbsp;&nbsp;The Life-Cycle of a Corpus</h1>
<p>Corpora are not born fully-formed, but involve careful preparation
and input from many people over an extended period.  Raw data needs
to be collected, cleaned up, documented, and stored in a systematic
structure.  Various layers of annotation might be applied, some requiring
specialized knowledge of the morphology or syntax of the language.
Success at this stage depends on creating an efficient workflow
involving appropriate tools and format converters.
Quality control procedures can be put in place to find inconsistencies
in the annotations, and to ensure the highest
possible level of inter-annotator agreement.  Because of the
scale and complexity of the task, large corpora may take years to
prepare, and involve tens or hundreds of person-years of effort.
In this section we briefly review the various stages in the
life-cycle of a corpus.</p>
<div class="section" id="three-corpus-creation-scenarios">
<h2>2.1&nbsp;&nbsp;&nbsp;Three Corpus Creation Scenarios</h2>
<p>In one type of corpus, the design unfolds over
in the course of the creator's explorations. This is the pattern
typical of traditional &quot;field linguistics,&quot; in which material from
elicitation sessions is analyzed as it is gathered, with tomorrow's elicitation often based
on questions that arise in analyzing today's. The resulting corpus
is then used during subsequent years of research, and may serve
as an archival resource indefinitely.  Computerization is an
obvious boon to work of this type, as exemplified by the popular
program Shoebox, now over two decades old and re-released as Toolbox
(see <a class="reference external" href="ch02.html#sec-lexical-resources">4</a>).
Other software tools, even simple word processors and spreadsheets, are routinely
used to acquire the data.  In the next section we will look at how to extract
data from these sources.</p>
<p>Another corpus creation scenario is typical of experimental research
where a body of carefully-designed material is collected from a range of human subjects,
then analyzed to evaluate a hypothesis or develop a technology.
It has become common for such databases to be shared and re-used
within a laboratory or company, and often to be published more
widely. Corpora of this type are the basis of the
&quot;common task&quot; method of research management, which over the
past two decades has become the norm in government-funded research
programs in language technology.
We have already encountered many such corpora in the earlier chapters;
we will see how to write Python programs to implement the kinds of
curation tasks that are necessary before such corpora are published.</p>
<p>Finally, there are efforts to gather a &quot;reference corpus&quot; for a
particular language, such as the <em>American National Corpus</em> (ANC)
and the <em>British National Corpus</em> (BNC). Here the goal
has been to produce a comprehensive record of the
many forms, styles and uses of a language.
Apart from the sheer challenge of scale, there is a heavy reliance
on automatic annotation tools together with post-editing to
fix any errors.  However, we can write programs to locate
and repair the errors, and also to analyze the corpus for balance.</p>
</div>
<div class="section" id="quality-control">
<h2>2.2&nbsp;&nbsp;&nbsp;Quality Control</h2>
<p>Good tools for automatic and manual preparation of data
are essential.  However the creation of a high-quality corpus depends just
as much on such mundane things as documentation, training, and workflow.
Annotation guidelines define the task and document the markup
conventions.  They may be regularly updated to cover difficult
cases, along with new rules that are devised to achieve more
consistent annotations.  Annotators need to be trained in the
procedures, including methods for resolving cases not covered
in the guidelines.  A workflow needs to be established, possibly
with supporting software, to keep track of which files have been initialized,
annotated, validated, manually checked, and so on.  There may be multiple layers of
annotation, provided by different specialists.  Cases of uncertainty
or disagreement may require adjudication.</p>
<p>Large annotation tasks require multiple annotators, which
raises the problem of achieving consistency.
How consistently can a group of annotators perform?
We can easily measure consistency by having a portion of
the source material independently annotated by two people.
This may reveal shortcomings in the guidelines or
differing abilities with the annotation task.
In cases where quality is paramount, the entire corpus can
be annotated twice, and any inconsistencies adjudicated
by an expert.</p>
<p>It is considered best practice to report the inter-annotator
agreement that was achieved for a corpus (e.g. by double-annotating
10% of the corpus).  This score serves as a helpful upper bound on
the expected performance of any automatic system that is trained
on this corpus.</p>
<div class="admonition caution">
<p class="first admonition-title">Caution!</p>
<p class="last">Care should be exercised when interpreting an inter-annotator
agreement score, since annotation tasks vary greatly in their
difficulty.  For example, 90% agreement would be a terrible
score for part-of-speech tagging, but an exceptional score
for semantic role labeling.</p>
</div>
<p>The <a name="kappa_index_term" /><span class="termdef">Kappa</span> coefficient K measures agreement between
two people making category judgments, correcting for expected
chance agreement.  For example, suppose an item is to be annotated,
and four coding options are equally likely.
Then two people coding randomly would be expected to agree 25% of the time.
Thus, an agreement of 25% will be assigned K = 0, and better
levels of agreement will be scaled accordingly.
For an agreement of 50%, we would get
K = 0.333, as 50 is a third of the way from 25 to 100.
Many other agreement measures exist; see <tt class="doctest"><span class="pre">help(nltk.metrics.agreement)</span></tt> for details.</p>
<span class="target" id="fig-windowdiff"></span><div class="figure" id="fig-windowdiff">
<img alt="../images/windowdiff.png" src="../images/windowdiff.png" style="width: 399.75px; height: 100.5px;" />
<p class="caption"><span class="caption-label">Figure 2.1</span>: Three Segmentations of a Sequence: The small rectangles represent characters,
words, sentences, in short, any sequence which might be divided into
linguistic units; S<sub>1</sub> and S<sub>2</sub> are in close
agreement, but both differ significantly from S<sub>3</sub>.</p>
</div>
<p>We can also measure the agreement between two independent segmentations
of language input, e.g. for tokenization, sentence segmentation,
named-entity detection.  In <a class="reference internal" href="#fig-windowdiff">2.1</a> we see three
possible segmentations of a sequence of items
which might have been produced by annotators (or programs).
Although none of them agree exactly, S<sub>1</sub> and S<sub>2</sub>
are in close agreement, and we would like a suitable measure.
Windowdiff is a simple algorithm for evaluating the agreement of
two segmentations by running a sliding window over the
data and awarding partial credit for near misses.
If we preprocess our tokens into a sequence of zeros and ones, to
record when a token is followed by a boundary, we can represent the
segmentations as strings, and apply the windowdiff scorer.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>s1 = <span class="pysrc-string">&quot;00000010000000001000000&quot;</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>s2 = <span class="pysrc-string">&quot;00000001000000010000000&quot;</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>s3 = <span class="pysrc-string">&quot;00010000000000000001000&quot;</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.windowdiff(s1, s1, 3)
<span class="pysrc-output">0.0</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.windowdiff(s1, s2, 3)
<span class="pysrc-output">0.190...</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>nltk.windowdiff(s2, s3, 3)
<span class="pysrc-output">0.571...</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>In the above example, the window had a size of 3.
The windowdiff computation slides this window across a pair of strings.
At each position it totals up the number of boundaries found inside this
window, for both strings, then computes the difference.  These differences
are then summed.
We can increase or shrink the window size to control the sensitivity of the
measure.</p>
</div>
<div class="section" id="curation-vs-evolution">
<h2>2.3&nbsp;&nbsp;&nbsp;Curation vs Evolution</h2>
<p>As large corpora are published, researchers are increasingly
likely to base their investigations on balanced, focused
subsets that were derived from corpora produced for entirely
different reasons.  For instance, the Switchboard database,
originally collected for speaker identification research,
has since been used as the basis for published studies in speech recognition,
word pronunciation, disfluency, syntax, intonation and discourse structure.
The motivations for recycling linguistic corpora include the
desire to save time and effort, the desire to work on material
available to others for replication, and sometimes a desire to study
more naturalistic forms of linguistic behavior than would be possible
otherwise. The process of choosing a subset for such a study may
count as a non-trivial contribution in itself.</p>
<p>In addition to selecting an appropriate subset of a corpus, this
new work could involve reformatting a text file
(e.g. converting to XML), renaming files, retokenizing the text,
selecting a subset of the data to enrich, and so forth.
Multiple research groups might do this work independently, as illustrated
in <a class="reference internal" href="#fig-evolution">2.2</a>.  At a later date, should someone want to combine sources
of information from different versions, the task will probably be extremely onerous.</p>
<span class="target" id="fig-evolution"></span><div class="figure" id="fig-evolution">
<img alt="../images/evolution.png" src="../images/evolution.png" style="width: 780.12px; height: 222.12px;" />
<p class="caption"><span class="caption-label">Figure 2.2</span>: Evolution of a Corpus over Time: After a corpus is published, research
groups will use it independently, selecting and enriching different pieces;
later research that seeks to integrate separate annotations confronts
the difficult challenge of aligning the annotations.</p>
</div>
<p>The task of using derived corpora is made even more difficult by the lack of
any record about how the derived version was created, and which version is the most
up-to-date.</p>
<p>An alternative to this chaotic situation is for a corpus to be centrally curated,
and for committees of experts to revise and extend it at periodic
intervals, considering submissions from third-parties, and publishing new releases
from time to time.  Print dictionaries and national corpora may be centrally curated in
this way.  However, for most corpora this model is simply impractical.</p>
<p>A middle course is for the original corpus publication to have a scheme for identifying
any sub-part.  Each sentence, tree, or lexical entry, could have a globally unique
identifier, and each token, node or field (respectively) could have a relative offset.
Annotations, including segmentations, could reference the source using
this identifier scheme (a method which is known as <a name="standoff_annotation_index_term" /><span class="termdef">standoff annotation</span>).
This way, new annotations could be distributed independently of the source, and
multiple independent annotations of the same source could be
compared and updated without touching the source.</p>
<p>If the corpus publication is provided in multiple versions, the version
number or date could be part of the identification scheme.
A table of correspondences between identifiers across editions of the corpus
would permit any standoff annotations to be updated easily.</p>
<div class="admonition caution">
<p class="first admonition-title">Caution!</p>
<p class="last">Sometimes an updated corpus contains revisions of base material that
has been externally annotated.  Tokens might be split or merged, and constituents
may have been rearranged.  There may not be a one-to-one correspondence between
old and new identifiers.  It is better to cause standoff annotations to break
on such components of the new version than to silently allow their identifiers
to refer to incorrect locations.</p>
</div>
</div>
</div>
<div class="section" id="acquiring-data">
<h1>3&nbsp;&nbsp;&nbsp;Acquiring Data</h1>
<div class="section" id="obtaining-data-from-the-web">
<h2>3.1&nbsp;&nbsp;&nbsp;Obtaining Data from the Web</h2>
<p>The Web is a rich source of data for language analysis purposes.  We have already
discussed methods for accessing individual files, RSS feeds, and search engine
results (see <a class="reference external" href="ch03.html#sec-accessing-text">3.1</a>).  However, in some cases we want to
obtain large quantities of web text.</p>
<p>The simplest approach is to obtain a published corpus of web text.  The ACL
Special Interest Group on Web as Corpus (SIGWAC) maintains a list of resources
at <tt class="doctest"><span class="pre">http://www.sigwac.org.uk/</span></tt>.
The advantage of using a well-defined web corpus is that they are documented,
stable, and permit reproducible experimentation.</p>
<p>If the desired content is localized to a particular website, there are many
utilities for capturing all the accessible contents of a site, such as
<em>GNU Wget</em> <tt class="doctest"><span class="pre">http://www.gnu.org/software/wget/</span></tt>.
For maximal flexibility and control, a web crawler can be used,
such as <em>Heritrix</em> <tt class="doctest"><span class="pre">http://crawler.archive.org/</span></tt>.
Crawlers permit fine-grained control over where to look, which links to
follow, and how to organize the results <a class="reference external" href="bibliography.html#croft2009" id="id1">(Croft, Metzler, &amp; Strohman, 2009)</a>.
For example, if we want to compile a
bilingual text collection having corresponding pairs of documents in each language,
the crawler needs to detect the structure of the site in order to extract the
correspondence between the documents, and it needs to organize the downloaded
pages in such a way that the correspondence is captured.  It might be tempting
to write your own web-crawler, but there are dozens of pitfalls to do with
detecting MIME types, converting relative to absolute URLs, avoiding getting
trapped in cyclic link structures, dealing with network latencies, avoiding
overloading the site or being banned from accessing the site, and so on.</p>
</div>
<div class="section" id="obtaining-data-from-word-processor-files">
<h2>3.2&nbsp;&nbsp;&nbsp;Obtaining Data from Word Processor Files</h2>
<p>Word processing software is often used in the manual preparation
of texts and lexicons in projects that have limited computational
infrastructure.  Such projects often provide templates for data
entry, though the word processing software does not ensure that
the data is correctly structured.  For example, each text may
be required to have a title and date.  Similarly, each lexical
entry may have certain obligatory fields.
As the data grows in size and complexity, a larger
proportion of time may be spent maintaining its consistency.</p>
<p>How can we extract the content of such files so
that we can manipulate it in external programs?
Moreover, how can we validate the content of these files to
help authors create well-structured data, so that the
quality of the data can be maximized in the context of
the original authoring process?</p>
<p>Consider a dictionary in
which each entry has a part-of-speech field, drawn from a set of 20
possibilities, displayed after the pronunciation field, and rendered
in 11-point bold.  No conventional word processor has search or macro
functions capable of verifying that all part-of-speech fields have
been correctly entered and displayed.  This task requires exhaustive
manual checking.  If the word processor permits the document to be
saved in a non-proprietary format, such as text, HTML, or XML, we can
sometimes write programs to do this checking automatically.</p>
<p>Consider the following fragment of a lexical entry:
&quot;sleep [sli:p] <strong>v.i.</strong> <em>condition of body and mind...</em>&quot;.
We can enter this in MSWord, then &quot;Save as Web Page&quot;,
then inspect the resulting HTML file:</p>
<pre class="literal-block">
&lt;p class=MsoNormal&gt;sleep
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  [&lt;span class=SpellE&gt;sli:p&lt;/span&gt;]
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  &lt;b&gt;&lt;span style='font-size:11.0pt'&gt;v.i.&lt;/span&gt;&lt;/b&gt;
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  &lt;i&gt;a condition of body and mind ...&lt;o:p&gt;&lt;/o:p&gt;&lt;/i&gt;
&lt;/p&gt;
</pre>
<p>Observe that the entry is represented as an HTML paragraph, using the
<tt class="doctest"><span class="pre">&lt;p&gt;</span></tt> element, and that the part of speech appears inside a <tt class="doctest"><span class="pre">&lt;span
style=<span class="pysrc-string">'font-size:11.0pt'</span>&gt;</span></tt> element.  The following program defines
the set of legal parts-of-speech, <tt class="doctest"><span class="pre">legal_pos</span></tt>.  Then it extracts all
11-point content from the <tt class="doctest"><span class="pre">dict.htm</span></tt> file and stores it in the set
<tt class="doctest"><span class="pre">used_pos</span></tt>.  Observe that the search pattern contains a
parenthesized sub-expression; only the material that matches this
sub-expression is returned by <tt class="doctest"><span class="pre">re.findall</span></tt>.  Finally, the program
constructs the set of illegal parts-of-speech as <tt class="doctest"><span class="pre">used_pos -
legal_pos</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>legal_pos = set([<span class="pysrc-string">'n'</span>, <span class="pysrc-string">'v.t.'</span>, <span class="pysrc-string">'v.i.'</span>, <span class="pysrc-string">'adj'</span>, <span class="pysrc-string">'det'</span>])
<span class="pysrc-prompt">&gt;&gt;&gt; </span>pattern = re.compile(r<span class="pysrc-string">&quot;'font-size:11.0pt'&gt;([a-z.]+)&lt;&quot;</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>document = open(<span class="pysrc-string">&quot;dict.htm&quot;</span>, encoding=<span class="pysrc-string">&quot;windows-1252&quot;</span>).read()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>used_pos = set(re.findall(pattern, document))
<span class="pysrc-prompt">&gt;&gt;&gt; </span>illegal_pos = used_pos.difference(legal_pos)
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(list(illegal_pos))
<span class="pysrc-output">['v.i', 'intrans']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>This simple program represents the tip of the iceberg.  We can develop
sophisticated tools to check the consistency of word processor files,
and report errors so that the maintainer of the dictionary can correct
the original file <em>using the original word processor</em>.</p>
<p>Once we know the data is correctly formatted, we
can write other programs to convert the data into a different format.
The program in <a class="reference internal" href="#code-html2csv">3.1</a> strips out the HTML markup using the <tt class="doctest"><span class="pre">BeautifulSoup</span></tt> library,
extracts the words and their pronunciations, and generates output
in &quot;comma-separated value&quot; (CSV) format.</p>
<span class="target" id="code-html2csv"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-keyword">from</span> bs4 <span class="pysrc-keyword">import</span> BeautifulSoup

<span class="pysrc-keyword">def</span> <span class="pysrc-defname">lexical_data</span>(html_file, encoding=<span class="pysrc-string">&quot;utf-8&quot;</span>):
    SEP = <span class="pysrc-string">'_ENTRY'</span>
    html = open(html_file, encoding=encoding).read()
    html = re.sub(r<span class="pysrc-string">'&lt;p'</span>, SEP + <span class="pysrc-string">'&lt;p'</span>, html)
    text = BeautifulSoup(html, <span class="pysrc-string">'html.parser'</span>).get_text()
    text = <span class="pysrc-string">' '</span>.join(text.split())
    <span class="pysrc-keyword">for</span> entry <span class="pysrc-keyword">in</span> text.split(SEP):
        <span class="pysrc-keyword">if</span> entry.count(<span class="pysrc-string">' '</span>) &gt; 2:
            yield entry.split(<span class="pysrc-string">' '</span>, 3)</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">import</span> csv
<span class="pysrc-prompt">&gt;&gt;&gt; </span>writer = csv.writer(open(<span class="pysrc-string">&quot;dict1.csv&quot;</span>, <span class="pysrc-string">&quot;w&quot;</span>, encoding=<span class="pysrc-string">&quot;utf-8&quot;</span>))
<span class="pysrc-prompt">&gt;&gt;&gt; </span>writer.writerows(lexical_data(<span class="pysrc-string">&quot;dict.htm&quot;</span>, encoding=<span class="pysrc-string">&quot;windows-1252&quot;</span>))</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_html2csv.py" type="text/x-python"><span class="caption-label">Example 3.1 (code_html2csv.py)</span></a>: <span class="caption-label">Figure 3.1</span>: Converting HTML Created by Microsoft Word into Comma-Separated Values</td></tr></p>
</table></div>
<dl class="docutils">
<dt>with gzip.open(fn+&quot;.gz&quot;,&quot;wb&quot;) as f_out:</dt>
<dd>f_out.write(bytes(s, 'UTF-8'))</dd>
</dl>
</div>
<div class="section" id="obtaining-data-from-spreadsheets-and-databases">
<h2>3.3&nbsp;&nbsp;&nbsp;Obtaining Data from Spreadsheets and Databases</h2>
<p>Spreadsheets are often used for acquiring wordlists or paradigms.
For example, a comparative wordlist may be created using a spreadsheet,
with a row for each cognate set, and a column for each language
(cf. <tt class="doctest"><span class="pre">nltk.corpus.swadesh</span></tt>, and <tt class="doctest"><span class="pre">www.rosettaproject.org</span></tt>).
Most spreadsheet software can export their data in CSV
&quot;comma-separated value&quot; format.  As we see below, it is easy for
Python programs to access these using the <tt class="doctest"><span class="pre">csv</span></tt> module.</p>
<p>Sometimes lexicons are stored in a full-fledged relational database.
When properly normalized, these databases can ensure the validity
of the data.  For example, we can
require that all parts-of-speech come from a specified vocabulary by
declaring that the part-of-speech field is an <em>enumerated type</em>
or a foreign key that references a separate part-of-speech table.
However, the relational model requires the structure of the data
(the schema) be declared in advance, and this runs counter to
the dominant approach to structuring linguistic data, which is
highly exploratory.  Fields which were assumed to be obligatory
and unique often turn out to be optional and repeatable.  A relational
database can accommodate this when it is fully known in advance, however
if it is not, or if just about every property turns out to be optional
or repeatable, the relational approach is unworkable.</p>
<p>Nevertheless, when our goal is simply to extract the contents from a database,
it is enough to dump out the tables (or SQL query results)
in CSV format and load them into our program.  Our program might perform
a linguistically motivated query which cannot be expressed in SQL, e.g.
<em>select all words that appear
in example sentences for which no dictionary entry is provided</em>.
For this task, we would need to extract enough information from a record
for it to be uniquely identified, along with the headwords and example
sentences.  Let's suppose this information was now available in a CSV file
<tt class="doctest"><span class="pre">dict.csv</span></tt>:</p>
<pre class="literal-block">
&quot;sleep&quot;,&quot;sli:p&quot;,&quot;v.i&quot;,&quot;a condition of body and mind ...&quot;
&quot;walk&quot;,&quot;wo:k&quot;,&quot;v.intr&quot;,&quot;progress by lifting and setting down each foot ...&quot;
&quot;wake&quot;,&quot;weik&quot;,&quot;intrans&quot;,&quot;cease to sleep&quot;
</pre>
<p>Now we can express this query as shown below:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">import</span> csv
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = csv.reader(open(<span class="pysrc-string">'dict.csv'</span>))
<span class="pysrc-prompt">&gt;&gt;&gt; </span>pairs = [(lexeme, defn) <span class="pysrc-keyword">for</span> (lexeme, _, _, defn) <span class="pysrc-keyword">in</span> lexicon]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexemes, defns = zip(*pairs)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>defn_words = set(w <span class="pysrc-keyword">for</span> defn <span class="pysrc-keyword">in</span> defns <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> defn.split())
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sorted(defn_words.difference(lexemes))
<span class="pysrc-output">['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',</span>
<span class="pysrc-output">'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>This information would then guide the ongoing work to enrich the lexicon,
work that updates the content of the relational database.</p>
</div>
<div class="section" id="converting-data-formats">
<h2>3.4&nbsp;&nbsp;&nbsp;Converting Data Formats</h2>
<p>Annotated linguistic data rarely arrives in the most convenient format,
and it is often necessary to perform various kinds of format conversion.
Converting between character encodings has already been discussed
(see <a class="reference external" href="ch03.html#sec-unicode">3.3</a>).  Here we focus on the structure of the data.</p>
<p>In the simplest case, the input and output formats are isomorphic.
For instance, we might be converting lexical data from Toolbox
format to XML, and it is straightforward to transliterate the
entries one at a time (<a class="reference internal" href="#sec-working-with-xml">4</a>).  The structure
of the data is reflected in the structure of the required
program: a <tt class="doctest"><span class="pre"><span class="pysrc-keyword">for</span></span></tt> loop whose body takes care
of a single entry.</p>
<p>In another common case, the output is a digested form of the
input, such as an inverted file index.  Here it is necessary
to build an index structure in memory (see <a class="reference external" href="ch04.html#code-search-documents">4.8</a>),
then write it to a file in the desired format.
The following example constructs an index that maps
the words of a dictionary definition to the corresponding
lexeme <a class="reference internal" href="#map-word-lexeme"><span id="ref-map-word-lexeme"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> for each lexical entry <a class="reference internal" href="#lexical-entry"><span id="ref-lexical-entry"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>,
having tokenized the definition text <a class="reference internal" href="#definition-text"><span id="ref-definition-text"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>,
and discarded short words <a class="reference internal" href="#short-words"><span id="ref-short-words"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></span></a>.  Once the index has
been constructed we open a file and then iterate over
the index entries, to write out the lines in the required format <a class="reference internal" href="#required-format"><span id="ref-required-format"><img src="callouts/callout5.gif" alt="[5]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>idx = nltk.Index((defn_word, lexeme) <a name="map-word-lexeme" /><a href="#ref-map-word-lexeme"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-more">... </span>                 <span class="pysrc-keyword">for</span> (lexeme, defn) <span class="pysrc-keyword">in</span> pairs <a name="lexical-entry" /><a href="#ref-lexical-entry"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a>
<span class="pysrc-more">... </span>                 <span class="pysrc-keyword">for</span> defn_word <span class="pysrc-keyword">in</span> nltk.word_tokenize(defn) <a name="definition-text" /><a href="#ref-definition-text"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></a>
<span class="pysrc-more">... </span>                 <span class="pysrc-keyword">if</span> len(defn_word) &gt; 3) <a name="short-words" /><a href="#ref-short-words"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></a>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>with open(<span class="pysrc-string">&quot;dict.idx&quot;</span>, <span class="pysrc-string">&quot;w&quot;</span>) <span class="pysrc-keyword">as</span> idx_file:
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">for</span> word <span class="pysrc-keyword">in</span> sorted(idx):
<span class="pysrc-more">... </span>        idx_words = <span class="pysrc-string">', '</span>.join(idx[word])
<span class="pysrc-more">... </span>        idx_line = <span class="pysrc-string">&quot;{}: {}&quot;</span>.format(word, idx_words) <a name="required-format" /><a href="#ref-required-format"><img src="callouts/callout5.gif" alt="[5]" class="callout" /></a>
<span class="pysrc-more">... </span>        <span class="pysrc-keyword">print</span>(idx_line, file=idx_file)</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The resulting file <tt class="doctest"><span class="pre">dict.idx</span></tt> contains the following lines.  (With a larger
dictionary we would expect to find multiple lexemes listed for each index entry.)</p>
<pre class="literal-block">
body: sleep
cease: wake
condition: sleep
down: walk
each: walk
foot: walk
lifting: walk
mind: sleep
progress: walk
setting: walk
sleep: wake
</pre>
<p>In some cases, the input and output data both consist of two or more dimensions.
For instance, the input might be a set of files, each containing a single
column of word frequency data.  The required output might be a two-dimensional
table in which the original columns appear as rows.  In such cases we populate
an internal data structure by filling up one column at a time, then read off
the data one row at a time as we write data to the output file.</p>
<p>In the most vexing cases, the source and target formats have slightly different
coverage of the domain, and information is unavoidably lost when translating between them.
For example, we could combine multiple Toolbox files to create a single CSV file
containing a comparative wordlist, loosing all but the <tt class="doctest"><span class="pre">\lx</span></tt> field of the input files.
If the CSV file was later modified, it would be a labor-intensive process to inject
the changes into the original Toolbox files.  A partial solution to this &quot;round-tripping&quot;
problem is to associate explicit identifiers each linguistic object, and to propagate
the identifiers with the objects.</p>
</div>
<div class="section" id="deciding-which-layers-of-annotation-to-include">
<h2>3.5&nbsp;&nbsp;&nbsp;Deciding Which Layers of Annotation to Include</h2>
<p>Published corpora vary greatly in the richness of the information they contain.
At a minimum, a corpus will typically
contain at least a sequence of sound or orthographic symbols.  At the
other end of the spectrum, a corpus could contain a large amount of
information about the syntactic structure, morphology, prosody, and
semantic content of every sentence, plus annotation of discourse relations
or dialogue acts.  These extra layers of annotation
may be just what someone needs for performing a particular data analysis task.
For example, it may be much easier to find a given
linguistic pattern if we can search for specific syntactic structures;
and it may be easier to categorize a linguistic pattern if every word
has been tagged with its sense.  Here are some commonly provided
annotation layers:</p>
<ul class="simple">
<li>Word Tokenization: The orthographic form of text does not unambiguously
identify its tokens.  A tokenized and normalized version,
in addition to the conventional orthographic version,
may be a very convenient resource.</li>
<li>Sentence Segmentation: As we saw in <a class="reference external" href="ch03.html#chap-words">3</a>, sentence
segmentation can be more difficult than it seems.  Some corpora
therefore use explicit annotations to mark sentence segmentation.</li>
<li>Paragraph Segmentation: Paragraphs and other structural elements
(headings, chapters, etc.) may be explicitly annotated.</li>
<li>Part of Speech: The syntactic category of each word in a document.</li>
<li>Syntactic Structure: A tree structure showing the constituent
structure of a sentence.</li>
<li>Shallow Semantics: Named entity and coreference annotations, semantic role labels.</li>
<li>Dialogue and Discourse: dialogue act tags, rhetorical structure</li>
</ul>
<p>Unfortunately, there is not much consistency between existing corpora
in how they represent their annotations.  However, two general classes
of annotation representation should be distinguished.  <a name="inline_annotation_index_term" /><span class="termdef">Inline
annotation</span> modifies the original document by inserting special
symbols or control sequences that carry the annotated information.
For example, when part-of-speech tagging a document, the string
<tt class="doctest"><span class="pre"><span class="pysrc-string">&quot;fly&quot;</span></span></tt> might be replaced with the string <tt class="doctest"><span class="pre"><span class="pysrc-string">&quot;fly/NN&quot;</span></span></tt>, to indicate
that the word <em>fly</em> is a noun in this context.  In contrast, <a name="standoff_annotation_index_term_2" /><span class="termdef">standoff
annotation</span> does not modify the original document, but instead
creates a new file that adds annotation information using pointers
that reference the original document.  For example, this new document might
contain the string <tt class="doctest"><span class="pre"><span class="pysrc-string">&quot;&lt;token id=8 pos='NN'/&gt;&quot;</span></span></tt>, to indicate
that token 8 is a noun.  (We would want to be sure that the tokenization
itself was not subject to change, since it would cause such references
to break silently.)</p>
</div>
<div class="section" id="standards-and-tools">
<h2>3.6&nbsp;&nbsp;&nbsp;Standards and Tools</h2>
<p>For a corpus to be widely useful, it needs to be available in a widely
supported format.  However, the cutting edge of NLP research depends
on new kinds of annotations, which by definition are not widely supported.
In general, adequate tools for creation, publication and use of
linguistic data are not widely available.  Most projects must
develop their own set of tools for internal use, which is no help
to others who lack the necessary resources.
Furthermore, we do not have adequate, generally-accepted standards for
expressing the structure and content of corpora. Without
such standards, general-purpose tools are impossible &#8212; though at the
same time, without available tools, adequate standards are unlikely to
be developed, used and accepted.</p>
<p>One response to this situation has been to forge ahead with developing
a generic format which is sufficiently expressive to capture a wide variety of
annotation types (see <a class="reference internal" href="#sec-further-reading-data">8</a> for examples).
The challenge for NLP is to write programs that cope with the generality
of such formats.
For example, if the programming task involves tree data, and the
file format permits arbitrary directed graphs, then input data must
be validated to check for tree properties such as rootedness, connectedness,
and acyclicity.
If the input files contain other layers of annotation, the program
would need to know how to ignore them when the data was loaded,
but not invalidate or obliterate those layers when the tree data
was saved back to the file.</p>
<p>Another response has been to write one-off scripts
to manipulate corpus formats; such scripts litter the filespaces of many
NLP researchers.
NLTK's corpus readers are a more systematic
approach, founded on the premise that the work of parsing a corpus format
should only be done once (per programming language).</p>
<span class="target" id="fig-three-layer-arch"></span><div class="figure" id="fig-three-layer-arch">
<img alt="../images/three-layer-arch.png" src="../images/three-layer-arch.png" style="width: 538.8px; height: 241.5px;" />
<p class="caption"><span class="caption-label">Figure 3.2</span>: A Common Format vs A Common Interface</p>
</div>
<p>Instead of focussing on a common format, we believe it is more promising to
develop a common interface (cf. <tt class="doctest"><span class="pre">nltk.corpus</span></tt>).  Consider the
case of treebanks, an important corpus type for work in NLP.
There are many ways to store a phrase structure tree in a file.
We can use nested parentheses, or nested XML elements,
or a dependency notation with a (child-id, parent-id) pair on each line,
or an XML version of the dependency notation, etc.
However, in each case the logical structure is almost the same.
It is much easier to devise a common interface that allows
application programmers to write code to access tree data
using methods such as <tt class="doctest"><span class="pre">children()</span></tt>, <tt class="doctest"><span class="pre">leaves()</span></tt>, <tt class="doctest"><span class="pre">depth()</span></tt>, and
so forth.
Note that this approach follows accepted practice within
computer science, viz. abstract data types, object oriented design,
and the three layer architecture (<a class="reference internal" href="#fig-three-layer-arch">3.2</a>).
The last of these &#8212; from the world of relational databases &#8212;
allows end-user applications to use a common model (the &quot;relational model&quot;)
and a common language (SQL), to abstract away from the idiosyncrasies
of file storage, and allowing innovations in filesystem technologies
to occur without disturbing end-user applications.
In the same way, a common corpus interface
insulates application programs from data formats.</p>
<p>In this context, when creating a new corpus for dissemination, it is
expedient to use an existing widely-used format wherever possible.
When this is not possible, the corpus could be accompanied with software
&#8212; such as an <tt class="doctest"><span class="pre">nltk.corpus</span></tt> module &#8212; that supports
existing interface methods.</p>
</div>
<div class="section" id="special-considerations-when-working-with-endangered-languages">
<h2>3.7&nbsp;&nbsp;&nbsp;Special Considerations when Working with Endangered Languages</h2>
<p>The importance of language to science and the arts is matched in
significance by the cultural treasure embodied in language.
Each of the world's ~7,000 human languages is rich in unique respects,
in its oral histories and creation legends, down to its grammatical
constructions and its very words and their nuances of meaning.
Threatened remnant cultures have words to distinguish plant subspecies
according to therapeutic uses that are unknown to science.  Languages
evolve over time as they come into contact with each other, and each
one provides a unique window onto human pre-history.
In many parts of the world, small linguistic variations
from one town to the next add up to a completely different language in
the space of a half-hour drive.  For its breathtaking complexity and
diversity, human language is as a colorful tapestry stretching
through time and space.</p>
<p>However, most of the world's languages face extinction.
In response to this, many linguists are hard at work documenting the languages, constructing
rich records of this important facet of the world's linguistic heritage.
What can the field of NLP offer to help with this effort?  Developing
taggers, parsers, named-entity recognizers, etc,
is not an early priority, and there is usually insufficient data for
developing such tools in any case.  Instead, the most frequently voiced need is
to have better tools for collecting and curating data, with a focus
on texts and lexicons.</p>
<p>On the face of things, it should be a straightforward matter to start collecting texts
in an endangered language.  Even if we ignore vexed issues such as who
owns the texts, and sensitivities surrounding cultural knowledge contained in the texts,
there is the obvious practical issue of transcription.
Most languages lack a standard orthography.  When a language
has no literary tradition, the conventions of spelling and punctuation
are not well-established.  Therefore it is common practice
to create a lexicon in tandem with a text collection, continually updating
the lexicon as new words appear in the texts.  This work could be done
using a text processor (for the texts) and a spreadsheet (for the lexicon).
Better still, SIL's free linguistic software Toolbox and Fieldworks
provide sophisticated support for integrated creation of texts and lexicons.</p>
<p>When speakers of the language in question are trained to enter texts themselves,
a common obstacle is an overriding concern for correct spelling.  Having
a lexicon greatly helps this process, but we need to have lookup methods
that do not assume someone can determine the citation form of an arbitrary
word.  The problem may be acute for languages having a complex morphology that
includes prefixes.  In such cases it helps to tag lexical items with
semantic domains, and to permit lookup by semantic domain or by gloss.</p>
<p>Permitting lookup by pronunciation similarity is also a big help.
Here's a simple demonstration of how to do this.
The first step is to identify confusible letter sequences,
and map complex versions to simpler versions.  We might also notice
that the relative order of letters within a cluster of consonants
is a source of spelling errors, and so we normalize the
order of consonants.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>mappings = [(<span class="pysrc-string">'ph'</span>, <span class="pysrc-string">'f'</span>), (<span class="pysrc-string">'ght'</span>, <span class="pysrc-string">'t'</span>), (<span class="pysrc-string">'^kn'</span>, <span class="pysrc-string">'n'</span>), (<span class="pysrc-string">'qu'</span>, <span class="pysrc-string">'kw'</span>),
<span class="pysrc-more">... </span>            (<span class="pysrc-string">'[aeiou]+'</span>, <span class="pysrc-string">'a'</span>), (r<span class="pysrc-string">'(.)\1'</span>, r<span class="pysrc-string">'\1'</span>)]
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">def</span> <span class="pysrc-defname">signature</span>(word):
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">for</span> patt, repl <span class="pysrc-keyword">in</span> mappings:
<span class="pysrc-more">... </span>        word = re.sub(patt, repl, word)
<span class="pysrc-more">... </span>    pieces = re.findall(<span class="pysrc-string">'[^aeiou]+'</span>, word)
<span class="pysrc-more">... </span>    return <span class="pysrc-string">''</span>.join(char <span class="pysrc-keyword">for</span> piece <span class="pysrc-keyword">in</span> pieces <span class="pysrc-keyword">for</span> char <span class="pysrc-keyword">in</span> sorted(piece))[:8]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>signature(<span class="pysrc-string">'illefent'</span>)
<span class="pysrc-output">'lfnt'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>signature(<span class="pysrc-string">'ebsekwieous'</span>)
<span class="pysrc-output">'bskws'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>signature(<span class="pysrc-string">'nuculerr'</span>)
<span class="pysrc-output">'nclr'</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Next, we create a mapping from signatures to words, for all the words
in our lexicon.  We can use this to get candidate corrections for a
given input word (but we must first compute that word's signature).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>signatures = nltk.Index((signature(w), w) <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> nltk.corpus.words.words())
<span class="pysrc-prompt">&gt;&gt;&gt; </span>signatures[signature(<span class="pysrc-string">'nuculerr'</span>)]
<span class="pysrc-output">['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Finally, we should rank the results in terms of similarity with the original word.
This is done by the function <tt class="doctest"><span class="pre">rank()</span></tt>.  The only remaining function provides a
simple interface to the user:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">def</span> <span class="pysrc-defname">rank</span>(word, wordlist):
<span class="pysrc-more">... </span>    ranked = sorted((nltk.edit_distance(word, w), w) <span class="pysrc-keyword">for</span> w <span class="pysrc-keyword">in</span> wordlist)
<span class="pysrc-more">... </span>    return [word <span class="pysrc-keyword">for</span> (_, word) <span class="pysrc-keyword">in</span> ranked]
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">def</span> <span class="pysrc-defname">fuzzy_spell</span>(word):
<span class="pysrc-more">... </span>    sig = signature(word)
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">if</span> sig <span class="pysrc-keyword">in</span> signatures:
<span class="pysrc-more">... </span>        return rank(word, signatures[sig])
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">else</span>:
<span class="pysrc-more">... </span>        return []
<span class="pysrc-prompt">&gt;&gt;&gt; </span>fuzzy_spell(<span class="pysrc-string">'illefent'</span>)
<span class="pysrc-output">['olefiant', 'elephant', 'oliphant', 'elephanta']</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fuzzy_spell(<span class="pysrc-string">'ebsekwieous'</span>)
<span class="pysrc-output">['obsequious']</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>fuzzy_spell(<span class="pysrc-string">'nucular'</span>)
<span class="pysrc-output">['anicular', 'inocular', 'nucellar', 'nuclear', 'unocular', 'uniocular', 'unicolor']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>This is just one illustration where a simple program can facilitate access to lexical
data in a context where the writing system of a language may not be standardized, or
where users of the language may not have a good command of spellings.  Other simple
applications of NLP in this area include: building indexes to facilitate access to
data, gleaning wordlists from texts,
locating examples of word usage in constructing a lexicon, detecting prevalent or
exceptional patterns in poorly understood data, and performing specialized validation
on data created using various linguistic software tools.  We will return to the last
of these in <a class="reference internal" href="#sec-working-with-toolbox-data">5</a>.</p>
</div>
</div>
<div class="section" id="working-with-xml">
<span id="sec-working-with-xml"></span><h1>4&nbsp;&nbsp;&nbsp;Working with XML</h1>
<p>The Extensible Markup Language (XML) provides a framework for designing
domain-specific markup languages.  It is sometimes used for representing
annotated text and for lexical resources.  Unlike HTML with its predefined
tags, XML permits us to make up our own tags.  Unlike a database, XML
permits us to create data without first specifying its structure, and it
permits us to have optional and repeatable elements.  In this section
we briefly review some features of XML that are relevant for representing
linguistic data, and show how to access data stored in XML files using
Python programs.</p>
<div class="section" id="using-xml-for-linguistic-structures">
<h2>4.1&nbsp;&nbsp;&nbsp;Using XML for Linguistic Structures</h2>
<p>Thanks to its flexibility and extensibility, XML is a natural
choice for representing linguistic structures.  Here's an example
of a simple lexical entry.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;gloss&gt;any of the larger cetacean mammals having a streamlined
    body and breathing through a blowhole on the head&lt;/gloss&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>It consists of a series of XML tags enclosed in angle brackets.
Each opening tag, like <tt class="doctest"><span class="pre">&lt;gloss&gt;</span></tt> is matched with a closing tag, like <tt class="doctest"><span class="pre">&lt;/gloss&gt;</span></tt>;
together they constitute an <a name="xml_element_index_term" /><span class="termdef">XML element</span>.
The above example has been laid out nicely using whitespace, but it could
equally have been put on a single, long line.  Our approach to processing
XML will usually not be sensitive to whitespace.  In order for XML to be
<a name="well_formed_index_term" /><span class="termdef">well formed</span>, all opening tags must have corresponding closing tags, at the
same level of nesting (i.e. the XML document must be a well-formed tree).</p>
<p>XML permits us to repeat elements, e.g. to add another gloss field as we see below.
We will use different whitespace to underscore the point that layout
does not matter.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;&lt;headword&gt;whale&lt;/headword&gt;&lt;pos&gt;noun&lt;/pos&gt;&lt;gloss&gt;any of the
larger cetacean mammals having a streamlined body and breathing
through a blowhole on the head&lt;/gloss&gt;&lt;gloss&gt;a very large person;
impressive in size or qualities&lt;/gloss&gt;&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>A further step might be to link our lexicon to some external resource, such as WordNet,
using external identifiers.  In <a class="reference internal" href="#ex-xml-nested">(4)</a> we group the gloss and a synset identifier
inside a new element which we have called &quot;sense&quot;.</p>
<span class="target" id="ex-xml-nested"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;sense&gt;
    &lt;gloss&gt;any of the larger cetacean mammals having a streamlined
      body and breathing through a blowhole on the head&lt;/gloss&gt;
    &lt;synset&gt;whale.n.02&lt;/synset&gt;
  &lt;/sense&gt;
  &lt;sense&gt;
    &lt;gloss&gt;a very large person; impressive in size or qualities&lt;/gloss&gt;
    &lt;synset&gt;giant.n.04&lt;/synset&gt;
  &lt;/sense&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>Alternatively, we could have represented the synset identifier using an <a name="xml_attribute_index_term" /><span class="termdef">XML attribute</span>,
without the need for any nested structure, as in <a class="reference internal" href="#ex-xml-attribute">(5)</a>.</p>
<span class="target" id="ex-xml-attribute"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;gloss synset=&quot;whale.n.02&quot;&gt;any of the larger cetacean mammals having
      a streamlined body and breathing through a blowhole on the head&lt;/gloss&gt;
  &lt;gloss synset=&quot;giant.n.04&quot;&gt;a very large person; impressive in size or
      qualities&lt;/gloss&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>This illustrates some of the flexibility of XML.  If it seems somewhat arbitrary
that's because it is!  Following the rules of XML we can invent new attribute
names, and nest them as deeply as we like.  We can repeat elements, leave them
out, and put them in a different order each time.  We can have fields whose
presence depends on the value of some other field, e.g. if the part of speech
is &quot;verb&quot;, then the entry can have a <tt class="doctest"><span class="pre">past_tense</span></tt> element to hold the
past tense of the verb, but if the part of speech is &quot;noun&quot; no <tt class="doctest"><span class="pre">past_tense</span></tt>
element is permitted.  To impose some order over all
this freedom, we can constrain the structure of an XML file using a &quot;schema,&quot;
which is a declaration akin to a context free grammar.  Tools exist for
testing the <a name="validity_index_term" /><span class="termdef">validity</span> of an XML file with respect to a schema.</p>
</div>
<div class="section" id="the-role-of-xml">
<h2>4.2&nbsp;&nbsp;&nbsp;The Role of XML</h2>
<p>We can use XML to represent many kinds of linguistic information.
However, the flexibility comes at a price.  Each time we introduce a complication, such as by permitting
an element to be optional or repeated, we make more work for any program
that accesses the data.  We also make it more difficult to check the validity of the
data, or to interrogate the data using one of the XML query languages.</p>
<p>Thus, using XML to represent linguistic structures does not magically solve the data
modeling problem.  We still have to work out how to structure the data,
then define that structure with a schema, and then
write programs to read and write the format and convert it to other formats.
Similarly, we still need to follow some standard principles concerning
data normalization.  It is wise to avoid making duplicate copies of the
same information, so that we don't end up with inconsistent data when
only one copy is changed.  For example, a cross-reference that was
represented as <tt class="doctest"><span class="pre">&lt;xref&gt;headword&lt;/xref&gt;</span></tt> would duplicate the storage
of the headword of some other lexical entry, and the link would break
if the copy of the string at the other location was modified.
Existential dependencies between information types need to be
modeled, so that we can't create elements without a home.
For example, if sense definitions cannot exist independently
of a lexical entry, the <tt class="doctest"><span class="pre">sense</span></tt> element can be nested inside the <tt class="doctest"><span class="pre">entry</span></tt>
element.  Many-to-many relations need to be abstracted out of
hierarchical structures.  For example, if a word can have many corresponding
senses, and a sense can have several corresponding words, then
both words and senses must be enumerated separately, as must the
list of (word, sense) pairings.  This complex structure might even be split
across three separate XML files.</p>
<p>As we can see, although XML provides us with a convenient format
accompanied by an extensive collection of tools, it offers no panacea.</p>
</div>
<div class="section" id="the-elementtree-interface">
<h2>4.3&nbsp;&nbsp;&nbsp;The ElementTree Interface</h2>
<p>Python's ElementTree module provides a convenient way to access
data stored in XML files.  ElementTree is part of Python's
standard library (since Python 2.5), and is also provided as
part of NLTK in case you are using Python 2.4.</p>
<p>We will illustrate the use of ElementTree using a collection
of Shakespeare plays that have been formatted using XML.
Let's load the XML file and inspect the raw data, first
at the top of the file <a class="reference internal" href="#top-of-file"><span id="ref-top-of-file"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>, where we see some
XML headers and the name of a schema called <tt class="doctest"><span class="pre">play.dtd</span></tt>,
followed by the <a name="root_element_index_term" /><span class="termdef">root element</span> <tt class="doctest"><span class="pre">PLAY</span></tt>.
We pick it up again at the start of Act 1 <a class="reference internal" href="#start-act-one"><span id="ref-start-act-one"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.
(Some blank lines have been omitted from the output.)</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant_file = nltk.data.find(<span class="pysrc-string">'corpora/shakespeare/merchant.xml'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>raw = open(merchant_file).read()
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(raw[:163]) <a name="top-of-file" /><a href="#ref-top-of-file"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-output">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
<span class="pysrc-output">&lt;?xml-stylesheet type=&quot;text/css&quot; href=&quot;shakes.css&quot;?&gt;</span>
<span class="pysrc-output">&lt;!-- &lt;!DOCTYPE PLAY SYSTEM &quot;play.dtd&quot;&gt; --&gt;</span>
<span class="pysrc-output">&lt;PLAY&gt;</span>
<span class="pysrc-output">&lt;TITLE&gt;The Merchant of Venice&lt;/TITLE&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(raw[1789:2006]) <a name="start-act-one" /><a href="#ref-start-act-one"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a>
<span class="pysrc-output">&lt;TITLE&gt;ACT I&lt;/TITLE&gt;</span>
<span class="pysrc-output">&lt;SCENE&gt;&lt;TITLE&gt;SCENE I.  Venice. A street.&lt;/TITLE&gt;</span>
<span class="pysrc-output">&lt;STAGEDIR&gt;Enter ANTONIO, SALARINO, and SALANIO&lt;/STAGEDIR&gt;</span>
<span class="pysrc-output">&lt;SPEECH&gt;</span>
<span class="pysrc-output">&lt;SPEAKER&gt;ANTONIO&lt;/SPEAKER&gt;</span>
<span class="pysrc-output">&lt;LINE&gt;In sooth, I know not why I am so sad:&lt;/LINE&gt;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>We have just accessed the XML data as a string.  As we can see,
the string at the start of Act 1 contains XML tags for title, scene, stage directions, and so forth.</p>
<p>The next step is to process the file contents as structured XML data,
using <tt class="doctest"><span class="pre">ElementTree</span></tt>.  We are processing a file (a multi-line string)
and building a tree, so its not surprising that the method name is <tt class="doctest"><span class="pre">parse</span></tt> <a class="reference internal" href="#xml-parse"><span id="ref-xml-parse"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>.
The variable <tt class="doctest"><span class="pre">merchant</span></tt> contains an XML element <tt class="doctest"><span class="pre">PLAY</span></tt> <a class="reference internal" href="#element-play"><span id="ref-element-play"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.
This element has internal structure; we can use an index
to get its first child, a <tt class="doctest"><span class="pre">TITLE</span></tt> element <a class="reference internal" href="#element-title"><span id="ref-element-title"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.
We can also see the text content of this element, the title of the play <a class="reference internal" href="#element-text"><span id="ref-element-text"><img src="callouts/callout4.gif" alt="[4]" class="callout" /></span></a>.
To get a list of all the child elements, we use the
<tt class="doctest"><span class="pre">getchildren()</span></tt> method <a class="reference internal" href="#getchildren-method"><span id="ref-getchildren-method"><img src="callouts/callout5.gif" alt="[5]" class="callout" /></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> xml.etree.ElementTree <span class="pysrc-keyword">import</span> ElementTree
<span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant = ElementTree().parse(merchant_file) <a name="xml-parse" /><a href="#ref-xml-parse"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant
<span class="pysrc-output">&lt;Element 'PLAY' at 0x10ac43d18&gt; # [_element-play]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[0]
<span class="pysrc-output">&lt;Element 'TITLE' at 0x10ac43c28&gt; # [_element-title]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[0].text
<span class="pysrc-output">'The Merchant of Venice' # [_element-text]</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant.getchildren() <a name="getchildren-method" /><a href="#ref-getchildren-method"><img src="callouts/callout5.gif" alt="[5]" class="callout" /></a>
<span class="pysrc-output">[&lt;Element 'TITLE' at 0x10ac43c28&gt;, &lt;Element 'PERSONAE' at 0x10ac43bd8&gt;,</span>
<span class="pysrc-output">&lt;Element 'SCNDESCR' at 0x10b067f98&gt;, &lt;Element 'PLAYSUBT' at 0x10af37048&gt;,</span>
<span class="pysrc-output">&lt;Element 'ACT' at 0x10af37098&gt;, &lt;Element 'ACT' at 0x10b936368&gt;,</span>
<span class="pysrc-output">&lt;Element 'ACT' at 0x10b934b88&gt;, &lt;Element 'ACT' at 0x10cfd8188&gt;,</span>
<span class="pysrc-output">&lt;Element 'ACT' at 0x10cfadb38&gt;]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The play consists of a title, the personae, a scene description, a subtitle, and five acts.
Each act has a title and some scenes, and each scene consists of speeches which are made
up of lines, a structure with four levels of nesting.  Let's dig down into Act IV:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][0].text
<span class="pysrc-output">'ACT IV'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1]
<span class="pysrc-output">&lt;Element 'SCENE' at 0x10cfd8228&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][0].text
<span class="pysrc-output">'SCENE I.  Venice. A court of justice.'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][54]
<span class="pysrc-output">&lt;Element 'SPEECH' at 0x10cfb02c8&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][54][0]
<span class="pysrc-output">&lt;Element 'SPEAKER' at 0x10cfb0318&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][54][0].text
<span class="pysrc-output">'PORTIA'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][54][1]
<span class="pysrc-output">&lt;Element 'LINE' at 0x10cfb0368&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>merchant[-2][1][54][1].text
<span class="pysrc-output">&quot;The quality of mercy is not strain'd,&quot;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Your Turn:</strong>
Repeat some of the above methods, for one of the other Shakespeare plays
included in the corpus, such as <span class="emphasis">Romeo and Juliet</span> or <span class="emphasis">Macbeth</span>;
for a list, see <tt class="doctest"><span class="pre">nltk.corpus.shakespeare.fileids()</span></tt>.</p>
</div>
<p>Although we can access the entire tree this way, it is more convenient to search for
sub-elements with particular names.  Recall that the elements at the top level have
several types.  We can iterate over just the types we are interested in (such as
the acts), using <tt class="doctest"><span class="pre">merchant.findall(<span class="pysrc-string">'ACT'</span>)</span></tt>.  Here's an example of doing such
tag-specific searches at every level of nesting:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> i, act <span class="pysrc-keyword">in</span> enumerate(merchant.findall(<span class="pysrc-string">'ACT'</span>)):
<span class="pysrc-more">... </span>    <span class="pysrc-keyword">for</span> j, scene <span class="pysrc-keyword">in</span> enumerate(act.findall(<span class="pysrc-string">'SCENE'</span>)):
<span class="pysrc-more">... </span>        <span class="pysrc-keyword">for</span> k, speech <span class="pysrc-keyword">in</span> enumerate(scene.findall(<span class="pysrc-string">'SPEECH'</span>)):
<span class="pysrc-more">... </span>            <span class="pysrc-keyword">for</span> line <span class="pysrc-keyword">in</span> speech.findall(<span class="pysrc-string">'LINE'</span>):
<span class="pysrc-more">... </span>                <span class="pysrc-keyword">if</span> <span class="pysrc-string">'music'</span> <span class="pysrc-keyword">in</span> str(line.text):
<span class="pysrc-more">... </span>                    <span class="pysrc-keyword">print</span>(<span class="pysrc-string">&quot;Act %d Scene %d Speech %d: %s&quot;</span> % (i+1, j+1, k+1, line.text))
<span class="pysrc-output">Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;</span>
<span class="pysrc-output">Act 3 Scene 2 Speech 9: Fading in music: that the comparison</span>
<span class="pysrc-output">Act 3 Scene 2 Speech 9: And what is music then? Then music is</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 23: And bring your music forth into the air.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 23: And draw her home with music.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 25: Or any air of music touch their ears,</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 25: But music for the time doth change his nature.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 25: The man that hath no music in himself,</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 29: It is your music, madam, of the house.</span>
<span class="pysrc-output">Act 5 Scene 1 Speech 32: No better a musician than the wren.</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Instead of navigating each step of the way down the hierarchy, we can search for
particular embedded elements.  For example, let's examine the sequence of speakers.
We can use a frequency distribution to see who has the most to say:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> collections <span class="pysrc-keyword">import</span> Counter
<span class="pysrc-prompt">&gt;&gt;&gt; </span>speaker_seq = [s.text <span class="pysrc-keyword">for</span> s <span class="pysrc-keyword">in</span> merchant.findall(<span class="pysrc-string">'ACT/SCENE/SPEECH/SPEAKER'</span>)]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>speaker_freq = Counter(speaker_seq)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>top5 = speaker_freq.most_common(5)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>top5
<span class="pysrc-output">[('PORTIA', 117), ('SHYLOCK', 79), ('BASSANIO', 73),</span>
<span class="pysrc-output">('GRATIANO', 48), ('LORENZO', 47)]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>We can also look for patterns in who follows who in the dialogues.
Since there's 23 speakers, we need to reduce the &quot;vocabulary&quot;
to a manageable size first, using the method described in
<a class="reference external" href="ch05.html#sec-dictionaries">3</a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> collections <span class="pysrc-keyword">import</span> defaultdict
<span class="pysrc-prompt">&gt;&gt;&gt; </span>abbreviate = defaultdict(<span class="pysrc-keyword">lambda</span>: <span class="pysrc-string">'OTH'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> speaker, _ <span class="pysrc-keyword">in</span> top5:
<span class="pysrc-more">... </span>    abbreviate[speaker] = speaker[:4]
<span class="pysrc-more">...</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>speaker_seq2 = [abbreviate[speaker] <span class="pysrc-keyword">for</span> speaker <span class="pysrc-keyword">in</span> speaker_seq]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))
<span class="pysrc-prompt">&gt;&gt;&gt; </span>cfd.tabulate()
<span class="pysrc-output">     ANTO BASS GRAT  OTH PORT SHYL</span>
<span class="pysrc-output">ANTO    0   11    4   11    9   12</span>
<span class="pysrc-output">BASS   10    0   11   10   26   16</span>
<span class="pysrc-output">GRAT    6    8    0   19    9    5</span>
<span class="pysrc-output"> OTH    8   16   18  153   52   25</span>
<span class="pysrc-output">PORT    7   23   13   53    0   21</span>
<span class="pysrc-output">SHYL   15   15    2   26   21    0</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Ignoring the entries for exchanges between people
other than the top 5 (labeled <tt class="doctest"><span class="pre">OTH</span></tt>), the largest value suggests
that Portia and Bassanio have the most frequent interactions.</p>
</div>
<div class="section" id="using-elementtree-for-accessing-toolbox-data">
<h2>4.4&nbsp;&nbsp;&nbsp;Using ElementTree for Accessing Toolbox Data</h2>
<p>In <a class="reference external" href="ch02.html#sec-lexical-resources">4</a> we saw a simple interface
for accessing Toolbox data, a popular and well-established format
used by linguists for managing data.
In this section we discuss a variety of
techniques for manipulating Toolbox data in ways that are not supported
by the Toolbox software.  The methods we discuss could be
applied to other record-structured data, regardless of the actual file format.</p>
<p>We can use the <tt class="doctest"><span class="pre">toolbox.xml()</span></tt> method to access a Toolbox
file and load it into an <tt class="doctest"><span class="pre">elementtree</span></tt> object.  This file
contains a lexicon for the Rotokas language of Papua New Guinea.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk.corpus <span class="pysrc-keyword">import</span> toolbox
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = toolbox.xml(<span class="pysrc-string">'rotokas.dic'</span>)</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>There are two ways to access the contents of the lexicon object, by
indexes and by paths.  Indexes use the familiar syntax, thus
<tt class="doctest"><span class="pre">lexicon[3]</span></tt> returns entry number 3 (which is actually the fourth
entry counting from zero); <tt class="doctest"><span class="pre">lexicon[3][0]</span></tt> returns its first field:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon[3][0]
<span class="pysrc-output">&lt;Element 'lx' at 0x10b2f6958&gt;</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon[3][0].tag
<span class="pysrc-output">'lx'</span>
<span class="pysrc-output"></span><span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon[3][0].text
<span class="pysrc-output">'kaa'</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>The second way to access the contents of the lexicon object uses
paths.  The lexicon is a series of <tt class="doctest"><span class="pre">record</span></tt> objects, each containing
a series of field objects, such as <tt class="doctest"><span class="pre">lx</span></tt> and <tt class="doctest"><span class="pre">ps</span></tt>.  We can
conveniently address all of the lexemes using the path <tt class="doctest"><span class="pre">record/lx</span></tt>.
Here we use the <tt class="doctest"><span class="pre">findall()</span></tt> function to search for any matches to
the path <tt class="doctest"><span class="pre">record/lx</span></tt>, and we access the text content of the element,
normalizing it to lowercase.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>[lexeme.text.lower() <span class="pysrc-keyword">for</span> lexeme <span class="pysrc-keyword">in</span> lexicon.findall(<span class="pysrc-string">'record/lx'</span>)]
<span class="pysrc-output">['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',</span>
<span class="pysrc-output">'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Let's view the Toolbox data in XML format.  The <tt class="doctest"><span class="pre">write()</span></tt> method of
<tt class="doctest"><span class="pre">ElementTree</span></tt> expects a file object.  We usually create one of these
using Python's built-in <tt class="doctest"><span class="pre">open()</span></tt> function.  In order to see the output
displayed on the screen, we can use a special pre-defined file object
called <tt class="doctest"><span class="pre">stdout</span></tt> <a class="reference internal" href="#sys-stdout"><span id="ref-sys-stdout"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a> (standard output), defined in Python's <tt class="doctest"><span class="pre">sys</span></tt> module.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">import</span> sys
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk.util <span class="pysrc-keyword">import</span> elementtree_indent
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> xml.etree.ElementTree <span class="pysrc-keyword">import</span> ElementTree
<span class="pysrc-prompt">&gt;&gt;&gt; </span>elementtree_indent(lexicon)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>tree = ElementTree(lexicon[3])
<span class="pysrc-prompt">&gt;&gt;&gt; </span>tree.write(sys.stdout, encoding=<span class="pysrc-string">'unicode'</span>) <a name="sys-stdout" /><a href="#ref-sys-stdout"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
<span class="pysrc-output">&lt;record&gt;</span>
<span class="pysrc-output">  &lt;lx&gt;kaa&lt;/lx&gt;</span>
<span class="pysrc-output">  &lt;ps&gt;N&lt;/ps&gt;</span>
<span class="pysrc-output">  &lt;pt&gt;MASC&lt;/pt&gt;</span>
<span class="pysrc-output">  &lt;cl&gt;isi&lt;/cl&gt;</span>
<span class="pysrc-output">  &lt;ge&gt;cooking banana&lt;/ge&gt;</span>
<span class="pysrc-output">  &lt;tkp&gt;banana bilong kukim&lt;/tkp&gt;</span>
<span class="pysrc-output">  &lt;pt&gt;itoo&lt;/pt&gt;</span>
<span class="pysrc-output">  &lt;sf&gt;FLORA&lt;/sf&gt;</span>
<span class="pysrc-output">  &lt;dt&gt;12/Aug/2005&lt;/dt&gt;</span>
<span class="pysrc-output">  &lt;ex&gt;Taeavi iria kaa isi kovopaueva kaparapasia.&lt;/ex&gt;</span>
<span class="pysrc-output">  &lt;xp&gt;Taeavi i bin planim gaden banana bilong kukim tasol long paia.&lt;/xp&gt;</span>
<span class="pysrc-output">  &lt;xe&gt;Taeavi planted banana in order to cook it.&lt;/xe&gt;</span>
<span class="pysrc-output">&lt;/record&gt;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="formatting-entries">
<h2>4.5&nbsp;&nbsp;&nbsp;Formatting Entries</h2>
<p>We can use the same idea we saw above to generate HTML tables instead of plain text.
This would be useful for publishing a Toolbox lexicon on the web.
It produces HTML elements <tt class="doctest"><span class="pre">&lt;table&gt;</span></tt>, <tt class="doctest"><span class="pre">&lt;tr&gt;</span></tt> (table row), and
<tt class="doctest"><span class="pre">&lt;td&gt;</span></tt> (table data).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>html = <span class="pysrc-string">&quot;&lt;table&gt;\n&quot;</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">for</span> entry <span class="pysrc-keyword">in</span> lexicon[70:80]:
<span class="pysrc-more">... </span>    lx = entry.findtext(<span class="pysrc-string">'lx'</span>)
<span class="pysrc-more">... </span>    ps = entry.findtext(<span class="pysrc-string">'ps'</span>)
<span class="pysrc-more">... </span>    ge = entry.findtext(<span class="pysrc-string">'ge'</span>)
<span class="pysrc-more">... </span>    html += <span class="pysrc-string">&quot;  &lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\n&quot;</span> % (lx, ps, ge)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>html += <span class="pysrc-string">&quot;&lt;/table&gt;&quot;</span>
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(html)
<span class="pysrc-output">&lt;table&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;CLASS&lt;/td&gt;&lt;td&gt;child&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakaevira&lt;/td&gt;&lt;td&gt;ADV&lt;/td&gt;&lt;td&gt;small-like&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakapikoa&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakapikoto&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;newborn baby&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakapu&lt;/td&gt;&lt;td&gt;V&lt;/td&gt;&lt;td&gt;place in sling for purpose of carrying&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakapua&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;sling for lifting&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakara&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;arm band&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;Kakarapaia&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;village name&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">  &lt;tr&gt;&lt;td&gt;kakarau&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;frog&lt;/td&gt;&lt;/tr&gt;</span>
<span class="pysrc-output">&lt;/table&gt;</span></pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
</div>
<div class="section" id="working-with-toolbox-data">
<span id="sec-working-with-toolbox-data"></span><h1>5&nbsp;&nbsp;&nbsp;Working with Toolbox Data</h1>
<p>Given the popularity of Toolbox amongst linguists, we will discuss some further
methods for working with Toolbox data.  Many of the methods discussed in previous
chapters, such as counting, building frequency distributions, tabulating co-occurrences,
can be applied to the content of Toolbox entries.  For example, we can trivially
compute the average number of fields for each entry:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk.corpus <span class="pysrc-keyword">import</span> toolbox
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = toolbox.xml(<span class="pysrc-string">'rotokas.dic'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>sum(len(entry) <span class="pysrc-keyword">for</span> entry <span class="pysrc-keyword">in</span> lexicon) / len(lexicon)
<span class="pysrc-output">13.635...</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>In this section we will discuss two tasks that arise in the context of documentary
linguistics, neither of which is supported by the Toolbox software.</p>
<div class="section" id="adding-a-field-to-each-entry">
<h2>5.1&nbsp;&nbsp;&nbsp;Adding a Field to Each Entry</h2>
<p>It is often convenient to add new fields that are derived automatically from
existing ones.  Such fields often facilitate search and analysis.
For instance, in <a class="reference internal" href="#code-add-cv-field">5.1</a> we define a function <tt class="doctest"><span class="pre">cv()</span></tt> which
maps a string of consonants and vowels to the corresponding CV sequence,
e.g. <tt class="doctest"><span class="pre">kakapua</span></tt> would map to <tt class="doctest"><span class="pre">CVCVCVV</span></tt>.
This mapping has four steps.  First, the string is converted to lowercase,
then we replace any non-alphabetic characters <tt class="doctest"><span class="pre">[^a-z]</span></tt> with an underscore.
Next, we replace all vowels with <tt class="doctest"><span class="pre">V</span></tt>.  Finally, anything that is not
a <tt class="doctest"><span class="pre">V</span></tt> or an underscore must be a consonant, so we replace it with a <tt class="doctest"><span class="pre">C</span></tt>.
Now, we can scan the lexicon and add a new <tt class="doctest"><span class="pre">cv</span></tt> field after every <tt class="doctest"><span class="pre">lx</span></tt> field.
<a class="reference internal" href="#code-add-cv-field">5.1</a> shows what this does to a particular entry; note
the last line of output, which shows the new <tt class="doctest"><span class="pre">cv</span></tt> field.</p>
<span class="target" id="code-add-cv-field"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-keyword">from</span> xml.etree.ElementTree <span class="pysrc-keyword">import</span> SubElement

<span class="pysrc-keyword">def</span> <span class="pysrc-defname">cv</span>(s):
    s = s.lower()
    s = re.sub(r<span class="pysrc-string">'[^a-z]'</span>,     r<span class="pysrc-string">'_'</span>, s)
    s = re.sub(r<span class="pysrc-string">'[aeiou]'</span>,    r<span class="pysrc-string">'V'</span>, s)
    s = re.sub(r<span class="pysrc-string">'[^V_]'</span>,      r<span class="pysrc-string">'C'</span>, s)
    return (s)

<span class="pysrc-keyword">def</span> <span class="pysrc-defname">add_cv_field</span>(entry):
    <span class="pysrc-keyword">for</span> field <span class="pysrc-keyword">in</span> entry:
        <span class="pysrc-keyword">if</span> field.tag == <span class="pysrc-string">'lx'</span>:
            cv_field = SubElement(entry, <span class="pysrc-string">'cv'</span>)
            cv_field.text = cv(field.text)</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = toolbox.xml(<span class="pysrc-string">'rotokas.dic'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>add_cv_field(lexicon[53])
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">print</span>(nltk.toolbox.to_sfm_string(lexicon[53]))
<span class="pysrc-output">\lx kaeviro</span>
<span class="pysrc-output">\ps V</span>
<span class="pysrc-output">\pt A</span>
<span class="pysrc-output">\ge lift off</span>
<span class="pysrc-output">\ge take off</span>
<span class="pysrc-output">\tkp go antap</span>
<span class="pysrc-output">\sc MOTION</span>
<span class="pysrc-output">\vx 1</span>
<span class="pysrc-output">\nt used to describe action of plane</span>
<span class="pysrc-output">\dt 03/Jun/2005</span>
<span class="pysrc-output">\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.</span>
<span class="pysrc-output">\xp Pita i go antap na lukim haus win i bagarapim.</span>
<span class="pysrc-output">\xe Peter went to look at the house that the wind destroyed.</span>
<span class="pysrc-output">\cv CVVCVCV</span></pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_add_cv_field.py" type="text/x-python"><span class="caption-label">Example 5.1 (code_add_cv_field.py)</span></a>: <span class="caption-label">Figure 5.1</span>: Adding a new <tt class="doctest"><span class="pre">cv</span></tt> field to a lexical entry</td></tr></p>
</table></div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If a Toolbox file is being continually updated, the program in
code-add-cv-field will need to be run more than once.  It would
be possible to modify <tt class="doctest"><span class="pre">add_cv_field()</span></tt> to modify the contents
of an existing entry.  However, it is a safer practice to use such
programs to create enriched files for the purpose of data analysis,
without replacing the manually curated source files.</p>
</div>
</div>
<div class="section" id="validating-a-toolbox-lexicon">
<h2>5.2&nbsp;&nbsp;&nbsp;Validating a Toolbox Lexicon</h2>
<p>Many lexicons in Toolbox format do not conform to any particular schema.
Some entries may include extra fields, or may order existing fields
in a new way.
Manually inspecting thousands of lexical entries is not practicable.
However, we can easily identify frequent field sequences,
with the help of a <tt class="doctest"><span class="pre">Counter</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> collections <span class="pysrc-keyword">import</span> Counter
<span class="pysrc-prompt">&gt;&gt;&gt; </span>field_sequences = Counter(<span class="pysrc-string">':'</span>.join(field.tag <span class="pysrc-keyword">for</span> field <span class="pysrc-keyword">in</span> entry) <span class="pysrc-keyword">for</span> entry <span class="pysrc-keyword">in</span> lexicon)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>field_sequences.most_common()
<span class="pysrc-output">[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),</span>
<span class="pysrc-output">('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27), ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20), ...]</span></pre>
</td>
</tr></table></td></tr>
</table></div>
<p>After inspecting these field sequences we could devise a context
free grammar for lexical entries.  The grammar in <a class="reference internal" href="#code-toolbox-validation">5.2</a>
uses the CFG format we saw in <a class="reference external" href="ch08.html#chap-parse">8.</a>.  Such a grammar models the implicit
nested structure of Toolbox entries, and builds a tree structure in which the
leaves of the tree are individual field names.  Finally, we iterate
over the entries and report their conformance with the grammar, as
shown in <a class="reference internal" href="#code-toolbox-validation">5.2</a>.
Those that are accepted by the grammar are prefixed with a <tt class="doctest"><span class="pre"><span class="pysrc-string">'+'</span></span></tt> <a class="reference internal" href="#accepted-entries"><span id="ref-accepted-entries"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></span></a>,
and those that are rejected are prefixed with a <tt class="doctest"><span class="pre"><span class="pysrc-string">'-'</span></span></tt> <a class="reference internal" href="#rejected-entries"><span id="ref-rejected-entries"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></span></a>.
During the process of developing such a grammar
it helps to filter out some of the tags <a class="reference internal" href="#ignored-tags"><span id="ref-ignored-tags"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></span></a>.</p>
<span class="target" id="code-toolbox-validation"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
grammar = nltk.CFG.fromstring(<span class="pysrc-string">'''</span>
<span class="pysrc-string">  S -&gt; Head PS Glosses Comment Date Sem_Field Examples</span>
<span class="pysrc-string">  Head -&gt; Lexeme Root</span>
<span class="pysrc-string">  Lexeme -&gt; &quot;lx&quot;</span>
<span class="pysrc-string">  Root -&gt; &quot;rt&quot; |</span>
<span class="pysrc-string">  PS -&gt; &quot;ps&quot;</span>
<span class="pysrc-string">  Glosses -&gt; Gloss Glosses |</span>
<span class="pysrc-string">  Gloss -&gt; &quot;ge&quot; | &quot;tkp&quot; | &quot;eng&quot;</span>
<span class="pysrc-string">  Date -&gt; &quot;dt&quot;</span>
<span class="pysrc-string">  Sem_Field -&gt; &quot;sf&quot;</span>
<span class="pysrc-string">  Examples -&gt; Example Ex_Pidgin Ex_English Examples |</span>
<span class="pysrc-string">  Example -&gt; &quot;ex&quot;</span>
<span class="pysrc-string">  Ex_Pidgin -&gt; &quot;xp&quot;</span>
<span class="pysrc-string">  Ex_English -&gt; &quot;xe&quot;</span>
<span class="pysrc-string">  Comment -&gt; &quot;cmt&quot; | &quot;nt&quot; |</span>
<span class="pysrc-string">  '''</span>)

<span class="pysrc-keyword">def</span> <span class="pysrc-defname">validate_lexicon</span>(grammar, lexicon, ignored_tags):
    rd_parser = nltk.RecursiveDescentParser(grammar)
    <span class="pysrc-keyword">for</span> entry <span class="pysrc-keyword">in</span> lexicon:
        marker_list = [field.tag <span class="pysrc-keyword">for</span> field <span class="pysrc-keyword">in</span> entry <span class="pysrc-keyword">if</span> field.tag <span class="pysrc-keyword">not</span> <span class="pysrc-keyword">in</span> ignored_tags]
        <span class="pysrc-keyword">if</span> list(rd_parser.parse(marker_list)):
            <span class="pysrc-keyword">print</span>(<span class="pysrc-string">&quot;+&quot;</span>, <span class="pysrc-string">':'</span>.join(marker_list)) <a name="accepted-entries" /><a href="#ref-accepted-entries"><img src="callouts/callout1.gif" alt="[1]" class="callout" /></a>
        <span class="pysrc-keyword">else</span>:
            <span class="pysrc-keyword">print</span>(<span class="pysrc-string">&quot;-&quot;</span>, <span class="pysrc-string">':'</span>.join(marker_list)) <a name="rejected-entries" /><a href="#ref-rejected-entries"><img src="callouts/callout2.gif" alt="[2]" class="callout" /></a></pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = toolbox.xml(<span class="pysrc-string">'rotokas.dic'</span>)[10:20]
<span class="pysrc-prompt">&gt;&gt;&gt; </span>ignored_tags = [<span class="pysrc-string">'arg'</span>, <span class="pysrc-string">'dcsv'</span>, <span class="pysrc-string">'pt'</span>, <span class="pysrc-string">'vx'</span>] <a name="ignored-tags" /><a href="#ref-ignored-tags"><img src="callouts/callout3.gif" alt="[3]" class="callout" /></a>
<span class="pysrc-prompt">&gt;&gt;&gt; </span>validate_lexicon(grammar, lexicon, ignored_tags)
<span class="pysrc-output">- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe</span>
<span class="pysrc-output">- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe</span>
<span class="pysrc-output">- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe</span>
<span class="pysrc-output">- lx:ps:ge:tkp:nt:sf:dt</span>
<span class="pysrc-output">- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe</span>
<span class="pysrc-output">- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe</span>
<span class="pysrc-output">- lx:rt:ps:ge:ge:tkp:dt</span>
<span class="pysrc-output">- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe</span>
<span class="pysrc-output">- lx:rt:ps:ge:tkp:dt:ex:xp:xe</span>
<span class="pysrc-output">- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe</span></pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_toolbox_validation.py" type="text/x-python"><span class="caption-label">Example 5.2 (code_toolbox_validation.py)</span></a>: <span class="caption-label">Figure 5.2</span>: Validating Toolbox Entries Using a Context Free Grammar</td></tr></p>
</table></div>
<p>Another approach would be to use a chunk parser (<a class="reference external" href="ch07.html#chap-chunk">7.</a>),
since these are much more effective at identifying partial
structures, and can report the partial structures that have
been identified.  In <a class="reference internal" href="#code-chunk-toolbox">5.3</a> we set up
a chunk grammar for the entries of a lexicon, then parse each entry.
A sample of the output from this program is shown in <a class="reference internal" href="#fig-iu-mien">5.4</a>.</p>
<span class="target" id="code-chunk-toolbox"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
grammar = r<span class="pysrc-string">&quot;&quot;&quot;</span>
<span class="pysrc-string">      lexfunc: {&lt;lf&gt;(&lt;lv&gt;&lt;ln|le&gt;*)*}</span>
<span class="pysrc-string">      example: {&lt;rf|xv&gt;&lt;xn|xe&gt;*}</span>
<span class="pysrc-string">      sense:   {&lt;sn&gt;&lt;ps&gt;&lt;pn|gv|dv|gn|gp|dn|rn|ge|de|re&gt;*&lt;example&gt;*&lt;lexfunc&gt;*}</span>
<span class="pysrc-string">      record:   {&lt;lx&gt;&lt;hm&gt;&lt;sense&gt;+&lt;dt&gt;}</span>
<span class="pysrc-string">    &quot;&quot;&quot;</span></pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar"
        onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"
        >&nbsp;</td>
<td class="pysrc"><pre class="doctest">
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> xml.etree.ElementTree <span class="pysrc-keyword">import</span> ElementTree
<span class="pysrc-prompt">&gt;&gt;&gt; </span><span class="pysrc-keyword">from</span> nltk.toolbox <span class="pysrc-keyword">import</span> ToolboxData
<span class="pysrc-prompt">&gt;&gt;&gt; </span>db = ToolboxData()
<span class="pysrc-prompt">&gt;&gt;&gt; </span>db.open(nltk.data.find(<span class="pysrc-string">'corpora/toolbox/iu_mien_samp.db'</span>))
<span class="pysrc-prompt">&gt;&gt;&gt; </span>lexicon = db.parse(grammar, encoding=<span class="pysrc-string">'utf8'</span>)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>tree = ElementTree(lexicon)
<span class="pysrc-prompt">&gt;&gt;&gt; </span>with open(<span class="pysrc-string">&quot;iu_mien_samp.xml&quot;</span>, <span class="pysrc-string">&quot;wb&quot;</span>) <span class="pysrc-keyword">as</span> output:
<span class="pysrc-more">... </span>    tree.write(output)</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="pylisting/code_chunk_toolbox.py" type="text/x-python"><span class="caption-label">Example 5.3 (code_chunk_toolbox.py)</span></a>: <span class="caption-label">Figure 5.3</span>: Chunking a Toolbox Lexicon: A chunk grammar describing the structure of
entries for a lexicon for <span class="emphasis">Iu Mien</span>, a language of China.</td></tr></p>
</table></div>
<span class="target" id="fig-iu-mien"></span><div class="figure" id="fig-iu-mien">
<img alt="../images/iu-mien.png" src="../images/iu-mien.png" style="width: 537.5px; height: 429.0px;" />
<p class="caption"><span class="caption-label">Figure 5.4</span>: XML Representation of a Lexical Entry, Resulting from Chunk Parsing a Toolbox Record</p>
</div>
</div>
</div>
<div class="section" id="describing-language-resources-using-olac-metadata">
<h1>6&nbsp;&nbsp;&nbsp;Describing Language Resources using OLAC Metadata</h1>
<p>Members of the NLP community have a common need for
discovering language resources with high precision and recall.
The solution which has been developed by the Digital Libraries
community involves metadata aggregation.</p>
<div class="section" id="what-is-metadata">
<h2>6.1&nbsp;&nbsp;&nbsp;What is Metadata?</h2>
<p>The simplest definition of metadata is &quot;structured data about data.&quot;
Metadata is descriptive information about an object or resource whether
it be physical or electronic. While the term metadata itself is relatively new,
the underlying concepts behind metadata have been in use for as long as
collections of information have been organized.
Library catalogs represent a well-established type of metadata;
they have served as collection management and resource discovery
tools for decades. Metadata can be generated either
&quot;by hand&quot; or generated automatically using software.</p>
<p>The Dublin Core Metadata Initiative began in 1995 to develop
conventions for resource discovery on the web.
The Dublin Core metadata elements represent a broad,
interdisciplinary consensus about the core set of elements
that are likely to be widely useful to support resource discovery.
The Dublin Core consists of 15 metadata elements, where each
element is optional and repeatable: Title, Creator, Subject,
Description, Publisher, Contributor, Date, Type, Format,
Identifier, Source, Language, Relation, Coverage, Rights.
This metadata set can be used to describe resources that
exist in digital or traditional formats.</p>
<p>The Open Archives initiative (OAI) provides a common framework
across digital repositories of scholarly materials regardless of their type,
including documents, data, software, recordings, physical artifacts,
digital surrogates, and so forth.
Each repository consists of a network accessible server offering
public access to archived items.  Each item has a unique identifier,
and is associated with a Dublin Core metadata record (and possibly additional
records in other formats).  The OAI defines a protocol for metadata search
services to &quot;harvest&quot; the contents of repositories.</p>
</div>
<div class="section" id="olac-open-language-archives-community">
<h2>6.2&nbsp;&nbsp;&nbsp;OLAC: Open Language Archives Community</h2>
<p>The Open Language Archives Community (OLAC) is an international
partnership of institutions and individuals who are creating a
worldwide virtual library of language resources by:
(i) developing consensus on best current practice for
the digital archiving of language resources, and
(ii) developing a network of interoperating repositories
and services for housing and accessing such resources.
OLAC's home on the web is at <tt class="doctest"><span class="pre">http://www.language-archives.org/</span></tt>.</p>
<p>OLAC Metadata is a standard for describing language resources.
Uniform description across repositories is ensured by limiting
the values of certain metadata elements to the use of terms
from controlled vocabularies. OLAC metadata can be used to
describe data and tools, in both physical and digital formats.
OLAC metadata extends the Dublin Core Metadata Set,
a widely accepted standard for describing resources of all types.
To this core set, OLAC adds descriptors to cover fundamental
properties of language resources, such as subject language and
linguistic type.  Here's an example of a complete OLAC record:</p>
<pre class="literal-block">
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;olac:olac xmlns:olac=&quot;http://www.language-archives.org/OLAC/1.1/&quot;
           xmlns=&quot;http://purl.org/dc/elements/1.1/&quot;
           xmlns:dcterms=&quot;http://purl.org/dc/terms/&quot;
           xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
           xsi:schemaLocation=&quot;http://www.language-archives.org/OLAC/1.1/
                http://www.language-archives.org/OLAC/1.1/olac.xsd&quot;&gt;
  &lt;title&gt;A grammar of Kayardild. With comparative notes on Tangkic.&lt;/title&gt;
  &lt;creator&gt;Evans, Nicholas D.&lt;/creator&gt;
  &lt;subject&gt;Kayardild grammar&lt;/subject&gt;
  &lt;subject xsi:type=&quot;olac:language&quot; olac:code=&quot;gyd&quot;&gt;Kayardild&lt;/subject&gt;
  &lt;language xsi:type=&quot;olac:language&quot; olac:code=&quot;en&quot;&gt;English&lt;/language&gt;
  &lt;description&gt;Kayardild Grammar (ISBN 3110127954)&lt;/description&gt;
  &lt;publisher&gt;Berlin - Mouton de Gruyter&lt;/publisher&gt;
  &lt;contributor xsi:type=&quot;olac:role&quot; olac:code=&quot;author&quot;&gt;Nicholas Evans&lt;/contributor&gt;
  &lt;format&gt;hardcover, 837 pages&lt;/format&gt;
  &lt;relation&gt;related to ISBN 0646119966&lt;/relation&gt;
  &lt;coverage&gt;Australia&lt;/coverage&gt;
  &lt;type xsi:type=&quot;olac:linguistic-type&quot; olac:code=&quot;language_description&quot;/&gt;
  &lt;type xsi:type=&quot;dcterms:DCMIType&quot;&gt;Text&lt;/type&gt;
&lt;/olac:olac&gt;
</pre>
<p>Participating language archives publish their catalogs in an XML
format, and these records are regularly &quot;harvested&quot; by OLAC services
using the OAI protocol.  In addition to this software infrastructure,
OLAC has documented a series of best practices for describing
language resources, through a process that involved extended
consultation with the language resources community (e.g. see
<tt class="doctest"><span class="pre">http://www.language-archives.org/REC/bpr.html</span></tt>).</p>
<p>OLAC repositories can be searched using a query engine on the OLAC website.
Searching for &quot;German lexicon&quot; finds the following resources, amongst others:</p>
<ul class="simple">
<li>CALLHOME German Lexicon
<tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:www.ldc.upenn.edu:LDC97L18</span></tt></li>
<li>MULTILEX multilingual lexicon
<tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:elra.icp.inpg.fr:M0001</span></tt></li>
<li>Slelex Siemens Phonetic lexicon
<tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:elra.icp.inpg.fr:S0048</span></tt></li>
</ul>
<p>Searching for &quot;Korean&quot; finds a newswire corpus, a treebank, a lexicon,
a child-language corpus, interlinear glossed texts.  It also finds software
including a syntactic analyzer and a morphological analyzer.</p>
<p>Observe that the above URLs include a substring of the form:
<tt class="doctest"><span class="pre">oai:www.ldc.upenn.edu:LDC97L18</span></tt>.  This is an OAI identifier,
using a URI scheme registered with ICANN
(the <em>Internet Corporation for Assigned Names and Numbers</em>).
These identifiers have the format <tt class="doctest"><span class="pre">oai:</span></tt><em>archive</em>:<em>local_id</em>,
where <tt class="doctest"><span class="pre">oai</span></tt> is the name of the URI scheme,
<em>archive</em> is an archive identifier such as <tt class="doctest"><span class="pre">www.ldc.upenn.edu</span></tt>,
and <em>local_id</em> is the resource identifier assigned by the archive, e.g. <tt class="doctest"><span class="pre">LDC97L18</span></tt>.</p>
<p>Given an OAI identifier for an OLAC resource, it is possible to retrieve
the complete XML record for the resource using a URL of the following form:</p>
<pre class="literal-block">
http://www.language-archives.org/static-records/oai:archive:local_id
</pre>
</div>
<div class="section" id="disseminating-language-resources">
<h2>6.3&nbsp;&nbsp;&nbsp;Disseminating Language Resources</h2>
<p>The Linguistic Data Consortium hosts the <a name="nltk_data_repository_index_term" /><span class="termdef">NLTK Data Repository</span>,
an open-access archive where community members can upload corpora and
saved models.  These resources can be easily accessed using NLTK's
downloader tool.</p>
</div>
</div>
<div class="section" id="summary">
<h1>7&nbsp;&nbsp;&nbsp;Summary</h1>
<ul class="simple">
<li>Fundamental data types, present in most corpora, are annotated texts
and lexicons.  Texts have a temporal structure, while lexicons have a
record structure.</li>
<li>The lifecycle of a corpus includes data collection, annotation, quality
control, and publication.  The lifecycle continues after publication as
the corpus is modified and enriched during the course of research.</li>
<li>Corpus development involves a balance between capturing a representative
sample of language usage, and capturing enough material from any one
source or genre to be useful; multiplying out the dimensions of
variability is usually not feasible because of resource limitations.</li>
<li>XML provides a useful format for the storage and interchange of linguistic
data, but provides no shortcuts for solving pervasive data modeling problems.</li>
<li>Toolbox format is widely used in language documentation projects; we can
write programs to support the curation of Toolbox files, and to convert
them to XML.</li>
<li>The Open Language Archives Community (OLAC) provides an infrastructure for
documenting and discovering language resources.</li>
</ul>
</div>
<div class="section" id="further-reading">
<span id="sec-further-reading-data"></span><h1>8&nbsp;&nbsp;&nbsp;Further Reading</h1>
<p>Extra materials for this chapter are posted at <tt class="doctest"><span class="pre">http://nltk.org/</span></tt>, including links to freely
available resources on the web.</p>
<p>The primary sources of linguistic corpora are the <em>Linguistic Data Consortium</em> and
the <em>European Language Resources Agency</em>, both with extensive online catalogs.
More details concerning the major corpora mentioned in the chapter are available:
American National Corpus <a class="reference external" href="bibliography.html#reppen2005anc" id="id2">(Reppen, Ide, &amp; Suderman, 2005)</a>,
British National Corpus <a class="reference external" href="bibliography.html#bnc1999" id="id3">({BNC}, 1999)</a>,
Thesaurus Linguae Graecae <a class="reference external" href="bibliography.html#tlg1999" id="id4">({TLG}, 1999)</a>,
Child Language Data Exchange System (CHILDES) <a class="reference external" href="bibliography.html#macwhinney1995childes" id="id5">(MacWhinney, 1995)</a>,
TIMIT <a class="reference external" href="bibliography.html#garofolo1986timit" id="id6">(S., Lamel, &amp; William, 1986)</a>.</p>
<p>Two special interest groups of the Association for Computational Linguistics
that organize regular workshops with published proceedings are
SIGWAC, which promotes the use of the web as a corpus and has
sponsored the CLEANEVAL task for removing HTML markup,
and SIGANN, which is encouraging efforts towards interoperability
of linguistic annotations.</p>
<p>Full details of the Toolbox data format are provided with the distribution <a class="reference external" href="bibliography.html#buseman1996shoebox" id="id7">(Buseman, Buseman, &amp; Early, 1996)</a>,
and with the latest distribution, freely available from <tt class="doctest"><span class="pre">http://www.sil.org/computing/toolbox/</span></tt>.
For guidelines on the process of constructing a Toolbox lexicon see
<tt class="doctest"><span class="pre">http://www.sil.org/computing/ddp/</span></tt>.
More examples of our efforts with the Toolbox
are documented in <a class="reference external" href="bibliography.html#bird1999nels" id="id8">(Tamanji, Hirotani, &amp; Hall, 1999)</a>, <a class="reference external" href="bibliography.html#robinson2007toolbox" id="id9">(Robinson, Aumann, &amp; Bird, 2007)</a>.
Dozens of other tools for linguistic data management are available, some
surveyed by <a class="reference external" href="bibliography.html#bird2003portability" id="id10">(Bird &amp; Simons, 2003)</a>.
See also the proceedings of the &quot;LaTeCH&quot; workshops on
language technology for cultural heritage data.</p>
<p>There are many excellent resources for XML (e.g. <tt class="doctest"><span class="pre">http://zvon.org/</span></tt>)
and for writing Python programs to work with XML.  Many editors have XML modes.
XML formats for lexical information include
OLIF <tt class="doctest"><span class="pre">http://www.olif.net/</span></tt>
and LIFT <tt class="doctest"><span class="pre">http://code.google.com/p/lift-standard/</span></tt>.</p>
<p>For a survey of linguistic annotation software, see the
<em>Linguistic Annotation Page</em> at <tt class="doctest"><span class="pre">http://www.ldc.upenn.edu/annotation/</span></tt>.
The initial proposal for standoff annotation was <a class="reference external" href="bibliography.html#thompson1997standoff" id="id11">(Thompson &amp; McKelvie, 1997)</a>.
An abstract data model for linguistic annotations, called
&quot;annotation graphs&quot;, was proposed in <a class="reference external" href="bibliography.html#bird2001annotation" id="id12">(Bird &amp; Liberman, 2001)</a>.
A general-purpose ontology for linguistic description (GOLD) is
documented at <tt class="doctest"><span class="pre">http://www.linguistics-ontology.org/</span></tt>.</p>
<p>For guidance on planning and constructing a corpus, see <a class="reference external" href="bibliography.html#meyer2002" id="id13">(Meyer, 2002)</a>, <a class="reference external" href="bibliography.html#farghaly2003" id="id14">(Farghaly, 2003)</a>
More details of methods for scoring inter-annotator agreement are available
in <a class="reference external" href="bibliography.html#artsteinpoesio2008" id="id15">(Artstein &amp; Poesio, 2008)</a>, <a class="reference external" href="bibliography.html#pevzner2002windowdiff" id="id16">(Pevzner &amp; Hearst, 2002)</a>.</p>
<p>Rotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg Aumann.</p>
<p>For more information about the Open Language Archives Community, visit
<tt class="doctest"><span class="pre">http://www.language-archives.org/</span></tt>, or see <a class="reference external" href="bibliography.html#simonsbird2003llc" id="id17">(Simons &amp; Bird, 2003)</a>.</p>
</div>
<div class="section" id="exercises">
<h1>9&nbsp;&nbsp;&nbsp;Exercises</h1>
<ol class="arabic">
<li><p class="first">&#9681; In <a class="reference internal" href="#code-add-cv-field">5.1</a> the new field appeared at the bottom of the
entry.  Modify this program so that it inserts the new subelement right after
the <tt class="doctest"><span class="pre">lx</span></tt> field.  (Hint: create the new <tt class="doctest"><span class="pre">cv</span></tt> field using <tt class="doctest"><span class="pre">Element(<span class="pysrc-string">'cv'</span>)</span></tt>,
assign a text value to it, then use the <tt class="doctest"><span class="pre">insert()</span></tt> method of the parent element.)</p>
</li>
<li><p class="first">&#9681; Write a function that deletes a specified field from a lexical entry.
(We could use this to sanitize our lexical data before giving it to others,
e.g. by removing fields containing irrelevant or uncertain content.)</p>
</li>
<li><p class="first">&#9681; Write a program that scans an HTML dictionary file to find entries
having an illegal part-of-speech field, and reports the <em>headword</em> for
each entry.</p>
</li>
<li><p class="first">&#9681; Write a program to find any parts of speech (<tt class="doctest"><span class="pre">ps</span></tt> field) that
occurred less than ten times.  Perhaps these are typing mistakes?</p>
</li>
<li><p class="first">&#9681; We saw a method for discovering cases of whole-word reduplication.
Write a function to find words that may contain partial
reduplication.  Use the <tt class="doctest"><span class="pre">re.search()</span></tt> method, and the following
regular expression: <tt class="doctest"><span class="pre">(..+)\1</span></tt></p>
</li>
<li><p class="first">&#9681; We saw a method for adding a <tt class="doctest"><span class="pre">cv</span></tt> field.  There is an interesting
issue with keeping this up-to-date when someone modifies the content
of the <tt class="doctest"><span class="pre">lx</span></tt> field on which it is based.  Write a version of this
program to add a <tt class="doctest"><span class="pre">cv</span></tt> field, replacing any existing <tt class="doctest"><span class="pre">cv</span></tt> field.</p>
</li>
<li><p class="first">&#9681; Write a function to add a new field <tt class="doctest"><span class="pre">syl</span></tt> which gives a count of
the number of syllables in the word.</p>
</li>
<li><p class="first">&#9681; Write a function which displays the complete entry for a lexeme.
When the lexeme is incorrectly spelled it should display the entry
for the most similarly spelled lexeme.</p>
</li>
<li><p class="first">&#9681; Write a function that takes a lexicon and finds which pairs of
consecutive fields are most frequent (e.g. <tt class="doctest"><span class="pre">ps</span></tt> is often followed by <tt class="doctest"><span class="pre">pt</span></tt>).
(This might help us to discover some of the structure of a lexical entry.)</p>
</li>
<li><p class="first">&#9681; Create a spreadsheet using office software, containing one lexical entry per
row, consisting of a headword, a part of speech, and a gloss.  Save the
spreadsheet in CSV format.  Write Python code to read the CSV file and print it in
Toolbox format, using <tt class="doctest"><span class="pre">lx</span></tt> for the headword, <tt class="doctest"><span class="pre">ps</span></tt> for the part of speech,
and <tt class="doctest"><span class="pre">gl</span></tt> for the gloss.</p>
</li>
<li><p class="first">&#9681; Index the words of Shakespeare's plays, with the help of <tt class="doctest"><span class="pre">nltk.Index</span></tt>.
The resulting data structure should permit lookup on individual words such as <span class="example">music</span>,
returning a list of references to acts, scenes and speeches, of the form
<tt class="doctest"><span class="pre">[(3, 2, 9), (5, 1, 23), ...]</span></tt>, where <tt class="doctest"><span class="pre">(3, 2, 9)</span></tt> indicates
Act 3 Scene 2 Speech 9.</p>
</li>
<li><p class="first">&#9681; Construct a conditional frequency distribution which records the word length
for each speech in <span class="emphasis">The Merchant of Venice</span>, conditioned on the name of the character,
e.g. <tt class="doctest"><span class="pre">cfd[<span class="pysrc-string">'PORTIA'</span>][12]</span></tt> would give us the number of speeches by Portia
consisting of 12 words.</p>
</li>
<li><p class="first">&#9733; Obtain a comparative wordlist in CSV format, and write a program
that prints those cognates having an edit-distance of at least three
from each other.</p>
</li>
<li><p class="first">&#9733; Build an index of those lexemes which appear in example sentences.
Suppose the lexeme for a given entry is <em>w</em>.
Then add a single cross-reference field <tt class="doctest"><span class="pre">xrf</span></tt> to this entry, referencing
the headwords of other entries having example sentences containing
<em>w</em>.  Do this for all entries and save the result as a toolbox-format file.</p>
</li>
<li><p class="first">&#9681; Write a recursive function to produce an XML representation for a
tree, with non-terminals represented as XML elements, and leaves represented
as text content, e.g.:</p>
<pre class="literal-block">
&lt;S&gt;
  &lt;NP type=&quot;SBJ&quot;&gt;
    &lt;NP&gt;
      &lt;NNP&gt;Pierre&lt;/NNP&gt;
      &lt;NNP&gt;Vinken&lt;/NNP&gt;
    &lt;/NP&gt;
    &lt;COMMA&gt;,&lt;/COMMA&gt;
</pre>
</li>
</ol>
<!-- Footer to be used in all chapters -->
<div class="admonition admonition-about-this-document">
<p class="first admonition-title">About this document...</p>
<p>UPDATED FOR NLTK 3.0.
This is a chapter from <em>Natural Language Processing with Python</em>,
by <a class="reference external" href="http://stevenbird.net/">Steven Bird</a>, <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Ewan Klein</a> and <a class="reference external" href="http://ed.loper.org/">Edward Loper</a>,
Copyright &#169; 2019 the authors.
It is distributed with the <em>Natural Language Toolkit</em> [<tt class="doctest"><span class="pre">http://nltk.org/</span></tt>],
Version 3.0, under the terms of the
<em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License</em>
[<a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">This document was built on
Wed  4 Sep 2019 11:40:48 ACST</p>
</div>
</div>
</div>
</body>
</html>
