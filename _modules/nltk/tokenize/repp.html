
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>nltk.tokenize.repp &#8212; NLTK 3.4.1 documentation</title>
    <link rel="stylesheet" href="../../../_static/agogo.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head><body>
    <div class="header-wrapper" role="banner">
      <div class="header">
        <div class="headertitle"><a
          href="../../../index.html">NLTK 3.4.1 documentation</a></div>
        <div class="rel" role="navigation" aria-label="related navigation">
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
       </div>
    </div>

    <div class="content-wrapper">
      <div class="content">
        <div class="document">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for nltk.tokenize.repp</h1><div class="highlight"><pre>
<span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1"># Natural Language Toolkit: Interface to the Repp Tokenizer</span>
<span class="c1">#</span>
<span class="c1"># Copyright (C) 2001-2015 NLTK Project</span>
<span class="c1"># Authors: Rebecca Dridan and Stephan Oepen</span>
<span class="c1"># Contributors: Liling Tan</span>
<span class="c1">#</span>
<span class="c1"># URL: &lt;http://nltk.org/&gt;</span>
<span class="c1"># For license information, see LICENSE.TXT</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">unicode_literals</span><span class="p">,</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">tempfile</span>

<span class="kn">from</span> <span class="nn">six</span> <span class="k">import</span> <span class="n">text_type</span>

<span class="kn">from</span> <span class="nn">nltk.data</span> <span class="k">import</span> <span class="n">ZipFilePathPointer</span>
<span class="kn">from</span> <span class="nn">nltk.internals</span> <span class="k">import</span> <span class="n">find_dir</span>

<span class="kn">from</span> <span class="nn">nltk.tokenize.api</span> <span class="k">import</span> <span class="n">TokenizerI</span>


<div class="viewcode-block" id="ReppTokenizer"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer">[docs]</a><span class="k">class</span> <span class="nc">ReppTokenizer</span><span class="p">(</span><span class="n">TokenizerI</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A class for word tokenization using the REPP parser described in</span>
<span class="sd">    Rebecca Dridan and Stephan Oepen (2012) Tokenization: Returning to a</span>
<span class="sd">    Long Solved Problem - A Survey, Contrastive  Experiment, Recommendations,</span>
<span class="sd">    and Toolkit. In ACL. http://anthology.aclweb.org/P/P12/P12-2.pdf#page=406</span>

<span class="sd">    &gt;&gt;&gt; sents = [&#39;Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve.&#39; ,</span>
<span class="sd">    ... &#39;But rule-based tokenizers are hard to maintain and their rules language specific.&#39; ,</span>
<span class="sd">    ... &#39;We evaluated our method on three languages and obtained error rates of 0.27% (English), 0.35% (Dutch) and 0.76% (Italian) for our best models.&#39;</span>
<span class="sd">    ... ]</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = ReppTokenizer(&#39;/home/alvas/repp/&#39;) # doctest: +SKIP</span>
<span class="sd">    &gt;&gt;&gt; for sent in sents:                             # doctest: +SKIP</span>
<span class="sd">    ...     tokenizer.tokenize(sent)                   # doctest: +SKIP</span>
<span class="sd">    ...</span>
<span class="sd">    (u&#39;Tokenization&#39;, u&#39;is&#39;, u&#39;widely&#39;, u&#39;regarded&#39;, u&#39;as&#39;, u&#39;a&#39;, u&#39;solved&#39;, u&#39;problem&#39;, u&#39;due&#39;, u&#39;to&#39;, u&#39;the&#39;, u&#39;high&#39;, u&#39;accuracy&#39;, u&#39;that&#39;, u&#39;rulebased&#39;, u&#39;tokenizers&#39;, u&#39;achieve&#39;, u&#39;.&#39;)</span>
<span class="sd">    (u&#39;But&#39;, u&#39;rule-based&#39;, u&#39;tokenizers&#39;, u&#39;are&#39;, u&#39;hard&#39;, u&#39;to&#39;, u&#39;maintain&#39;, u&#39;and&#39;, u&#39;their&#39;, u&#39;rules&#39;, u&#39;language&#39;, u&#39;specific&#39;, u&#39;.&#39;)</span>
<span class="sd">    (u&#39;We&#39;, u&#39;evaluated&#39;, u&#39;our&#39;, u&#39;method&#39;, u&#39;on&#39;, u&#39;three&#39;, u&#39;languages&#39;, u&#39;and&#39;, u&#39;obtained&#39;, u&#39;error&#39;, u&#39;rates&#39;, u&#39;of&#39;, u&#39;0.27&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;English&#39;, u&#39;)&#39;, u&#39;,&#39;, u&#39;0.35&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Dutch&#39;, u&#39;)&#39;, u&#39;and&#39;, u&#39;0.76&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Italian&#39;, u&#39;)&#39;, u&#39;for&#39;, u&#39;our&#39;, u&#39;best&#39;, u&#39;models&#39;, u&#39;.&#39;)</span>

<span class="sd">    &gt;&gt;&gt; for sent in tokenizer.tokenize_sents(sents): # doctest: +SKIP</span>
<span class="sd">    ...     print sent                               # doctest: +SKIP</span>
<span class="sd">    ...</span>
<span class="sd">    (u&#39;Tokenization&#39;, u&#39;is&#39;, u&#39;widely&#39;, u&#39;regarded&#39;, u&#39;as&#39;, u&#39;a&#39;, u&#39;solved&#39;, u&#39;problem&#39;, u&#39;due&#39;, u&#39;to&#39;, u&#39;the&#39;, u&#39;high&#39;, u&#39;accuracy&#39;, u&#39;that&#39;, u&#39;rulebased&#39;, u&#39;tokenizers&#39;, u&#39;achieve&#39;, u&#39;.&#39;)</span>
<span class="sd">    (u&#39;But&#39;, u&#39;rule-based&#39;, u&#39;tokenizers&#39;, u&#39;are&#39;, u&#39;hard&#39;, u&#39;to&#39;, u&#39;maintain&#39;, u&#39;and&#39;, u&#39;their&#39;, u&#39;rules&#39;, u&#39;language&#39;, u&#39;specific&#39;, u&#39;.&#39;)</span>
<span class="sd">    (u&#39;We&#39;, u&#39;evaluated&#39;, u&#39;our&#39;, u&#39;method&#39;, u&#39;on&#39;, u&#39;three&#39;, u&#39;languages&#39;, u&#39;and&#39;, u&#39;obtained&#39;, u&#39;error&#39;, u&#39;rates&#39;, u&#39;of&#39;, u&#39;0.27&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;English&#39;, u&#39;)&#39;, u&#39;,&#39;, u&#39;0.35&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Dutch&#39;, u&#39;)&#39;, u&#39;and&#39;, u&#39;0.76&#39;, u&#39;%&#39;, u&#39;(&#39;, u&#39;Italian&#39;, u&#39;)&#39;, u&#39;for&#39;, u&#39;our&#39;, u&#39;best&#39;, u&#39;models&#39;, u&#39;.&#39;)</span>
<span class="sd">    &gt;&gt;&gt; for sent in tokenizer.tokenize_sents(sents, keep_token_positions=True): # doctest: +SKIP</span>
<span class="sd">    ...     print sent                                                          # doctest: +SKIP</span>
<span class="sd">    ...</span>
<span class="sd">    [(u&#39;Tokenization&#39;, 0, 12), (u&#39;is&#39;, 13, 15), (u&#39;widely&#39;, 16, 22), (u&#39;regarded&#39;, 23, 31), (u&#39;as&#39;, 32, 34), (u&#39;a&#39;, 35, 36), (u&#39;solved&#39;, 37, 43), (u&#39;problem&#39;, 44, 51), (u&#39;due&#39;, 52, 55), (u&#39;to&#39;, 56, 58), (u&#39;the&#39;, 59, 62), (u&#39;high&#39;, 63, 67), (u&#39;accuracy&#39;, 68, 76), (u&#39;that&#39;, 77, 81), (u&#39;rulebased&#39;, 82, 91), (u&#39;tokenizers&#39;, 92, 102), (u&#39;achieve&#39;, 103, 110), (u&#39;.&#39;, 110, 111)]</span>
<span class="sd">    [(u&#39;But&#39;, 0, 3), (u&#39;rule-based&#39;, 4, 14), (u&#39;tokenizers&#39;, 15, 25), (u&#39;are&#39;, 26, 29), (u&#39;hard&#39;, 30, 34), (u&#39;to&#39;, 35, 37), (u&#39;maintain&#39;, 38, 46), (u&#39;and&#39;, 47, 50), (u&#39;their&#39;, 51, 56), (u&#39;rules&#39;, 57, 62), (u&#39;language&#39;, 63, 71), (u&#39;specific&#39;, 72, 80), (u&#39;.&#39;, 80, 81)]</span>
<span class="sd">    [(u&#39;We&#39;, 0, 2), (u&#39;evaluated&#39;, 3, 12), (u&#39;our&#39;, 13, 16), (u&#39;method&#39;, 17, 23), (u&#39;on&#39;, 24, 26), (u&#39;three&#39;, 27, 32), (u&#39;languages&#39;, 33, 42), (u&#39;and&#39;, 43, 46), (u&#39;obtained&#39;, 47, 55), (u&#39;error&#39;, 56, 61), (u&#39;rates&#39;, 62, 67), (u&#39;of&#39;, 68, 70), (u&#39;0.27&#39;, 71, 75), (u&#39;%&#39;, 75, 76), (u&#39;(&#39;, 77, 78), (u&#39;English&#39;, 78, 85), (u&#39;)&#39;, 85, 86), (u&#39;,&#39;, 86, 87), (u&#39;0.35&#39;, 88, 92), (u&#39;%&#39;, 92, 93), (u&#39;(&#39;, 94, 95), (u&#39;Dutch&#39;, 95, 100), (u&#39;)&#39;, 100, 101), (u&#39;and&#39;, 102, 105), (u&#39;0.76&#39;, 106, 110), (u&#39;%&#39;, 110, 111), (u&#39;(&#39;, 112, 113), (u&#39;Italian&#39;, 113, 120), (u&#39;)&#39;, 120, 121), (u&#39;for&#39;, 122, 125), (u&#39;our&#39;, 126, 129), (u&#39;best&#39;, 130, 134), (u&#39;models&#39;, 135, 141), (u&#39;.&#39;, 141, 142)]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">repp_dir</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">repp_dir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_repptokenizer</span><span class="p">(</span><span class="n">repp_dir</span><span class="p">)</span>
        <span class="c1"># Set a directory to store the temporary files.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">working_dir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">gettempdir</span><span class="p">()</span>
        <span class="c1"># Set an encoding for the input strings.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span>

<div class="viewcode-block" id="ReppTokenizer.tokenize"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer.tokenize">[docs]</a>    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Use Repp to tokenize a single sentence.</span>

<span class="sd">        :param sentence: A single sentence string.</span>
<span class="sd">        :type sentence: str</span>
<span class="sd">        :return: A tuple of tokens.</span>
<span class="sd">        :rtype: tuple(str)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenize_sents</span><span class="p">([</span><span class="n">sentence</span><span class="p">]))</span></div>

<div class="viewcode-block" id="ReppTokenizer.tokenize_sents"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer.tokenize_sents">[docs]</a>    <span class="k">def</span> <span class="nf">tokenize_sents</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">keep_token_positions</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize multiple sentences using Repp.</span>

<span class="sd">        :param sentences: A list of sentence strings.</span>
<span class="sd">        :type sentences: list(str)</span>
<span class="sd">        :return: A list of tuples of tokens</span>
<span class="sd">        :rtype: iter(tuple(str))</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;repp_input.&#39;</span><span class="p">,</span> <span class="nb">dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">working_dir</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">input_file</span><span class="p">:</span>
            <span class="c1"># Write sentences to temporary input file.</span>
            <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
                <span class="n">input_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text_type</span><span class="p">(</span><span class="n">sent</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">input_file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="c1"># Generate command to run REPP.</span>
            <span class="n">cmd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_repp_command</span><span class="p">(</span><span class="n">input_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="c1"># Decode the stdout and strips the ending newline.</span>
            <span class="n">repp_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_execute</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">tokenized_sent</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_repp_outputs</span><span class="p">(</span><span class="n">repp_output</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_token_positions</span><span class="p">:</span>
                    <span class="c1"># Removes token position information.</span>
                    <span class="n">tokenized_sent</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">ends</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">tokenized_sent</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">tokenized_sent</span></div>

<div class="viewcode-block" id="ReppTokenizer.generate_repp_command"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer.generate_repp_command">[docs]</a>    <span class="k">def</span> <span class="nf">generate_repp_command</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputfilename</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This module generates the REPP command to be used at the terminal.</span>

<span class="sd">        :param inputfilename: path to the input file</span>
<span class="sd">        :type inputfilename: str</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">repp_dir</span> <span class="o">+</span> <span class="s1">&#39;/src/repp&#39;</span><span class="p">]</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">&#39;-c&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">repp_dir</span> <span class="o">+</span> <span class="s1">&#39;/erg/repp.set&#39;</span><span class="p">]</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="s1">&#39;--format&#39;</span><span class="p">,</span> <span class="s1">&#39;triple&#39;</span><span class="p">]</span>
        <span class="n">cmd</span> <span class="o">+=</span> <span class="p">[</span><span class="n">inputfilename</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">cmd</span></div>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_execute</span><span class="p">(</span><span class="n">cmd</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">(</span><span class="n">cmd</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">)</span>
        <span class="n">stdout</span><span class="p">,</span> <span class="n">stderr</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">communicate</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">stdout</span>

<div class="viewcode-block" id="ReppTokenizer.parse_repp_outputs"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer.parse_repp_outputs">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">parse_repp_outputs</span><span class="p">(</span><span class="n">repp_output</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This module parses the tri-tuple format that REPP outputs using the</span>
<span class="sd">        &quot;--format triple&quot; option and returns an generator with tuple of string</span>
<span class="sd">        tokens.</span>

<span class="sd">        :param repp_output:</span>
<span class="sd">        :type repp_output: type</span>
<span class="sd">        :return: an iterable of the tokenized sentences as tuples of strings</span>
<span class="sd">        :rtype: iter(tuple)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">line_regex</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s1">&#39;^\((\d+), (\d+), (.+)\)$&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">MULTILINE</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="n">repp_output</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">):</span>
            <span class="n">words_with_positions</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">start</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">end</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">line_regex</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="n">words</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">words_with_positions</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">words_with_positions</span></div>

<div class="viewcode-block" id="ReppTokenizer.find_repptokenizer"><a class="viewcode-back" href="../../../api/nltk.tokenize.html#nltk.tokenize.repp.ReppTokenizer.find_repptokenizer">[docs]</a>    <span class="k">def</span> <span class="nf">find_repptokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">repp_dirname</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A module to find REPP tokenizer binary and its *repp.set* config file.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">repp_dirname</span><span class="p">):</span>  <span class="c1"># If a full path is given.</span>
            <span class="n">_repp_dir</span> <span class="o">=</span> <span class="n">repp_dirname</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># Try to find path to REPP directory in environment variables.</span>
            <span class="n">_repp_dir</span> <span class="o">=</span> <span class="n">find_dir</span><span class="p">(</span><span class="n">repp_dirname</span><span class="p">,</span> <span class="n">env_vars</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;REPP_TOKENIZER&#39;</span><span class="p">,))</span>
        <span class="c1"># Checks for the REPP binary and erg/repp.set config file.</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">_repp_dir</span> <span class="o">+</span> <span class="s1">&#39;/src/repp&#39;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">_repp_dir</span> <span class="o">+</span> <span class="s1">&#39;/erg/repp.set&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_repp_dir</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
        </div>
        <div class="sidebar">
          <h3>Table of Contents</h3>
          <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../news.html">NLTK News</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installing NLTK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">Installing NLTK Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contribute to NLTK</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki/FAQ">FAQ</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/nltk/nltk/wiki">Wiki</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/nltk.html">API</a></li>
<li class="toctree-l1"><a class="reference external" href="http://www.nltk.org/howto">HOWTO</a></li>
</ul>

          <div role="search">
            <h3 style="margin-top: 1.5em;">Search</h3>
            <form class="search" action="../../../search.html" method="get">
                <input type="text" name="q" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
            </form>
          </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer-wrapper">
      <div class="footer">
        <div class="left">
          <div role="navigation" aria-label="related navigaton">
            <a href="../../../py-modindex.html" title="Python Module Index"
              >modules</a> |
            <a href="../../../genindex.html" title="General Index"
              >index</a>
          </div>
          <div role="note" aria-label="source link">
          </div>
        </div>

        <div class="right">
          
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, NLTK Project.
      Last updated on Apr 17, 2019.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.5.
    </div>
        </div>
        <div class="clearer"></div>
      </div>
    </div>

  </body>
</html>